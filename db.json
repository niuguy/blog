{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"source/pic/101_01.png","path":"pic/101_01.png","modified":1,"renderable":0},{"_id":"source/pic/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-09-21%20%E4%B8%8A%E5%8D%888.24.29.png","path":"pic/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-09-21%20%E4%B8%8A%E5%8D%888.24.29.png","modified":1,"renderable":0},{"_id":"source/pic/101_03.png","path":"pic/101_03.png","modified":1,"renderable":0},{"_id":"source/pic/bp-02.png","path":"pic/bp-02.png","modified":1,"renderable":0},{"_id":"source/pic/bp-04.png","path":"pic/bp-04.png","modified":1,"renderable":0},{"_id":"source/pic/bp-05.png","path":"pic/bp-05.png","modified":1,"renderable":0},{"_id":"source/pic/xyz2.png","path":"pic/xyz2.png","modified":1,"renderable":0},{"_id":"source/pic/xyz.png","path":"pic/xyz.png","modified":1,"renderable":0},{"_id":"source/pic/bp-03.png","path":"pic/bp-03.png","modified":1,"renderable":0},{"_id":"source/pic/bp-01.png","path":"pic/bp-01.png","modified":1,"renderable":0},{"_id":"source/pic/skip_gram.npg.png","path":"pic/skip_gram.npg.png","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32-bulb.png","path":"images/favicon-32-bulb.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-180-bulb-precomposed.png","path":"images/favicon-180-bulb-precomposed.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-36-bulb.png","path":"images/favicon-36-bulb.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"source/pic/101_02.png","path":"pic/101_02.png","modified":1,"renderable":0},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1529912930831},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1529912930831},{"_id":"themes/next/.DS_Store","hash":"10cffa2be29caa7183da61355ba3682088e071b1","modified":1538947447441},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1529912930831},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1529912930834},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1529912930834},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1529912930833},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1529912930834},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1529912930834},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1529912930834},{"_id":"themes/next/README.cn.md","hash":"2c766b3369ed477bce134a5450dab45bef161504","modified":1529912930835},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1529912930834},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1529912930836},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1529912930859},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1529912930837},{"_id":"themes/next/_config.yml","hash":"5996c63a6cc384a1cf7174aee636922f5a08a657","modified":1538947646910},{"_id":"themes/next/README.md","hash":"8ce60ce578963eb4e1eb5e33e1efc2fc4779af9c","modified":1529912930835},{"_id":"source/.DS_Store","hash":"6aa45b338d52303c3246b1eba6bf2b7373a6bbba","modified":1541675798776},{"_id":"source/CNAME","hash":"6eedc0d1319e596910dca020a4ac8a00d904e1fd","modified":1537008380457},{"_id":"source/index.md","hash":"aa1e88aa8a9d3601f766efb7a017e9612bc8ff0f","modified":1541677982716},{"_id":"themes/next/.git/config","hash":"bf7d1df65cf34d0f25a7184a58c37a09f72e4be7","modified":1529912930821},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1529912930818},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1529912414098},{"_id":"themes/next/.git/index","hash":"28a091314d8c961adcfa3363b32ddc6de3516cef","modified":1529912930931},{"_id":"themes/next/.git/packed-refs","hash":"69237944e31c16fe545d1f47b0b1e5b1d99660da","modified":1529912930816},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1529912930832},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"50d48c47162817a3810a9d9ad51104e83947419a","modified":1529912930832},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1529912930832},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1529912930833},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1529912930860},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1529912930860},{"_id":"themes/next/source/.DS_Store","hash":"55d4ced0c097faae03a2c32596409ab52c04852c","modified":1538947204747},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1529912930837},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1529912930837},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1529912930837},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1529912930838},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1529912930839},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1529912930840},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1529912930841},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1529912930840},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1529912930841},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1529912930841},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1529912930841},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1529912930841},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1529912930842},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1529912930842},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1529912930842},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1529912930843},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1529912930858},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1529912930858},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1529912930858},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1529912930859},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1529912930859},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1529912930858},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1529912930859},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1529912930929},{"_id":"source/_posts/.DS_Store","hash":"4c71ee9a1ec710acbad68bb13d98543c2159f107","modified":1537739631021},{"_id":"source/_posts/Backward-propagation-of-Neural-Network-explained.md","hash":"af44586e1f4d7f43a00ef5d38cd5f1511473d8d8","modified":1541678323800},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1529912930929},{"_id":"source/_drafts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1537536691201},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1529912930929},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1529912930837},{"_id":"source/_posts/Basics-of-words-embedding.md","hash":"14e8adc34dae8b48d0c446033b065368b48b3ca2","modified":1538688684832},{"_id":"source/_posts/Pandas-Handle-missing-data.md","hash":"070315bdc74f1ed03ebcfe5ecc342bb8a8b80e87","modified":1537740939816},{"_id":"source/_posts/Pandas-How-to-calculate-datetime-difference-in-years.md","hash":"e00b560a6f8e34b71b6da40f310d8a241e00b225","modified":1540223842627},{"_id":"source/_posts/Pandas-How-to-import-CSV.md","hash":"0ac62a5530f9d5b5ddafb0910c442cce9693eeb7","modified":1537115830211},{"_id":"source/_posts/Pandas-How-to-import-from-Sql-Server.md","hash":"82c0c47fc19bcaa7475dc07dfff5152c9972968c","modified":1539890010526},{"_id":"source/_posts/Pandas-How-to-drop-columns-rows.md","hash":"eff50c2b4f5e33ddc71179c35aabac7c51acff4c","modified":1537303758318},{"_id":"source/_posts/Pandas-How-to-list-all-columns.md","hash":"378e766ab32ac48d4271abe6f116bebc639efe8c","modified":1539984919314},{"_id":"source/_posts/Pandas-How-to-rename-columns.md","hash":"48a834f97fcdf283ebc54665266e40b072cf0162","modified":1537221817104},{"_id":"source/_posts/Pandas-How-to-plot-counts-of-each-value.md","hash":"aec38f3e4945b021bd83c9c9ab5f04c152bec05e","modified":1540239486968},{"_id":"source/_posts/Pandas-How-to-select-data.md","hash":"bebdc8e5be09646db7e60500467bac3425416fe4","modified":1537519895544},{"_id":"source/_posts/What-is-tf-data-and-how-to-use.md","hash":"506f4d2f07fbce9af0037ddae6554f873dc5978e","modified":1538078375482},{"_id":"source/_posts/What-is-Attention-and-how-to-use.md","hash":"2b1d3b8348032b2c7bf528b561e6b2dae2f0eb36","modified":1537536562423},{"_id":"source/_posts/Tensorflow101.md","hash":"1ea6307882ba40bc6942cbb43f14727cd55b60c7","modified":1537988972298},{"_id":"source/_posts/how-to-debug-python-with-vscode.md","hash":"9c8fbd1ded90b1c9775a2baa2ee878060dcdf1b8","modified":1539118235236},{"_id":"source/categories/index.md","hash":"b496eb670b43480619d1e82a3da099a833bc6904","modified":1536948290422},{"_id":"source/pic/101_01.png","hash":"c40451fc96899a93700dec4a8e7491a48cdc9882","modified":1531232869200},{"_id":"source/pic/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-09-21%20%E4%B8%8A%E5%8D%888.24.29.png","hash":"f745219c78e3b53960779086fbf7ffd2e65743c2","modified":1537514674280},{"_id":"source/pic/101_03.png","hash":"90c9b543ce96187170605f5243e1b3d0be080a05","modified":1531231162720},{"_id":"source/pic/bp-02.png","hash":"ac11f640cd091a3f3c75dd05223e6ccf3d9dd1e6","modified":1541674849524},{"_id":"source/pic/bp-04.png","hash":"4bbb32294d3be95e02b389e34ab7c06d72d7089d","modified":1541628084774},{"_id":"source/pic/bp-05.png","hash":"147d180d7db899c6117d0d7987e05f5feb8d261d","modified":1541629794839},{"_id":"source/pic/xyz2.png","hash":"f01af15dc2dbb701af57a0dc48887db5e141612a","modified":1540239451609},{"_id":"source/pic/xyz.png","hash":"b48898f44b89c3c512470ea68b1720febc5780d8","modified":1540238472772},{"_id":"source/pic/bp-03.png","hash":"8dcb46433724aad964f3e1f10f86468ebe7a0dd6","modified":1541674951696},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529912930886},{"_id":"source/pic/bp-01.png","hash":"f0f13854bab8c93232aaff8c2e842f780978d01d","modified":1541627589166},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1529912414098},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1529912414100},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1529912414099},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1529912414101},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1529912414102},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1529912414102},{"_id":"source/pic/skip_gram.npg.png","hash":"2044dda933984dc87a0f2590420666f45b17a41f","modified":1538685088146},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1529912414100},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1529912414099},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1529912414102},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1529912414101},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1529912414101},{"_id":"themes/next/.git/logs/HEAD","hash":"f3450f01c4a26e1b74983f82d691ef53c6b8c020","modified":1529912930820},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1529912930860},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1529912930861},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1529912930861},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1529912930861},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1529912930861},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1529912930861},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1529912930862},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1529912930862},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1529912930862},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1529912930886},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1529912930886},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1529912930886},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1529912930887},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1529912930887},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1529912930887},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1529912930887},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1529912930887},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1529912930888},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1529912930888},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1529912930888},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1529912930888},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1529912930888},{"_id":"themes/next/source/images/favicon-32-bulb.png","hash":"8ca96c6a9325cff9a80a67e6162dbc90e8606729","modified":1538947124270},{"_id":"themes/next/source/images/favicon-180-bulb-precomposed.png","hash":"c3a4fd25a1df38efee6ceff47557115db5617701","modified":1538947124412},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1529912930889},{"_id":"themes/next/source/images/favicon-36-bulb.png","hash":"64a42fb8ab4e5099122e741bae334b91968b7bc8","modified":1538947124452},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1529912930889},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1529912930889},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1529912930889},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1529912930889},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1529912930889},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1529912930842},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1529912930842},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1529912930848},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1529912930849},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1529912930850},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1529912930855},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1529912930855},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1529912930855},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1529912930855},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1529912930855},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1529912930856},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1529912930856},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1529912930843},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1529912930843},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1529912930843},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1529912930844},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1529912930844},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1529912930844},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1529912930843},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"6a54c3c85ff6b19d275827a327abbf4bd99b2ebf","modified":1529912930844},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1529912930846},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1529912930845},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1529912930846},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1529912930846},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1529912930845},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529912930885},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529912930886},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529912930879},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529912930879},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529912930879},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529912930849},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1529912930850},{"_id":"themes/next/.git/refs/heads/master","hash":"7999da428ebb87e5a2b27315d8d5123c1ccdfaa5","modified":1529912930819},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1529912930885},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1529912930885},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1529912930885},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1529912930886},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1529912930879},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1529912930879},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"d4714368b5ee3b9e6e42f81ce4c1fc8fa6a43f7c","modified":1538945173365},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1529912930879},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1529912930899},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1529912930899},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1529912930895},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1529912930899},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1529912930899},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1529912930917},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1529912930918},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1529912930918},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1529912930918},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1529912930918},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1529912930918},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1529912930918},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1529912930903},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1529912930919},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1529912930903},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1529912930904},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1529912930919},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1529912930920},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1529912930919},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1529912930920},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1529912930904},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1529912930920},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1529912930920},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1529912930920},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1529912930920},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1529912930921},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1529912930921},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1529912930921},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1529912930921},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1529912930921},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1529912930922},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1529912930921},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1529912930922},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1529912930890},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1529912930890},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1529912930890},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1529912930890},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1529912930890},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1529912930891},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1529912930891},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1529912930890},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1529912930905},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1529912930891},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1529912930891},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1529912930905},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1529912930905},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1529912930891},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1529912930906},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1529912930906},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1529912930926},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1529912930927},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1529912930928},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1529912930929},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1529912930929},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1529912930922},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1529912930922},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1529912930923},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1529912930849},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1529912930849},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1529912930850},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1529912930850},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1529912930850},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1529912930851},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1529912930851},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1529912930851},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1529912930852},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1529912930851},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1529912930852},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1529912930852},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1529912930852},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1529912930852},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1529912930853},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1529912930853},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1529912930853},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1529912930853},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1529912930854},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1529912930854},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1529912930854},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1529912930854},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1529912930854},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1529912930854},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1529912930855},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1529912930857},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1529912930857},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1529912930858},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1529912930857},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1529912930845},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1529912930845},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1529912930846},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1529912930847},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1529912930846},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1529912930847},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1529912930848},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1529912930848},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1529912930848},{"_id":"source/pic/101_02.png","hash":"d62b79f4effd83ea240bb1161a1e84bc99de6a74","modified":1531231201793},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1529912930917},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"f3450f01c4a26e1b74983f82d691ef53c6b8c020","modified":1529912930820},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1529912930818},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1529912930880},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1529912930880},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1529912930881},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1529912930881},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1529912930882},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1529912930881},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1529912930880},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1529912930882},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1529912930883},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1529912930883},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4aac01962520d60b03b23022ab601ad4bd19c08c","modified":1529912930883},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1529912930883},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1529912930883},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1529912930884},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1529912930884},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1529912930884},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1529912930885},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1529912930884},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1529912930885},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1529912930894},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1529912930895},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1529912930895},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1529912930863},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1529912930863},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1529912930863},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1529912930864},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1529912930863},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1529912930868},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1529912930873},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1529912930877},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1529912930878},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1529912930877},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1529912930877},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1529912930878},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1529912930878},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1529912930900},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1529912930878},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1529912930900},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1529912930901},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1529912930900},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1529912930900},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1529912930901},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1529912930902},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1529912930903},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1529912930903},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1529912930905},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1529912930904},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1529912930926},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1529912930926},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1529912930891},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1529912930906},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1529912930906},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1529912930907},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1529912930856},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1529912930856},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1529912930894},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1529912930928},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1529912930916},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1529912930916},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"f3450f01c4a26e1b74983f82d691ef53c6b8c020","modified":1529912930818},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1529912930882},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1529912930882},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1529912930884},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1529912930893},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1529912930893},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1529912930893},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1529912930894},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1529912930893},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1529912930864},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1529912930864},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1529912930864},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1529912930865},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1529912930864},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1529912930865},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1529912930866},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1529912930867},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1529912930867},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1529912930867},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1529912930867},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1529912930866},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1529912930866},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1529912930866},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1529912930868},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1529912930868},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1529912930868},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1529912930868},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1529912930869},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1529912930869},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1529912930869},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1529912930869},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1529912930869},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1529912930870},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1529912930870},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1529912930870},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1529912930870},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1529912930870},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1529912930870},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1529912930873},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1529912930873},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1529912930873},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1529912930873},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1529912930873},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1529912930874},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1529912930874},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1529912930874},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1529912930871},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1529912930871},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1529912930871},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1529912930871},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1529912930871},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1529912930871},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1529912930872},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1529912930872},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1529912930872},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1529912930872},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1529912930875},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1529912930875},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1529912930875},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1529912930876},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1529912930875},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1529912930876},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1529912930876},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1529912930876},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1529912930877},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1529912930876},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1529912930874},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1529912930901},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1529912930901},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1529912930901},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1529912930902},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1529912930902},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1529912930902},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1529912930908},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1529912930910},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1529912930914},{"_id":"themes/next/.git/objects/pack/pack-88df782ce6fe81241cc1f463a32af916d4193f78.idx","hash":"bb5ce2ffd2b23aeab4e99d217116d616aeac700b","modified":1529912930793},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1529912930898},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1529912930925},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1529912930913},{"_id":"themes/next/.git/objects/pack/pack-88df782ce6fe81241cc1f463a32af916d4193f78.pack","hash":"66da6ac57fd465b0b8f43f6be7c446eeee84eb9f","modified":1529912930787}],"Category":[{"name":"Machine Learning","_id":"cjo8jh3bt0004m2tqgyht698y"},{"name":"pandas","_id":"cjo8jh3c0000bm2tqaidnyd2i"},{"name":"Pandas","_id":"cjo8jh3c2000gm2tq0caa0gmg"},{"name":"tensorflow","_id":"cjo8jh3ca000xm2tq5335ckop"},{"name":"tools","_id":"cjo8jh3cb0010m2tqzejcidy7"}],"Data":[],"Page":[{"title":"Notes on Data Science","date":"2018-09-21T14:50:40.000Z","tags":null,"_content":"\n### Machine Learning\n<!-- * [Backward propagation of Neural Network explained](/Machine-Learning/Backward-propagation-of-Neural-Network-explained/)\n -->* [What is Attention and how to use](/Machine-Learning/What-is-Attention-and-how-to-use/)\n* [Basics of words embedding](/Machine-Learning/Basics-of-words-embedding/)\n\n--------\n\n### Pandas How-to\n\n* [[Pandas]How to select data](/pandas/Pandas-How-to-select-data/)\n* [[Pandas]How to import CSV](/pandas/Pandas-How-to-import-CSV/)\n* [[Pandas]How to rename columns](/pandas/Pandas-How-to-rename-columns/)\n* [[Pandas]How to drop columns rows](/pandas/Pandas-How-to-drop-columns-rows/)\n* [[Pandas]Handle missing data](/pandas/Pandas-Handle-missing-data/)\n* [[Pandas]How to import from Sql Server](/pandas/Pandas-How-to-import-from-Sql-Server/)\n* [[Pandas]How to list all columns](/pandas/Pandas-How-to-list-all-columns/)\n* [[Pandas]How to calculate datetime difference in years](/pandas/Pandas-How-to-calculate-datetime-difference-in-years/)\n* [[Pandas]How to plot counts of each value](/pandas/Pandas-How-to-plot-counts-of-each-value/)\n\n---\n\n### Tensorflow How-to\n\n* [Tensorflow101](/tensorflow/Tensorflow101/)\n* [What is tf.data and how to use](/tensorflow/What-is-tf-data-and-how-to-use/)\n\n","source":"index.md","raw":"---\ntitle: Notes on Data Science\ndate: 2018-09-21 14:50:40\ntags:\n---\n\n### Machine Learning\n<!-- * [Backward propagation of Neural Network explained](/Machine-Learning/Backward-propagation-of-Neural-Network-explained/)\n -->* [What is Attention and how to use](/Machine-Learning/What-is-Attention-and-how-to-use/)\n* [Basics of words embedding](/Machine-Learning/Basics-of-words-embedding/)\n\n--------\n\n### Pandas How-to\n\n* [[Pandas]How to select data](/pandas/Pandas-How-to-select-data/)\n* [[Pandas]How to import CSV](/pandas/Pandas-How-to-import-CSV/)\n* [[Pandas]How to rename columns](/pandas/Pandas-How-to-rename-columns/)\n* [[Pandas]How to drop columns rows](/pandas/Pandas-How-to-drop-columns-rows/)\n* [[Pandas]Handle missing data](/pandas/Pandas-Handle-missing-data/)\n* [[Pandas]How to import from Sql Server](/pandas/Pandas-How-to-import-from-Sql-Server/)\n* [[Pandas]How to list all columns](/pandas/Pandas-How-to-list-all-columns/)\n* [[Pandas]How to calculate datetime difference in years](/pandas/Pandas-How-to-calculate-datetime-difference-in-years/)\n* [[Pandas]How to plot counts of each value](/pandas/Pandas-How-to-plot-counts-of-each-value/)\n\n---\n\n### Tensorflow How-to\n\n* [Tensorflow101](/tensorflow/Tensorflow101/)\n* [What is tf.data and how to use](/tensorflow/What-is-tf-data-and-how-to-use/)\n\n","updated":"2018-11-08T11:53:02.716Z","path":"index.html","comments":1,"layout":"page","_id":"cjo8jh36j0000m2tqneqvop8q","content":"<h3 id=\"Machine-Learning\"><a href=\"#Machine-Learning\" class=\"headerlink\" title=\"Machine Learning\"></a>Machine Learning</h3><p><!-- * [Backward propagation of Neural Network explained](/Machine-Learning/Backward-propagation-of-Neural-Network-explained/)\n -->* <a href=\"/Machine-Learning/What-is-Attention-and-how-to-use/\">What is Attention and how to use</a></p>\n<ul>\n<li><a href=\"/Machine-Learning/Basics-of-words-embedding/\">Basics of words embedding</a></li>\n</ul>\n<hr>\n<h3 id=\"Pandas-How-to\"><a href=\"#Pandas-How-to\" class=\"headerlink\" title=\"Pandas How-to\"></a>Pandas How-to</h3><ul>\n<li><a href=\"/pandas/Pandas-How-to-select-data/\">[Pandas]How to select data</a></li>\n<li><a href=\"/pandas/Pandas-How-to-import-CSV/\">[Pandas]How to import CSV</a></li>\n<li><a href=\"/pandas/Pandas-How-to-rename-columns/\">[Pandas]How to rename columns</a></li>\n<li><a href=\"/pandas/Pandas-How-to-drop-columns-rows/\">[Pandas]How to drop columns rows</a></li>\n<li><a href=\"/pandas/Pandas-Handle-missing-data/\">[Pandas]Handle missing data</a></li>\n<li><a href=\"/pandas/Pandas-How-to-import-from-Sql-Server/\">[Pandas]How to import from Sql Server</a></li>\n<li><a href=\"/pandas/Pandas-How-to-list-all-columns/\">[Pandas]How to list all columns</a></li>\n<li><a href=\"/pandas/Pandas-How-to-calculate-datetime-difference-in-years/\">[Pandas]How to calculate datetime difference in years</a></li>\n<li><a href=\"/pandas/Pandas-How-to-plot-counts-of-each-value/\">[Pandas]How to plot counts of each value</a></li>\n</ul>\n<hr>\n<h3 id=\"Tensorflow-How-to\"><a href=\"#Tensorflow-How-to\" class=\"headerlink\" title=\"Tensorflow How-to\"></a>Tensorflow How-to</h3><ul>\n<li><a href=\"/tensorflow/Tensorflow101/\">Tensorflow101</a></li>\n<li><a href=\"/tensorflow/What-is-tf-data-and-how-to-use/\">What is tf.data and how to use</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Machine-Learning\"><a href=\"#Machine-Learning\" class=\"headerlink\" title=\"Machine Learning\"></a>Machine Learning</h3><p><!-- * [Backward propagation of Neural Network explained](/Machine-Learning/Backward-propagation-of-Neural-Network-explained/)\n -->* <a href=\"/Machine-Learning/What-is-Attention-and-how-to-use/\">What is Attention and how to use</a></p>\n<ul>\n<li><a href=\"/Machine-Learning/Basics-of-words-embedding/\">Basics of words embedding</a></li>\n</ul>\n<hr>\n<h3 id=\"Pandas-How-to\"><a href=\"#Pandas-How-to\" class=\"headerlink\" title=\"Pandas How-to\"></a>Pandas How-to</h3><ul>\n<li><a href=\"/pandas/Pandas-How-to-select-data/\">[Pandas]How to select data</a></li>\n<li><a href=\"/pandas/Pandas-How-to-import-CSV/\">[Pandas]How to import CSV</a></li>\n<li><a href=\"/pandas/Pandas-How-to-rename-columns/\">[Pandas]How to rename columns</a></li>\n<li><a href=\"/pandas/Pandas-How-to-drop-columns-rows/\">[Pandas]How to drop columns rows</a></li>\n<li><a href=\"/pandas/Pandas-Handle-missing-data/\">[Pandas]Handle missing data</a></li>\n<li><a href=\"/pandas/Pandas-How-to-import-from-Sql-Server/\">[Pandas]How to import from Sql Server</a></li>\n<li><a href=\"/pandas/Pandas-How-to-list-all-columns/\">[Pandas]How to list all columns</a></li>\n<li><a href=\"/pandas/Pandas-How-to-calculate-datetime-difference-in-years/\">[Pandas]How to calculate datetime difference in years</a></li>\n<li><a href=\"/pandas/Pandas-How-to-plot-counts-of-each-value/\">[Pandas]How to plot counts of each value</a></li>\n</ul>\n<hr>\n<h3 id=\"Tensorflow-How-to\"><a href=\"#Tensorflow-How-to\" class=\"headerlink\" title=\"Tensorflow How-to\"></a>Tensorflow How-to</h3><ul>\n<li><a href=\"/tensorflow/Tensorflow101/\">Tensorflow101</a></li>\n<li><a href=\"/tensorflow/What-is-tf-data-and-how-to-use/\">What is tf.data and how to use</a></li>\n</ul>\n"},{"title":"categories","date":"2018-09-14T19:04:32.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-09-14 19:04:32\ntype: \"categories\"\n---\n","updated":"2018-09-14T18:04:50.422Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjo8jh3bq0002m2tqmtxlmfs3","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Backward propagation of Neural Network explained","date":"2018-11-08T11:14:28.000Z","mathjax":true,"_content":"\n\n# Backward propagation of Neural Network explained\n\nBackpropagation is the foundation of the deep neural network. Usually, we consider it to be kind of 'dark magic' we are not able to understand. However, it should not be the black box which we stay away. In this article, I will try to explain backpropagation as well as the whole neural network step by step in the original mathematical way.\n\n## Outline\n\n* Overview of the architecture\n* Initialize parameters\n* Implement forward propagation\n* Compute Loss\n* Implement Backward propagation\n* Update parameters\n\n## 1. The architecture\n\nThis neural network I'm going to explain is a 2-Layer neural network. The first layer is Linear + Sigmoid, and the second Layer is Linear + Softmax. \n\n![image-20181107215309162](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-01.png)\n\nThe architecture in the math formula\n$$\nf(X)= Relu( Sigmoid(  X \\times W_{ih} +b_{1}) \\times W_{ho} + b_{2})\n$$\n\n##2.Initialize parameters\n\nWe take one example which has two features like below\n$$\nX= [x1, x2]= [0.1, 0.2]\n$$\nThe parameters are taken randomly. \n$$\nW_{is}=\n\n\\begin{bmatrix}\nW_{i1h1} & W_{i1h2}\\\\\nW_{i2h1} & W_{i2h2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.7 & 0.6\\\\\n0.5 & 0.4\n\\end{bmatrix}\n$$\n\n$$\nW_{sr}=\n\\begin{bmatrix}\nW_{h1o1}&W_{h1o2}\\\\\nW_{h2o1}&W_{h2o2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.4&0.6\\\\\n0.9&0.8\n\\end{bmatrix}\n$$\n\n$$\nb_{1}=[b_{11}, b_{12}] =[0.5,0.6]\n$$\n\n$$\nb_{2}= [b_{21}, b_{22}] \n= [0.7, 0.9]\n$$\n\n\n\n## 3. Forward Propagation\n\n### 3.1 Layer1:\n\n![image-20181108110049524](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-02.png)\n\n### Linear \n\n$$\nX \\times  W_{ih}  + b_{1}\n=\n[x1, x2]\n\\times\n\\begin{bmatrix}\nW_{i1h1} & W_{i1h2}\\\\\nW_{i2h1} & W_{i2h2}\n\\end{bmatrix}\n+\n[b_{11}, b_{12}]\n=\n[0.1, 0.2]\n\\times\n\\begin{bmatrix}\n0.7 & 0.6\\\\\n0.5 & 0.4\n\\end{bmatrix}\n+\n[0.5, 0.6]\n=\n[0.67, 0.74]\n$$\n\n### Sigmoid\n\n$$\nSigmoid(x)=1/(1+e^{-x})\n$$\n\n$$\n[h_{out1},h_{out2}]\n=\nSigmoid(X \\times W_{ih} + b_{1}) =\n[Sigmoid(0.67),Sigmoid(0.74)]\n=\n[0.6615,0.6770]\n$$\n\n### 3.2 Layer2:\n\n![image-20181108110231696](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-03.png)\n\n### Linear\n\n$$\n[o_{in1}, o_{in2}]\n=\n[h_{out1}, h_{out2}] \\times W_{ho}  + b_{2}\n=\n[h_{out1}, h_{out2}]\n\\times\n\\begin{bmatrix}\nW_{h1o1} & W_{h1o2}\\\\\nW_{h2o1} & W_{h2o2}\n\\end{bmatrix}\n+\n[b_{21}, b_{22}]\n=\\\\\n[0.6615, 0.6770]\n\\times\n\\begin{bmatrix}\n0.7 & 0.5\\\\\n0.6 & 0.4\n\\end{bmatrix}\n+\n[0.5, 0.6]\n=\n\n[1.3693, 1.0391]\n$$\n\n### Softmax\n\n$$\nSoftmax(x)_{j}= \\frac{e^{xj}}{\\sum_{i=1}^n e^{xi}} (j=1,2...n)\n$$\n\n$$\n[o_{out1},o_{out2}]\n=\nSoftmax(X\\times W_{ho} + b_{2})=\n[\n\\frac{e^{1.3693}}{e^{1.3693} +e^{1.0391}} ,\n\\frac{e^{1.0391}}{e^{1.3693} +e^{1.0391}} \n]\n=\n[0.5818,0.4182]\n$$\n\n##4. Compute Loss\n\nThe Loss function here we use is cross-entropy cost\n$$\nCrossentropy= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))\n$$\nThe actual output should be \n$$\n[y_{1},y_{2}] \n=\n[1.00, 0.00]\n$$\nSince we only have one example, that means  'm = 1',  the total loss is computed as follows :\n$$\n-\\frac{1}{1} \\sum\\limits_{i = 1}^{1} (y_{1}\\log\\left(o_{out1}\\right) + 0 +0 + 1*log(1-o_{out2}))=\n-(1*log(0.5818)+0+0+1*log(1-0.4182))==0.4704\n$$\n\n## 5. Backward Propagation\n\nIn this section, we will go through backward propagation stage by stage.\n\n### 5.1 Basic Derivatives\n\n####Sigmoid:\n\n$$\n\\frac{\\partial Sigmoid(x)}{\\partial x}\n=\n\\frac{\\partial \\frac{1}{(1+e^{-x})}}{\\partial x}\n=\n\\frac{ e^{-x}}{(1+e^{-x})^2}\n=\n(\\frac{1+e^{-x}-1}{1+e^{-x}})\\frac{1}{1+e^{-x}}\n=\n(1 - Sigmoid(x))\\times Sigmoid(x)\n$$\n\n####Softmax:\n\nAt first we know:\n\nFor \n$$\nf(x)=\\frac{g(x)}{h(x)}\n$$\n\n$$\nf'(x) = \\frac{g'(x)h(x)-g(x)h'(x)}{[h(x)]^2}\n$$\n\nThen the derivation of Softmax is \n$$\n\\frac{\\partial Softmax(x)}{\\partial x1}=\\frac{e^{x1}(e^{x1}+e^{x2})-e^{x1}e^{x1} }{(e^{x1}+e^{x2})^2} = \\frac{e^{x1+x2}}{(e^{x1}+e^{x2})^2}\n$$\n\n###5.2 The backward Pass\n\n#### 5.2.1 Layer1-Layer2\n\n####Weight derivatives with respect to the error\n\n![image-20181107220124773](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-04.png)\n\nConsider W<sub>ho</sub> , we want to know how W<sub>ho</sub> will affect the total error, aka the value of \n$$\n\\frac{\\partial E_{total}}{\\partial W_{ho}}\n$$\n[Chain Rule](https://en.wikipedia.org/wiki/Chain_rule) states that:\n$$\n\\frac{\\partial z}{\\partial x} =\n\\frac{\\partial z}{\\partial y}\\cdot\\frac{\\partial y}{\\partial x}\n$$\nSo we have\n$$\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}}=\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{int1}} \n\\cdot \\frac{\\partial o_{int1}}{\\partial W_{h2o1}}\n$$\nLet's break this through stage by stage\n\n* Stage1\n\n$$\n\\frac{\\partial E_{total}}{\\partial o_{out1}} = \n\\frac{\\partial (-(y_{1}*log(o_{out1})+(1-y_{1})*log(1-o_{out1})))}{\\partial o_{out1}}+0\n=-\\frac{1}{o_{out1}}=-1/0.5818=-1.719\n$$\n\n* Stage2\n\n$$\n\\frac{\\partial o_{1}}{\\partial i_{1}}=\n\\frac{\\partial Softmax(i_{1})}{\\partial i_{1}}\n=\n\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}\n=\n\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}\n=\n\\frac{e^{1.3693}\\cdot e^{1.0391}}{(e^{1.3693}+e^{1.0391})^2}=0.2433\n$$\n\n* Stage3 \n\n$$\n\\frac{\\partial o_{in1}}{\\partial W_{h2o1}} = \n\\frac{\\partial (h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1}+b_{21})}{\\partial W_{h2o1}}\n= h_{out2}=0.6770\n$$\n\nFinally we apply the chain rule:\n$$\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}}= -1.719 * 0.2433 * 0.677 = -0.2831\n$$\nLet's go through all the weights in Layer2\n$$\nW_{ho}'=\n\\begin{bmatrix}\nW_{h1o1}' & W_{h1o2}'\\\\\nW_{h2o1}' & W_{h2o2}'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial W_{h1o1}} & \\frac{\\partial E_{total}}{\\partial W_{h1o2}} \\\\\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}} & \\frac{\\partial E_{total}}{\\partial W_{h2o2}} \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \n\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h1o1}}\n&\n\\frac{\\partial E_{total}}{\\partial o_{out2}}\n\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \n\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h1o2}}\n\\\\\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \n\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h2o1}} \n&\n\\frac{\\partial E_{total}}{\\partial o_{out2}}\n\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \n\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h2o2}}\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-1.719*0.2433*0.6615 & -2.3912*0.2433*0.6615 \\\\\n-1.719*0.2433*0.6770 & -2.3912*0.2433*0.6770\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-0.2767 & -0.3733\\\\\n-0.2831 & -0.3853\n\\end{bmatrix}\n$$\n\n#### Update weights according to learning rate\n\nOur training target is to make the prediction value approximate the correct value, while it can be transferred to minimize the error by updating weights with the help of learning rate. Suppose the learning rate is 0.02.\n\nWe got the updated weight matrix as folows\n$$\nW_{ho}^* =\n\\begin{bmatrix}\nW_{h1o1} - \\eta W_{h1o1}' & W_{h1o2} - \\eta W_{h1o2}'\\\\\nW_{h2o1} - \\eta W_{h2o1}' & W_{h2o2} - \\eta W_{h2o2}'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.4 - 0.02*(-0.2767)&0.6 - 0.02*(-0.3733)\\\\\n0.9 - 0.02*(-0.2831)&0.8 - 0.02*(-0.3853)\n\\end{bmatrix}\\\\\n=\n\\begin{bmatrix}\n0.4055 & 0.6075\\\\\n0.9057 & 0.8077\n\\end{bmatrix}\n$$\nThat is the updated weight of Layer1-Layer2. The update of Input-Layer weights is the same story I will illustrate as follows.\n\n####5.2.2 Layer0(Input Layer) - Layer1 \n\n![image-20181107222954838](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-05.png)\n\nFollow the path of the previous chapter\n\n* Stage1:\n\n$$\n\\frac{\\partial h_{out1}}{\\partial h_{in1}}\n=\n\\frac{\\partial Sigmoid(h_{in1})}{\\partial h_{in1}}\n=\nSigmoid(h_{in1})*(1-Sigmoid(h_{in1}))\n=\\\\\nSigmoid(0.67)*(1-Sigmoid(0.67))=0.2239\n$$\n\n* Stage2:\n\n$$\n\\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n=\n\\frac{\\partial(x1 * W_{i1h1} + x2 * W_{i2h1} + b11)}{\\partial W_{i2h1}}\n=\nx2=0.2\n$$\n\nApply the chain rule:\n$$\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}}=\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \n\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n$$\nWe already got the second and third derivations, regarding the first derivation, we apply the chain rule again, but in the opposite direction.\n$$\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n= \n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\frac{\\partial o_{out1}}{\\partial o_{in1}}\n\\frac{\\partial o_{in1}}{\\partial h1_{out}}\n$$\nWe have computed the first and second results, and the third one is merely a deviation of the linear function\n$$\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n= \n-1.719*0.2433* \n\\frac{\\partial(h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1})}{\\partial h_{out1}}\n=\\\\\n-1.719*0.2433*W_{h1o1}\n=-1.719*0.2433*0.4\n=-0.1673\n$$\nThen we got\n$$\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}}= -0.1673*0.2239 * 0.2=-0.0075\n$$\nSimilarly , we can get the Layer0-Layer1 derivatives with respective to the total error\n$$\nW_{ih}'\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial W_{i1h1}} & \\frac{\\partial E_{total}}{\\partial W_{i1h2}}\\\\\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}} & \\frac{\\partial E_{total}}{\\partial W_{i2h2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h1_{out}}{\\partial h1_{in}} \n\\cdot \\frac{\\partial h1_{in}}{\\partial W_{i1h1}}\n& \n\\frac{\\partial E_{total}}{\\partial h_{out2}}\n\\cdot \\frac{\\partial h2_{out}}{\\partial h2_{in}} \n\\cdot \\frac{\\partial h2_{in}}{\\partial W_{i1h2}}\n\\\\\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \n\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n& \n\\frac{\\partial E_{total}}{\\partial h_{out2}}\n\\cdot \\frac{\\partial h_{out2}}{\\partial h_{in2}} \n\\cdot \\frac{\\partial h_{in2}}{\\partial W_{i2h2}}\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-0.1673*0.2239 * 0.1 &  -0.4654*0.2187*0.1\\\\\n-0.1673*0.2239 * 0.2 &  -0.4654*0.2187*0.2\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-0.0037 &  -0.0102\\\\\n-0.0075 &  -0.0204\n\\end{bmatrix}\n$$\n\n#### Update weights according to learning rate\n\nUpdate the weights with learning rate 0.02we got the final weight matrix\n$$\nW_{ih}^*=\n\\begin{bmatrix}\nW_{i1h1} - \\eta (W_{i1h1}') &  W_{i1h2} - \\eta (W_{i1h2}')\\\\\nW_{i2h1} - \\eta (W_{i2h1}') &  W_{i2h2} - \\eta (W_{i2h2}')\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(0.7 -0.2*(-0.0037))& (0.6 - 0.2 *(-0.0102)) \\\\\n(0.5 - 0.2*(-0.0075)) & (0.4 - 0.2 * (-0.0204))\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n0.7007 & 0.6020\\\\\n0.5015 & 0.4041\n\\end{bmatrix}\n$$\n\n###5.3 Wrap up\n\nFinally we get all the weights updated\n$$\nW_{ho}^* = \n\\begin{bmatrix}\n0.4055 & 0.6075\\\\\n0.9057 & 0.8077\n\\end{bmatrix}\n$$\n\n$$\nW_{ih}^*=\n\\begin{bmatrix}\n0.7007 & 0.6020\\\\\n0.5015 & 0.4041\n\\end{bmatrix}\n$$\n\n## 6. Conclusion\n\n* Backpropagation is beautiful designed architecture. Every gate in the diagram gets some input and makes some output, the gradients of input concerning the output indicates how strongly the gate wants the output to increase or decrease. The communication between these \"smart\" gates make it possible for complicated prediction or classification tasks.\n* The activation function matters. Take Sigmoid as an example, and we saw the gradients of its gates \"vanish\" significantly to 0.00XXX, this will make the rest of backward pass almost to zero due to the multiplication in chain rule. So we should always be nervous in Sigmoid, Relu is possibly a better choice.\n* If we look back to the computing process,  a lot can be done when we implement the neural network with codes, such as the caching of gradients when we do forward propagation and the extracting of common gradient computation functions.\n\n## 7. Reference\n\n1. <https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c>.\n2. <https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/>.  \n3. <https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b>\n4. <http://cs231n.github.io/optimization-2/> I\n5. <https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/> \n6. <https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb>","source":"_posts/Backward-propagation-of-Neural-Network-explained.md","raw":"---\ntitle: Backward propagation of Neural Network explained \ndate: 2018-11-08 11:14:28\ncategories: 'Machine Learning'\nmathjax: true\n---\n\n\n# Backward propagation of Neural Network explained\n\nBackpropagation is the foundation of the deep neural network. Usually, we consider it to be kind of 'dark magic' we are not able to understand. However, it should not be the black box which we stay away. In this article, I will try to explain backpropagation as well as the whole neural network step by step in the original mathematical way.\n\n## Outline\n\n* Overview of the architecture\n* Initialize parameters\n* Implement forward propagation\n* Compute Loss\n* Implement Backward propagation\n* Update parameters\n\n## 1. The architecture\n\nThis neural network I'm going to explain is a 2-Layer neural network. The first layer is Linear + Sigmoid, and the second Layer is Linear + Softmax. \n\n![image-20181107215309162](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-01.png)\n\nThe architecture in the math formula\n$$\nf(X)= Relu( Sigmoid(  X \\times W_{ih} +b_{1}) \\times W_{ho} + b_{2})\n$$\n\n##2.Initialize parameters\n\nWe take one example which has two features like below\n$$\nX= [x1, x2]= [0.1, 0.2]\n$$\nThe parameters are taken randomly. \n$$\nW_{is}=\n\n\\begin{bmatrix}\nW_{i1h1} & W_{i1h2}\\\\\nW_{i2h1} & W_{i2h2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.7 & 0.6\\\\\n0.5 & 0.4\n\\end{bmatrix}\n$$\n\n$$\nW_{sr}=\n\\begin{bmatrix}\nW_{h1o1}&W_{h1o2}\\\\\nW_{h2o1}&W_{h2o2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.4&0.6\\\\\n0.9&0.8\n\\end{bmatrix}\n$$\n\n$$\nb_{1}=[b_{11}, b_{12}] =[0.5,0.6]\n$$\n\n$$\nb_{2}= [b_{21}, b_{22}] \n= [0.7, 0.9]\n$$\n\n\n\n## 3. Forward Propagation\n\n### 3.1 Layer1:\n\n![image-20181108110049524](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-02.png)\n\n### Linear \n\n$$\nX \\times  W_{ih}  + b_{1}\n=\n[x1, x2]\n\\times\n\\begin{bmatrix}\nW_{i1h1} & W_{i1h2}\\\\\nW_{i2h1} & W_{i2h2}\n\\end{bmatrix}\n+\n[b_{11}, b_{12}]\n=\n[0.1, 0.2]\n\\times\n\\begin{bmatrix}\n0.7 & 0.6\\\\\n0.5 & 0.4\n\\end{bmatrix}\n+\n[0.5, 0.6]\n=\n[0.67, 0.74]\n$$\n\n### Sigmoid\n\n$$\nSigmoid(x)=1/(1+e^{-x})\n$$\n\n$$\n[h_{out1},h_{out2}]\n=\nSigmoid(X \\times W_{ih} + b_{1}) =\n[Sigmoid(0.67),Sigmoid(0.74)]\n=\n[0.6615,0.6770]\n$$\n\n### 3.2 Layer2:\n\n![image-20181108110231696](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-03.png)\n\n### Linear\n\n$$\n[o_{in1}, o_{in2}]\n=\n[h_{out1}, h_{out2}] \\times W_{ho}  + b_{2}\n=\n[h_{out1}, h_{out2}]\n\\times\n\\begin{bmatrix}\nW_{h1o1} & W_{h1o2}\\\\\nW_{h2o1} & W_{h2o2}\n\\end{bmatrix}\n+\n[b_{21}, b_{22}]\n=\\\\\n[0.6615, 0.6770]\n\\times\n\\begin{bmatrix}\n0.7 & 0.5\\\\\n0.6 & 0.4\n\\end{bmatrix}\n+\n[0.5, 0.6]\n=\n\n[1.3693, 1.0391]\n$$\n\n### Softmax\n\n$$\nSoftmax(x)_{j}= \\frac{e^{xj}}{\\sum_{i=1}^n e^{xi}} (j=1,2...n)\n$$\n\n$$\n[o_{out1},o_{out2}]\n=\nSoftmax(X\\times W_{ho} + b_{2})=\n[\n\\frac{e^{1.3693}}{e^{1.3693} +e^{1.0391}} ,\n\\frac{e^{1.0391}}{e^{1.3693} +e^{1.0391}} \n]\n=\n[0.5818,0.4182]\n$$\n\n##4. Compute Loss\n\nThe Loss function here we use is cross-entropy cost\n$$\nCrossentropy= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))\n$$\nThe actual output should be \n$$\n[y_{1},y_{2}] \n=\n[1.00, 0.00]\n$$\nSince we only have one example, that means  'm = 1',  the total loss is computed as follows :\n$$\n-\\frac{1}{1} \\sum\\limits_{i = 1}^{1} (y_{1}\\log\\left(o_{out1}\\right) + 0 +0 + 1*log(1-o_{out2}))=\n-(1*log(0.5818)+0+0+1*log(1-0.4182))==0.4704\n$$\n\n## 5. Backward Propagation\n\nIn this section, we will go through backward propagation stage by stage.\n\n### 5.1 Basic Derivatives\n\n####Sigmoid:\n\n$$\n\\frac{\\partial Sigmoid(x)}{\\partial x}\n=\n\\frac{\\partial \\frac{1}{(1+e^{-x})}}{\\partial x}\n=\n\\frac{ e^{-x}}{(1+e^{-x})^2}\n=\n(\\frac{1+e^{-x}-1}{1+e^{-x}})\\frac{1}{1+e^{-x}}\n=\n(1 - Sigmoid(x))\\times Sigmoid(x)\n$$\n\n####Softmax:\n\nAt first we know:\n\nFor \n$$\nf(x)=\\frac{g(x)}{h(x)}\n$$\n\n$$\nf'(x) = \\frac{g'(x)h(x)-g(x)h'(x)}{[h(x)]^2}\n$$\n\nThen the derivation of Softmax is \n$$\n\\frac{\\partial Softmax(x)}{\\partial x1}=\\frac{e^{x1}(e^{x1}+e^{x2})-e^{x1}e^{x1} }{(e^{x1}+e^{x2})^2} = \\frac{e^{x1+x2}}{(e^{x1}+e^{x2})^2}\n$$\n\n###5.2 The backward Pass\n\n#### 5.2.1 Layer1-Layer2\n\n####Weight derivatives with respect to the error\n\n![image-20181107220124773](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-04.png)\n\nConsider W<sub>ho</sub> , we want to know how W<sub>ho</sub> will affect the total error, aka the value of \n$$\n\\frac{\\partial E_{total}}{\\partial W_{ho}}\n$$\n[Chain Rule](https://en.wikipedia.org/wiki/Chain_rule) states that:\n$$\n\\frac{\\partial z}{\\partial x} =\n\\frac{\\partial z}{\\partial y}\\cdot\\frac{\\partial y}{\\partial x}\n$$\nSo we have\n$$\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}}=\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{int1}} \n\\cdot \\frac{\\partial o_{int1}}{\\partial W_{h2o1}}\n$$\nLet's break this through stage by stage\n\n* Stage1\n\n$$\n\\frac{\\partial E_{total}}{\\partial o_{out1}} = \n\\frac{\\partial (-(y_{1}*log(o_{out1})+(1-y_{1})*log(1-o_{out1})))}{\\partial o_{out1}}+0\n=-\\frac{1}{o_{out1}}=-1/0.5818=-1.719\n$$\n\n* Stage2\n\n$$\n\\frac{\\partial o_{1}}{\\partial i_{1}}=\n\\frac{\\partial Softmax(i_{1})}{\\partial i_{1}}\n=\n\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}\n=\n\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}\n=\n\\frac{e^{1.3693}\\cdot e^{1.0391}}{(e^{1.3693}+e^{1.0391})^2}=0.2433\n$$\n\n* Stage3 \n\n$$\n\\frac{\\partial o_{in1}}{\\partial W_{h2o1}} = \n\\frac{\\partial (h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1}+b_{21})}{\\partial W_{h2o1}}\n= h_{out2}=0.6770\n$$\n\nFinally we apply the chain rule:\n$$\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}}= -1.719 * 0.2433 * 0.677 = -0.2831\n$$\nLet's go through all the weights in Layer2\n$$\nW_{ho}'=\n\\begin{bmatrix}\nW_{h1o1}' & W_{h1o2}'\\\\\nW_{h2o1}' & W_{h2o2}'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial W_{h1o1}} & \\frac{\\partial E_{total}}{\\partial W_{h1o2}} \\\\\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}} & \\frac{\\partial E_{total}}{\\partial W_{h2o2}} \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \n\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h1o1}}\n&\n\\frac{\\partial E_{total}}{\\partial o_{out2}}\n\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \n\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h1o2}}\n\\\\\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \n\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h2o1}} \n&\n\\frac{\\partial E_{total}}{\\partial o_{out2}}\n\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \n\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h2o2}}\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-1.719*0.2433*0.6615 & -2.3912*0.2433*0.6615 \\\\\n-1.719*0.2433*0.6770 & -2.3912*0.2433*0.6770\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-0.2767 & -0.3733\\\\\n-0.2831 & -0.3853\n\\end{bmatrix}\n$$\n\n#### Update weights according to learning rate\n\nOur training target is to make the prediction value approximate the correct value, while it can be transferred to minimize the error by updating weights with the help of learning rate. Suppose the learning rate is 0.02.\n\nWe got the updated weight matrix as folows\n$$\nW_{ho}^* =\n\\begin{bmatrix}\nW_{h1o1} - \\eta W_{h1o1}' & W_{h1o2} - \\eta W_{h1o2}'\\\\\nW_{h2o1} - \\eta W_{h2o1}' & W_{h2o2} - \\eta W_{h2o2}'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.4 - 0.02*(-0.2767)&0.6 - 0.02*(-0.3733)\\\\\n0.9 - 0.02*(-0.2831)&0.8 - 0.02*(-0.3853)\n\\end{bmatrix}\\\\\n=\n\\begin{bmatrix}\n0.4055 & 0.6075\\\\\n0.9057 & 0.8077\n\\end{bmatrix}\n$$\nThat is the updated weight of Layer1-Layer2. The update of Input-Layer weights is the same story I will illustrate as follows.\n\n####5.2.2 Layer0(Input Layer) - Layer1 \n\n![image-20181107222954838](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-05.png)\n\nFollow the path of the previous chapter\n\n* Stage1:\n\n$$\n\\frac{\\partial h_{out1}}{\\partial h_{in1}}\n=\n\\frac{\\partial Sigmoid(h_{in1})}{\\partial h_{in1}}\n=\nSigmoid(h_{in1})*(1-Sigmoid(h_{in1}))\n=\\\\\nSigmoid(0.67)*(1-Sigmoid(0.67))=0.2239\n$$\n\n* Stage2:\n\n$$\n\\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n=\n\\frac{\\partial(x1 * W_{i1h1} + x2 * W_{i2h1} + b11)}{\\partial W_{i2h1}}\n=\nx2=0.2\n$$\n\nApply the chain rule:\n$$\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}}=\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \n\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n$$\nWe already got the second and third derivations, regarding the first derivation, we apply the chain rule again, but in the opposite direction.\n$$\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n= \n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\frac{\\partial o_{out1}}{\\partial o_{in1}}\n\\frac{\\partial o_{in1}}{\\partial h1_{out}}\n$$\nWe have computed the first and second results, and the third one is merely a deviation of the linear function\n$$\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n= \n-1.719*0.2433* \n\\frac{\\partial(h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1})}{\\partial h_{out1}}\n=\\\\\n-1.719*0.2433*W_{h1o1}\n=-1.719*0.2433*0.4\n=-0.1673\n$$\nThen we got\n$$\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}}= -0.1673*0.2239 * 0.2=-0.0075\n$$\nSimilarly , we can get the Layer0-Layer1 derivatives with respective to the total error\n$$\nW_{ih}'\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial W_{i1h1}} & \\frac{\\partial E_{total}}{\\partial W_{i1h2}}\\\\\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}} & \\frac{\\partial E_{total}}{\\partial W_{i2h2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h1_{out}}{\\partial h1_{in}} \n\\cdot \\frac{\\partial h1_{in}}{\\partial W_{i1h1}}\n& \n\\frac{\\partial E_{total}}{\\partial h_{out2}}\n\\cdot \\frac{\\partial h2_{out}}{\\partial h2_{in}} \n\\cdot \\frac{\\partial h2_{in}}{\\partial W_{i1h2}}\n\\\\\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \n\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n& \n\\frac{\\partial E_{total}}{\\partial h_{out2}}\n\\cdot \\frac{\\partial h_{out2}}{\\partial h_{in2}} \n\\cdot \\frac{\\partial h_{in2}}{\\partial W_{i2h2}}\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-0.1673*0.2239 * 0.1 &  -0.4654*0.2187*0.1\\\\\n-0.1673*0.2239 * 0.2 &  -0.4654*0.2187*0.2\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-0.0037 &  -0.0102\\\\\n-0.0075 &  -0.0204\n\\end{bmatrix}\n$$\n\n#### Update weights according to learning rate\n\nUpdate the weights with learning rate 0.02we got the final weight matrix\n$$\nW_{ih}^*=\n\\begin{bmatrix}\nW_{i1h1} - \\eta (W_{i1h1}') &  W_{i1h2} - \\eta (W_{i1h2}')\\\\\nW_{i2h1} - \\eta (W_{i2h1}') &  W_{i2h2} - \\eta (W_{i2h2}')\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(0.7 -0.2*(-0.0037))& (0.6 - 0.2 *(-0.0102)) \\\\\n(0.5 - 0.2*(-0.0075)) & (0.4 - 0.2 * (-0.0204))\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n0.7007 & 0.6020\\\\\n0.5015 & 0.4041\n\\end{bmatrix}\n$$\n\n###5.3 Wrap up\n\nFinally we get all the weights updated\n$$\nW_{ho}^* = \n\\begin{bmatrix}\n0.4055 & 0.6075\\\\\n0.9057 & 0.8077\n\\end{bmatrix}\n$$\n\n$$\nW_{ih}^*=\n\\begin{bmatrix}\n0.7007 & 0.6020\\\\\n0.5015 & 0.4041\n\\end{bmatrix}\n$$\n\n## 6. Conclusion\n\n* Backpropagation is beautiful designed architecture. Every gate in the diagram gets some input and makes some output, the gradients of input concerning the output indicates how strongly the gate wants the output to increase or decrease. The communication between these \"smart\" gates make it possible for complicated prediction or classification tasks.\n* The activation function matters. Take Sigmoid as an example, and we saw the gradients of its gates \"vanish\" significantly to 0.00XXX, this will make the rest of backward pass almost to zero due to the multiplication in chain rule. So we should always be nervous in Sigmoid, Relu is possibly a better choice.\n* If we look back to the computing process,  a lot can be done when we implement the neural network with codes, such as the caching of gradients when we do forward propagation and the extracting of common gradient computation functions.\n\n## 7. Reference\n\n1. <https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c>.\n2. <https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/>.  \n3. <https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b>\n4. <http://cs231n.github.io/optimization-2/> I\n5. <https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/> \n6. <https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb>","slug":"Backward-propagation-of-Neural-Network-explained","published":1,"updated":"2018-11-08T11:58:43.800Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3bo0001m2tq75eap7o3","content":"<h1 id=\"Backward-propagation-of-Neural-Network-explained\"><a href=\"#Backward-propagation-of-Neural-Network-explained\" class=\"headerlink\" title=\"Backward propagation of Neural Network explained\"></a>Backward propagation of Neural Network explained</h1><p>Backpropagation is the foundation of the deep neural network. Usually, we consider it to be kind of dark magic we are not able to understand. However, it should not be the black box which we stay away. In this article, I will try to explain backpropagation as well as the whole neural network step by step in the original mathematical way.</p>\n<h2 id=\"Outline\"><a href=\"#Outline\" class=\"headerlink\" title=\"Outline\"></a>Outline</h2><ul>\n<li>Overview of the architecture</li>\n<li>Initialize parameters</li>\n<li>Implement forward propagation</li>\n<li>Compute Loss</li>\n<li>Implement Backward propagation</li>\n<li>Update parameters</li>\n</ul>\n<h2 id=\"1-The-architecture\"><a href=\"#1-The-architecture\" class=\"headerlink\" title=\"1. The architecture\"></a>1. The architecture</h2><p>This neural network Im going to explain is a 2-Layer neural network. The first layer is Linear + Sigmoid, and the second Layer is Linear + Softmax. </p>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-01.png\" alt=\"image-20181107215309162\"></p>\n<p>The architecture in the math formula</p>\n<script type=\"math/tex; mode=display\">\nf(X)= Relu( Sigmoid(  X \\times W_{ih} +b_{1}) \\times W_{ho} + b_{2})</script><h2 id=\"2-Initialize-parameters\"><a href=\"#2-Initialize-parameters\" class=\"headerlink\" title=\"2.Initialize parameters\"></a>2.Initialize parameters</h2><p>We take one example which has two features like below</p>\n<script type=\"math/tex; mode=display\">\nX= [x1, x2]= [0.1, 0.2]</script><p>The parameters are taken randomly. </p>\n<script type=\"math/tex; mode=display\">\nW_{is}=\n\n\\begin{bmatrix}\nW_{i1h1} & W_{i1h2}\\\\\nW_{i2h1} & W_{i2h2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.7 & 0.6\\\\\n0.5 & 0.4\n\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\nW_{sr}=\n\\begin{bmatrix}\nW_{h1o1}&W_{h1o2}\\\\\nW_{h2o1}&W_{h2o2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.4&0.6\\\\\n0.9&0.8\n\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\nb_{1}=[b_{11}, b_{12}] =[0.5,0.6]</script><script type=\"math/tex; mode=display\">\nb_{2}= [b_{21}, b_{22}] \n= [0.7, 0.9]</script><h2 id=\"3-Forward-Propagation\"><a href=\"#3-Forward-Propagation\" class=\"headerlink\" title=\"3. Forward Propagation\"></a>3. Forward Propagation</h2><h3 id=\"3-1-Layer1\"><a href=\"#3-1-Layer1\" class=\"headerlink\" title=\"3.1 Layer1:\"></a>3.1 Layer1:</h3><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-02.png\" alt=\"image-20181108110049524\"></p>\n<h3 id=\"Linear\"><a href=\"#Linear\" class=\"headerlink\" title=\"Linear\"></a>Linear</h3><script type=\"math/tex; mode=display\">\nX \\times  W_{ih}  + b_{1}\n=\n[x1, x2]\n\\times\n\\begin{bmatrix}\nW_{i1h1} & W_{i1h2}\\\\\nW_{i2h1} & W_{i2h2}\n\\end{bmatrix}\n+\n[b_{11}, b_{12}]\n=\n[0.1, 0.2]\n\\times\n\\begin{bmatrix}\n0.7 & 0.6\\\\\n0.5 & 0.4\n\\end{bmatrix}\n+\n[0.5, 0.6]\n=\n[0.67, 0.74]</script><h3 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h3><script type=\"math/tex; mode=display\">\nSigmoid(x)=1/(1+e^{-x})</script><script type=\"math/tex; mode=display\">\n[h_{out1},h_{out2}]\n=\nSigmoid(X \\times W_{ih} + b_{1}) =\n[Sigmoid(0.67),Sigmoid(0.74)]\n=\n[0.6615,0.6770]</script><h3 id=\"3-2-Layer2\"><a href=\"#3-2-Layer2\" class=\"headerlink\" title=\"3.2 Layer2:\"></a>3.2 Layer2:</h3><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-03.png\" alt=\"image-20181108110231696\"></p>\n<h3 id=\"Linear-1\"><a href=\"#Linear-1\" class=\"headerlink\" title=\"Linear\"></a>Linear</h3><script type=\"math/tex; mode=display\">\n[o_{in1}, o_{in2}]\n=\n[h_{out1}, h_{out2}] \\times W_{ho}  + b_{2}\n=\n[h_{out1}, h_{out2}]\n\\times\n\\begin{bmatrix}\nW_{h1o1} & W_{h1o2}\\\\\nW_{h2o1} & W_{h2o2}\n\\end{bmatrix}\n+\n[b_{21}, b_{22}]\n=\\\\\n[0.6615, 0.6770]\n\\times\n\\begin{bmatrix}\n0.7 & 0.5\\\\\n0.6 & 0.4\n\\end{bmatrix}\n+\n[0.5, 0.6]\n=\n\n[1.3693, 1.0391]</script><h3 id=\"Softmax\"><a href=\"#Softmax\" class=\"headerlink\" title=\"Softmax\"></a>Softmax</h3><script type=\"math/tex; mode=display\">\nSoftmax(x)_{j}= \\frac{e^{xj}}{\\sum_{i=1}^n e^{xi}} (j=1,2...n)</script><script type=\"math/tex; mode=display\">\n[o_{out1},o_{out2}]\n=\nSoftmax(X\\times W_{ho} + b_{2})=\n[\n\\frac{e^{1.3693}}{e^{1.3693} +e^{1.0391}} ,\n\\frac{e^{1.0391}}{e^{1.3693} +e^{1.0391}} \n]\n=\n[0.5818,0.4182]</script><h2 id=\"4-Compute-Loss\"><a href=\"#4-Compute-Loss\" class=\"headerlink\" title=\"4. Compute Loss\"></a>4. Compute Loss</h2><p>The Loss function here we use is cross-entropy cost</p>\n<script type=\"math/tex; mode=display\">\nCrossentropy= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))</script><p>The actual output should be </p>\n<script type=\"math/tex; mode=display\">\n[y_{1},y_{2}] \n=\n[1.00, 0.00]</script><p>Since we only have one example, that means  m = 1,  the total loss is computed as follows :</p>\n<script type=\"math/tex; mode=display\">\n-\\frac{1}{1} \\sum\\limits_{i = 1}^{1} (y_{1}\\log\\left(o_{out1}\\right) + 0 +0 + 1*log(1-o_{out2}))=\n-(1*log(0.5818)+0+0+1*log(1-0.4182))==0.4704</script><h2 id=\"5-Backward-Propagation\"><a href=\"#5-Backward-Propagation\" class=\"headerlink\" title=\"5. Backward Propagation\"></a>5. Backward Propagation</h2><p>In this section, we will go through backward propagation stage by stage.</p>\n<h3 id=\"5-1-Basic-Derivatives\"><a href=\"#5-1-Basic-Derivatives\" class=\"headerlink\" title=\"5.1 Basic Derivatives\"></a>5.1 Basic Derivatives</h3><h4 id=\"Sigmoid-1\"><a href=\"#Sigmoid-1\" class=\"headerlink\" title=\"Sigmoid:\"></a>Sigmoid:</h4><script type=\"math/tex; mode=display\">\n\\frac{\\partial Sigmoid(x)}{\\partial x}\n=\n\\frac{\\partial \\frac{1}{(1+e^{-x})}}{\\partial x}\n=\n\\frac{ e^{-x}}{(1+e^{-x})^2}\n=\n(\\frac{1+e^{-x}-1}{1+e^{-x}})\\frac{1}{1+e^{-x}}\n=\n(1 - Sigmoid(x))\\times Sigmoid(x)</script><h4 id=\"Softmax-1\"><a href=\"#Softmax-1\" class=\"headerlink\" title=\"Softmax:\"></a>Softmax:</h4><p>At first we know:</p>\n<p>For </p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\frac{g(x)}{h(x)}</script><script type=\"math/tex; mode=display\">\nf'(x) = \\frac{g'(x)h(x)-g(x)h'(x)}{[h(x)]^2}</script><p>Then the derivation of Softmax is </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial Softmax(x)}{\\partial x1}=\\frac{e^{x1}(e^{x1}+e^{x2})-e^{x1}e^{x1} }{(e^{x1}+e^{x2})^2} = \\frac{e^{x1+x2}}{(e^{x1}+e^{x2})^2}</script><h3 id=\"5-2-The-backward-Pass\"><a href=\"#5-2-The-backward-Pass\" class=\"headerlink\" title=\"5.2 The backward Pass\"></a>5.2 The backward Pass</h3><h4 id=\"5-2-1-Layer1-Layer2\"><a href=\"#5-2-1-Layer1-Layer2\" class=\"headerlink\" title=\"5.2.1 Layer1-Layer2\"></a>5.2.1 Layer1-Layer2</h4><h4 id=\"Weight-derivatives-with-respect-to-the-error\"><a href=\"#Weight-derivatives-with-respect-to-the-error\" class=\"headerlink\" title=\"Weight derivatives with respect to the error\"></a>Weight derivatives with respect to the error</h4><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-04.png\" alt=\"image-20181107220124773\"></p>\n<p>Consider W<sub>ho</sub> , we want to know how W<sub>ho</sub> will affect the total error, aka the value of </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{ho}}</script><p><a href=\"https://en.wikipedia.org/wiki/Chain_rule\" target=\"_blank\" rel=\"noopener\">Chain Rule</a> states that:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial z}{\\partial x} =\n\\frac{\\partial z}{\\partial y}\\cdot\\frac{\\partial y}{\\partial x}</script><p>So we have</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}}=\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{int1}} \n\\cdot \\frac{\\partial o_{int1}}{\\partial W_{h2o1}}</script><p>Lets break this through stage by stage</p>\n<ul>\n<li>Stage1</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial o_{out1}} = \n\\frac{\\partial (-(y_{1}*log(o_{out1})+(1-y_{1})*log(1-o_{out1})))}{\\partial o_{out1}}+0\n=-\\frac{1}{o_{out1}}=-1/0.5818=-1.719</script><ul>\n<li>Stage2</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial o_{1}}{\\partial i_{1}}=\n\\frac{\\partial Softmax(i_{1})}{\\partial i_{1}}\n=\n\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}\n=\n\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}\n=\n\\frac{e^{1.3693}\\cdot e^{1.0391}}{(e^{1.3693}+e^{1.0391})^2}=0.2433</script><ul>\n<li>Stage3 </li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial o_{in1}}{\\partial W_{h2o1}} = \n\\frac{\\partial (h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1}+b_{21})}{\\partial W_{h2o1}}\n= h_{out2}=0.6770</script><p>Finally we apply the chain rule:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}}= -1.719 * 0.2433 * 0.677 = -0.2831</script><p>Lets go through all the weights in Layer2</p>\n<script type=\"math/tex; mode=display\">\nW_{ho}'=\n\\begin{bmatrix}\nW_{h1o1}' & W_{h1o2}'\\\\\nW_{h2o1}' & W_{h2o2}'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial W_{h1o1}} & \\frac{\\partial E_{total}}{\\partial W_{h1o2}} \\\\\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}} & \\frac{\\partial E_{total}}{\\partial W_{h2o2}} \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \n\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h1o1}}\n&\n\\frac{\\partial E_{total}}{\\partial o_{out2}}\n\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \n\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h1o2}}\n\\\\\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \n\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h2o1}} \n&\n\\frac{\\partial E_{total}}{\\partial o_{out2}}\n\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \n\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h2o2}}\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-1.719*0.2433*0.6615 & -2.3912*0.2433*0.6615 \\\\\n-1.719*0.2433*0.6770 & -2.3912*0.2433*0.6770\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-0.2767 & -0.3733\\\\\n-0.2831 & -0.3853\n\\end{bmatrix}</script><h4 id=\"Update-weights-according-to-learning-rate\"><a href=\"#Update-weights-according-to-learning-rate\" class=\"headerlink\" title=\"Update weights according to learning rate\"></a>Update weights according to learning rate</h4><p>Our training target is to make the prediction value approximate the correct value, while it can be transferred to minimize the error by updating weights with the help of learning rate. Suppose the learning rate is 0.02.</p>\n<p>We got the updated weight matrix as folows</p>\n<script type=\"math/tex; mode=display\">\nW_{ho}^* =\n\\begin{bmatrix}\nW_{h1o1} - \\eta W_{h1o1}' & W_{h1o2} - \\eta W_{h1o2}'\\\\\nW_{h2o1} - \\eta W_{h2o1}' & W_{h2o2} - \\eta W_{h2o2}'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.4 - 0.02*(-0.2767)&0.6 - 0.02*(-0.3733)\\\\\n0.9 - 0.02*(-0.2831)&0.8 - 0.02*(-0.3853)\n\\end{bmatrix}\\\\\n=\n\\begin{bmatrix}\n0.4055 & 0.6075\\\\\n0.9057 & 0.8077\n\\end{bmatrix}</script><p>That is the updated weight of Layer1-Layer2. The update of Input-Layer weights is the same story I will illustrate as follows.</p>\n<h4 id=\"5-2-2-Layer0-Input-Layer-Layer1\"><a href=\"#5-2-2-Layer0-Input-Layer-Layer1\" class=\"headerlink\" title=\"5.2.2 Layer0(Input Layer) - Layer1\"></a>5.2.2 Layer0(Input Layer) - Layer1</h4><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-05.png\" alt=\"image-20181107222954838\"></p>\n<p>Follow the path of the previous chapter</p>\n<ul>\n<li>Stage1:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_{out1}}{\\partial h_{in1}}\n=\n\\frac{\\partial Sigmoid(h_{in1})}{\\partial h_{in1}}\n=\nSigmoid(h_{in1})*(1-Sigmoid(h_{in1}))\n=\\\\\nSigmoid(0.67)*(1-Sigmoid(0.67))=0.2239</script><ul>\n<li>Stage2:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n=\n\\frac{\\partial(x1 * W_{i1h1} + x2 * W_{i2h1} + b11)}{\\partial W_{i2h1}}\n=\nx2=0.2</script><p>Apply the chain rule:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}}=\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \n\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}</script><p>We already got the second and third derivations, regarding the first derivation, we apply the chain rule again, but in the opposite direction.</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n= \n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\frac{\\partial o_{out1}}{\\partial o_{in1}}\n\\frac{\\partial o_{in1}}{\\partial h1_{out}}</script><p>We have computed the first and second results, and the third one is merely a deviation of the linear function</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n= \n-1.719*0.2433* \n\\frac{\\partial(h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1})}{\\partial h_{out1}}\n=\\\\\n-1.719*0.2433*W_{h1o1}\n=-1.719*0.2433*0.4\n=-0.1673</script><p>Then we got</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}}= -0.1673*0.2239 * 0.2=-0.0075</script><p>Similarly , we can get the Layer0-Layer1 derivatives with respective to the total error</p>\n<script type=\"math/tex; mode=display\">\nW_{ih}'\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial W_{i1h1}} & \\frac{\\partial E_{total}}{\\partial W_{i1h2}}\\\\\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}} & \\frac{\\partial E_{total}}{\\partial W_{i2h2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h1_{out}}{\\partial h1_{in}} \n\\cdot \\frac{\\partial h1_{in}}{\\partial W_{i1h1}}\n& \n\\frac{\\partial E_{total}}{\\partial h_{out2}}\n\\cdot \\frac{\\partial h2_{out}}{\\partial h2_{in}} \n\\cdot \\frac{\\partial h2_{in}}{\\partial W_{i1h2}}\n\\\\\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \n\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n& \n\\frac{\\partial E_{total}}{\\partial h_{out2}}\n\\cdot \\frac{\\partial h_{out2}}{\\partial h_{in2}} \n\\cdot \\frac{\\partial h_{in2}}{\\partial W_{i2h2}}\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-0.1673*0.2239 * 0.1 &  -0.4654*0.2187*0.1\\\\\n-0.1673*0.2239 * 0.2 &  -0.4654*0.2187*0.2\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-0.0037 &  -0.0102\\\\\n-0.0075 &  -0.0204\n\\end{bmatrix}</script><h4 id=\"Update-weights-according-to-learning-rate-1\"><a href=\"#Update-weights-according-to-learning-rate-1\" class=\"headerlink\" title=\"Update weights according to learning rate\"></a>Update weights according to learning rate</h4><p>Update the weights with learning rate 0.02we got the final weight matrix</p>\n<script type=\"math/tex; mode=display\">\nW_{ih}^*=\n\\begin{bmatrix}\nW_{i1h1} - \\eta (W_{i1h1}') &  W_{i1h2} - \\eta (W_{i1h2}')\\\\\nW_{i2h1} - \\eta (W_{i2h1}') &  W_{i2h2} - \\eta (W_{i2h2}')\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(0.7 -0.2*(-0.0037))& (0.6 - 0.2 *(-0.0102)) \\\\\n(0.5 - 0.2*(-0.0075)) & (0.4 - 0.2 * (-0.0204))\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n0.7007 & 0.6020\\\\\n0.5015 & 0.4041\n\\end{bmatrix}</script><h3 id=\"5-3-Wrap-up\"><a href=\"#5-3-Wrap-up\" class=\"headerlink\" title=\"5.3 Wrap up\"></a>5.3 Wrap up</h3><p>Finally we get all the weights updated</p>\n<script type=\"math/tex; mode=display\">\nW_{ho}^* = \n\\begin{bmatrix}\n0.4055 & 0.6075\\\\\n0.9057 & 0.8077\n\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\nW_{ih}^*=\n\\begin{bmatrix}\n0.7007 & 0.6020\\\\\n0.5015 & 0.4041\n\\end{bmatrix}</script><h2 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6. Conclusion\"></a>6. Conclusion</h2><ul>\n<li>Backpropagation is beautiful designed architecture. Every gate in the diagram gets some input and makes some output, the gradients of input concerning the output indicates how strongly the gate wants the output to increase or decrease. The communication between these smart gates make it possible for complicated prediction or classification tasks.</li>\n<li>The activation function matters. Take Sigmoid as an example, and we saw the gradients of its gates vanish significantly to 0.00XXX, this will make the rest of backward pass almost to zero due to the multiplication in chain rule. So we should always be nervous in Sigmoid, Relu is possibly a better choice.</li>\n<li>If we look back to the computing process,  a lot can be done when we implement the neural network with codes, such as the caching of gradients when we do forward propagation and the extracting of common gradient computation functions.</li>\n</ul>\n<h2 id=\"7-Reference\"><a href=\"#7-Reference\" class=\"headerlink\" title=\"7. Reference\"></a>7. Reference</h2><ol>\n<li><a href=\"&#109;&#97;&#x69;&#108;&#116;&#111;&#x3a;&#104;&#116;&#116;&#112;&#x73;&#58;&#x2f;&#47;&#x6d;&#x65;&#100;&#x69;&#117;&#x6d;&#x2e;&#x63;&#111;&#109;&#47;&#64;&#x31;&#52;&#112;&#114;&#x61;&#x6b;&#97;&#115;&#104;&#x2f;&#x62;&#97;&#x63;&#107;&#45;&#x70;&#114;&#111;&#112;&#97;&#103;&#97;&#x74;&#x69;&#x6f;&#110;&#45;&#105;&#x73;&#45;&#x76;&#x65;&#x72;&#x79;&#45;&#115;&#x69;&#x6d;&#x70;&#x6c;&#101;&#45;&#x77;&#104;&#x6f;&#x2d;&#x6d;&#97;&#x64;&#101;&#45;&#105;&#x74;&#x2d;&#x63;&#111;&#x6d;&#112;&#x6c;&#x69;&#99;&#x61;&#x74;&#101;&#x64;&#x2d;&#x39;&#x37;&#x62;&#x37;&#x39;&#52;&#x63;&#57;&#55;&#101;&#x35;&#x63;\">&#104;&#116;&#116;&#112;&#x73;&#58;&#x2f;&#47;&#x6d;&#x65;&#100;&#x69;&#117;&#x6d;&#x2e;&#x63;&#111;&#109;&#47;&#64;&#x31;&#52;&#112;&#114;&#x61;&#x6b;&#97;&#115;&#104;&#x2f;&#x62;&#97;&#x63;&#107;&#45;&#x70;&#114;&#111;&#112;&#97;&#103;&#97;&#x74;&#x69;&#x6f;&#110;&#45;&#105;&#x73;&#45;&#x76;&#x65;&#x72;&#x79;&#45;&#115;&#x69;&#x6d;&#x70;&#x6c;&#101;&#45;&#x77;&#104;&#x6f;&#x2d;&#x6d;&#97;&#x64;&#101;&#45;&#105;&#x74;&#x2d;&#x63;&#111;&#x6d;&#112;&#x6c;&#x69;&#99;&#x61;&#x74;&#101;&#x64;&#x2d;&#x39;&#x37;&#x62;&#x37;&#x39;&#52;&#x63;&#57;&#55;&#101;&#x35;&#x63;</a>.</li>\n<li><a href=\"https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\" target=\"_blank\" rel=\"noopener\">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>.  </li>\n<li><a href=\"&#109;&#97;&#x69;&#108;&#x74;&#x6f;&#58;&#x68;&#x74;&#116;&#x70;&#115;&#58;&#x2f;&#47;&#x6d;&#101;&#x64;&#105;&#x75;&#109;&#x2e;&#99;&#x6f;&#x6d;&#47;&#x40;&#107;&#x61;&#x72;&#112;&#x61;&#x74;&#x68;&#x79;&#x2f;&#x79;&#101;&#x73;&#45;&#121;&#111;&#x75;&#45;&#x73;&#104;&#x6f;&#x75;&#x6c;&#x64;&#45;&#117;&#x6e;&#x64;&#x65;&#x72;&#x73;&#x74;&#97;&#x6e;&#100;&#x2d;&#x62;&#x61;&#x63;&#107;&#x70;&#x72;&#111;&#x70;&#45;&#x65;&#x32;&#x66;&#x30;&#54;&#101;&#97;&#x62;&#52;&#57;&#x36;&#x62;\">&#x68;&#x74;&#116;&#x70;&#115;&#58;&#x2f;&#47;&#x6d;&#101;&#x64;&#105;&#x75;&#109;&#x2e;&#99;&#x6f;&#x6d;&#47;&#x40;&#107;&#x61;&#x72;&#112;&#x61;&#x74;&#x68;&#x79;&#x2f;&#x79;&#101;&#x73;&#45;&#121;&#111;&#x75;&#45;&#x73;&#104;&#x6f;&#x75;&#x6c;&#x64;&#45;&#117;&#x6e;&#x64;&#x65;&#x72;&#x73;&#x74;&#97;&#x6e;&#100;&#x2d;&#x62;&#x61;&#x63;&#107;&#x70;&#x72;&#111;&#x70;&#45;&#x65;&#x32;&#x66;&#x30;&#54;&#101;&#97;&#x62;&#52;&#57;&#x36;&#x62;</a></li>\n<li><a href=\"http://cs231n.github.io/optimization-2/\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/optimization-2/</a> I</li>\n<li><a href=\"https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/\" target=\"_blank\" rel=\"noopener\">https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/</a> </li>\n<li><a href=\"https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb\" target=\"_blank\" rel=\"noopener\">https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Backward-propagation-of-Neural-Network-explained\"><a href=\"#Backward-propagation-of-Neural-Network-explained\" class=\"headerlink\" title=\"Backward propagation of Neural Network explained\"></a>Backward propagation of Neural Network explained</h1><p>Backpropagation is the foundation of the deep neural network. Usually, we consider it to be kind of dark magic we are not able to understand. However, it should not be the black box which we stay away. In this article, I will try to explain backpropagation as well as the whole neural network step by step in the original mathematical way.</p>\n<h2 id=\"Outline\"><a href=\"#Outline\" class=\"headerlink\" title=\"Outline\"></a>Outline</h2><ul>\n<li>Overview of the architecture</li>\n<li>Initialize parameters</li>\n<li>Implement forward propagation</li>\n<li>Compute Loss</li>\n<li>Implement Backward propagation</li>\n<li>Update parameters</li>\n</ul>\n<h2 id=\"1-The-architecture\"><a href=\"#1-The-architecture\" class=\"headerlink\" title=\"1. The architecture\"></a>1. The architecture</h2><p>This neural network Im going to explain is a 2-Layer neural network. The first layer is Linear + Sigmoid, and the second Layer is Linear + Softmax. </p>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-01.png\" alt=\"image-20181107215309162\"></p>\n<p>The architecture in the math formula</p>\n<script type=\"math/tex; mode=display\">\nf(X)= Relu( Sigmoid(  X \\times W_{ih} +b_{1}) \\times W_{ho} + b_{2})</script><h2 id=\"2-Initialize-parameters\"><a href=\"#2-Initialize-parameters\" class=\"headerlink\" title=\"2.Initialize parameters\"></a>2.Initialize parameters</h2><p>We take one example which has two features like below</p>\n<script type=\"math/tex; mode=display\">\nX= [x1, x2]= [0.1, 0.2]</script><p>The parameters are taken randomly. </p>\n<script type=\"math/tex; mode=display\">\nW_{is}=\n\n\\begin{bmatrix}\nW_{i1h1} & W_{i1h2}\\\\\nW_{i2h1} & W_{i2h2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.7 & 0.6\\\\\n0.5 & 0.4\n\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\nW_{sr}=\n\\begin{bmatrix}\nW_{h1o1}&W_{h1o2}\\\\\nW_{h2o1}&W_{h2o2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.4&0.6\\\\\n0.9&0.8\n\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\nb_{1}=[b_{11}, b_{12}] =[0.5,0.6]</script><script type=\"math/tex; mode=display\">\nb_{2}= [b_{21}, b_{22}] \n= [0.7, 0.9]</script><h2 id=\"3-Forward-Propagation\"><a href=\"#3-Forward-Propagation\" class=\"headerlink\" title=\"3. Forward Propagation\"></a>3. Forward Propagation</h2><h3 id=\"3-1-Layer1\"><a href=\"#3-1-Layer1\" class=\"headerlink\" title=\"3.1 Layer1:\"></a>3.1 Layer1:</h3><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-02.png\" alt=\"image-20181108110049524\"></p>\n<h3 id=\"Linear\"><a href=\"#Linear\" class=\"headerlink\" title=\"Linear\"></a>Linear</h3><script type=\"math/tex; mode=display\">\nX \\times  W_{ih}  + b_{1}\n=\n[x1, x2]\n\\times\n\\begin{bmatrix}\nW_{i1h1} & W_{i1h2}\\\\\nW_{i2h1} & W_{i2h2}\n\\end{bmatrix}\n+\n[b_{11}, b_{12}]\n=\n[0.1, 0.2]\n\\times\n\\begin{bmatrix}\n0.7 & 0.6\\\\\n0.5 & 0.4\n\\end{bmatrix}\n+\n[0.5, 0.6]\n=\n[0.67, 0.74]</script><h3 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h3><script type=\"math/tex; mode=display\">\nSigmoid(x)=1/(1+e^{-x})</script><script type=\"math/tex; mode=display\">\n[h_{out1},h_{out2}]\n=\nSigmoid(X \\times W_{ih} + b_{1}) =\n[Sigmoid(0.67),Sigmoid(0.74)]\n=\n[0.6615,0.6770]</script><h3 id=\"3-2-Layer2\"><a href=\"#3-2-Layer2\" class=\"headerlink\" title=\"3.2 Layer2:\"></a>3.2 Layer2:</h3><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-03.png\" alt=\"image-20181108110231696\"></p>\n<h3 id=\"Linear-1\"><a href=\"#Linear-1\" class=\"headerlink\" title=\"Linear\"></a>Linear</h3><script type=\"math/tex; mode=display\">\n[o_{in1}, o_{in2}]\n=\n[h_{out1}, h_{out2}] \\times W_{ho}  + b_{2}\n=\n[h_{out1}, h_{out2}]\n\\times\n\\begin{bmatrix}\nW_{h1o1} & W_{h1o2}\\\\\nW_{h2o1} & W_{h2o2}\n\\end{bmatrix}\n+\n[b_{21}, b_{22}]\n=\\\\\n[0.6615, 0.6770]\n\\times\n\\begin{bmatrix}\n0.7 & 0.5\\\\\n0.6 & 0.4\n\\end{bmatrix}\n+\n[0.5, 0.6]\n=\n\n[1.3693, 1.0391]</script><h3 id=\"Softmax\"><a href=\"#Softmax\" class=\"headerlink\" title=\"Softmax\"></a>Softmax</h3><script type=\"math/tex; mode=display\">\nSoftmax(x)_{j}= \\frac{e^{xj}}{\\sum_{i=1}^n e^{xi}} (j=1,2...n)</script><script type=\"math/tex; mode=display\">\n[o_{out1},o_{out2}]\n=\nSoftmax(X\\times W_{ho} + b_{2})=\n[\n\\frac{e^{1.3693}}{e^{1.3693} +e^{1.0391}} ,\n\\frac{e^{1.0391}}{e^{1.3693} +e^{1.0391}} \n]\n=\n[0.5818,0.4182]</script><h2 id=\"4-Compute-Loss\"><a href=\"#4-Compute-Loss\" class=\"headerlink\" title=\"4. Compute Loss\"></a>4. Compute Loss</h2><p>The Loss function here we use is cross-entropy cost</p>\n<script type=\"math/tex; mode=display\">\nCrossentropy= -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))</script><p>The actual output should be </p>\n<script type=\"math/tex; mode=display\">\n[y_{1},y_{2}] \n=\n[1.00, 0.00]</script><p>Since we only have one example, that means  m = 1,  the total loss is computed as follows :</p>\n<script type=\"math/tex; mode=display\">\n-\\frac{1}{1} \\sum\\limits_{i = 1}^{1} (y_{1}\\log\\left(o_{out1}\\right) + 0 +0 + 1*log(1-o_{out2}))=\n-(1*log(0.5818)+0+0+1*log(1-0.4182))==0.4704</script><h2 id=\"5-Backward-Propagation\"><a href=\"#5-Backward-Propagation\" class=\"headerlink\" title=\"5. Backward Propagation\"></a>5. Backward Propagation</h2><p>In this section, we will go through backward propagation stage by stage.</p>\n<h3 id=\"5-1-Basic-Derivatives\"><a href=\"#5-1-Basic-Derivatives\" class=\"headerlink\" title=\"5.1 Basic Derivatives\"></a>5.1 Basic Derivatives</h3><h4 id=\"Sigmoid-1\"><a href=\"#Sigmoid-1\" class=\"headerlink\" title=\"Sigmoid:\"></a>Sigmoid:</h4><script type=\"math/tex; mode=display\">\n\\frac{\\partial Sigmoid(x)}{\\partial x}\n=\n\\frac{\\partial \\frac{1}{(1+e^{-x})}}{\\partial x}\n=\n\\frac{ e^{-x}}{(1+e^{-x})^2}\n=\n(\\frac{1+e^{-x}-1}{1+e^{-x}})\\frac{1}{1+e^{-x}}\n=\n(1 - Sigmoid(x))\\times Sigmoid(x)</script><h4 id=\"Softmax-1\"><a href=\"#Softmax-1\" class=\"headerlink\" title=\"Softmax:\"></a>Softmax:</h4><p>At first we know:</p>\n<p>For </p>\n<script type=\"math/tex; mode=display\">\nf(x)=\\frac{g(x)}{h(x)}</script><script type=\"math/tex; mode=display\">\nf'(x) = \\frac{g'(x)h(x)-g(x)h'(x)}{[h(x)]^2}</script><p>Then the derivation of Softmax is </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial Softmax(x)}{\\partial x1}=\\frac{e^{x1}(e^{x1}+e^{x2})-e^{x1}e^{x1} }{(e^{x1}+e^{x2})^2} = \\frac{e^{x1+x2}}{(e^{x1}+e^{x2})^2}</script><h3 id=\"5-2-The-backward-Pass\"><a href=\"#5-2-The-backward-Pass\" class=\"headerlink\" title=\"5.2 The backward Pass\"></a>5.2 The backward Pass</h3><h4 id=\"5-2-1-Layer1-Layer2\"><a href=\"#5-2-1-Layer1-Layer2\" class=\"headerlink\" title=\"5.2.1 Layer1-Layer2\"></a>5.2.1 Layer1-Layer2</h4><h4 id=\"Weight-derivatives-with-respect-to-the-error\"><a href=\"#Weight-derivatives-with-respect-to-the-error\" class=\"headerlink\" title=\"Weight derivatives with respect to the error\"></a>Weight derivatives with respect to the error</h4><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-04.png\" alt=\"image-20181107220124773\"></p>\n<p>Consider W<sub>ho</sub> , we want to know how W<sub>ho</sub> will affect the total error, aka the value of </p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{ho}}</script><p><a href=\"https://en.wikipedia.org/wiki/Chain_rule\" target=\"_blank\" rel=\"noopener\">Chain Rule</a> states that:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial z}{\\partial x} =\n\\frac{\\partial z}{\\partial y}\\cdot\\frac{\\partial y}{\\partial x}</script><p>So we have</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}}=\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{int1}} \n\\cdot \\frac{\\partial o_{int1}}{\\partial W_{h2o1}}</script><p>Lets break this through stage by stage</p>\n<ul>\n<li>Stage1</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial o_{out1}} = \n\\frac{\\partial (-(y_{1}*log(o_{out1})+(1-y_{1})*log(1-o_{out1})))}{\\partial o_{out1}}+0\n=-\\frac{1}{o_{out1}}=-1/0.5818=-1.719</script><ul>\n<li>Stage2</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial o_{1}}{\\partial i_{1}}=\n\\frac{\\partial Softmax(i_{1})}{\\partial i_{1}}\n=\n\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}\n=\n\\frac{e^{i_{1}} e^{i_{2}}}{(e^{i_{1}}+e^{i_{2}})^2}\n=\n\\frac{e^{1.3693}\\cdot e^{1.0391}}{(e^{1.3693}+e^{1.0391})^2}=0.2433</script><ul>\n<li>Stage3 </li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial o_{in1}}{\\partial W_{h2o1}} = \n\\frac{\\partial (h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1}+b_{21})}{\\partial W_{h2o1}}\n= h_{out2}=0.6770</script><p>Finally we apply the chain rule:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}}= -1.719 * 0.2433 * 0.677 = -0.2831</script><p>Lets go through all the weights in Layer2</p>\n<script type=\"math/tex; mode=display\">\nW_{ho}'=\n\\begin{bmatrix}\nW_{h1o1}' & W_{h1o2}'\\\\\nW_{h2o1}' & W_{h2o2}'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial W_{h1o1}} & \\frac{\\partial E_{total}}{\\partial W_{h1o2}} \\\\\n\\frac{\\partial E_{total}}{\\partial W_{h2o1}} & \\frac{\\partial E_{total}}{\\partial W_{h2o2}} \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \n\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h1o1}}\n&\n\\frac{\\partial E_{total}}{\\partial o_{out2}}\n\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \n\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h1o2}}\n\\\\\n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\cdot \\frac{\\partial o_{out1}}{\\partial o_{in1}} \n\\cdot \\frac{\\partial o_{in1}}{\\partial W_{h2o1}} \n&\n\\frac{\\partial E_{total}}{\\partial o_{out2}}\n\\cdot \\frac{\\partial o_{out2}}{\\partial o_{in2}} \n\\cdot \\frac{\\partial o_{in2}}{\\partial W_{h2o2}}\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-1.719*0.2433*0.6615 & -2.3912*0.2433*0.6615 \\\\\n-1.719*0.2433*0.6770 & -2.3912*0.2433*0.6770\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-0.2767 & -0.3733\\\\\n-0.2831 & -0.3853\n\\end{bmatrix}</script><h4 id=\"Update-weights-according-to-learning-rate\"><a href=\"#Update-weights-according-to-learning-rate\" class=\"headerlink\" title=\"Update weights according to learning rate\"></a>Update weights according to learning rate</h4><p>Our training target is to make the prediction value approximate the correct value, while it can be transferred to minimize the error by updating weights with the help of learning rate. Suppose the learning rate is 0.02.</p>\n<p>We got the updated weight matrix as folows</p>\n<script type=\"math/tex; mode=display\">\nW_{ho}^* =\n\\begin{bmatrix}\nW_{h1o1} - \\eta W_{h1o1}' & W_{h1o2} - \\eta W_{h1o2}'\\\\\nW_{h2o1} - \\eta W_{h2o1}' & W_{h2o2} - \\eta W_{h2o2}'\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.4 - 0.02*(-0.2767)&0.6 - 0.02*(-0.3733)\\\\\n0.9 - 0.02*(-0.2831)&0.8 - 0.02*(-0.3853)\n\\end{bmatrix}\\\\\n=\n\\begin{bmatrix}\n0.4055 & 0.6075\\\\\n0.9057 & 0.8077\n\\end{bmatrix}</script><p>That is the updated weight of Layer1-Layer2. The update of Input-Layer weights is the same story I will illustrate as follows.</p>\n<h4 id=\"5-2-2-Layer0-Input-Layer-Layer1\"><a href=\"#5-2-2-Layer0-Input-Layer-Layer1\" class=\"headerlink\" title=\"5.2.2 Layer0(Input Layer) - Layer1\"></a>5.2.2 Layer0(Input Layer) - Layer1</h4><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/bp-05.png\" alt=\"image-20181107222954838\"></p>\n<p>Follow the path of the previous chapter</p>\n<ul>\n<li>Stage1:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_{out1}}{\\partial h_{in1}}\n=\n\\frac{\\partial Sigmoid(h_{in1})}{\\partial h_{in1}}\n=\nSigmoid(h_{in1})*(1-Sigmoid(h_{in1}))\n=\\\\\nSigmoid(0.67)*(1-Sigmoid(0.67))=0.2239</script><ul>\n<li>Stage2:</li>\n</ul>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n=\n\\frac{\\partial(x1 * W_{i1h1} + x2 * W_{i2h1} + b11)}{\\partial W_{i2h1}}\n=\nx2=0.2</script><p>Apply the chain rule:</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}}=\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \n\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}</script><p>We already got the second and third derivations, regarding the first derivation, we apply the chain rule again, but in the opposite direction.</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n= \n\\frac{\\partial E_{total}}{\\partial o_{out1}}\n\\frac{\\partial o_{out1}}{\\partial o_{in1}}\n\\frac{\\partial o_{in1}}{\\partial h1_{out}}</script><p>We have computed the first and second results, and the third one is merely a deviation of the linear function</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n= \n-1.719*0.2433* \n\\frac{\\partial(h_{out1}*W_{h1o1} + h_{out2}*W_{h2o1})}{\\partial h_{out1}}\n=\\\\\n-1.719*0.2433*W_{h1o1}\n=-1.719*0.2433*0.4\n=-0.1673</script><p>Then we got</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}}= -0.1673*0.2239 * 0.2=-0.0075</script><p>Similarly , we can get the Layer0-Layer1 derivatives with respective to the total error</p>\n<script type=\"math/tex; mode=display\">\nW_{ih}'\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial W_{i1h1}} & \\frac{\\partial E_{total}}{\\partial W_{i1h2}}\\\\\n\\frac{\\partial E_{total}}{\\partial W_{i2h1}} & \\frac{\\partial E_{total}}{\\partial W_{i2h2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h1_{out}}{\\partial h1_{in}} \n\\cdot \\frac{\\partial h1_{in}}{\\partial W_{i1h1}}\n& \n\\frac{\\partial E_{total}}{\\partial h_{out2}}\n\\cdot \\frac{\\partial h2_{out}}{\\partial h2_{in}} \n\\cdot \\frac{\\partial h2_{in}}{\\partial W_{i1h2}}\n\\\\\n\\frac{\\partial E_{total}}{\\partial h_{out1}}\n\\cdot \\frac{\\partial h_{out1}}{\\partial h_{in1}} \n\\cdot \\frac{\\partial h_{in1}}{\\partial W_{i2h1}}\n& \n\\frac{\\partial E_{total}}{\\partial h_{out2}}\n\\cdot \\frac{\\partial h_{out2}}{\\partial h_{in2}} \n\\cdot \\frac{\\partial h_{in2}}{\\partial W_{i2h2}}\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-0.1673*0.2239 * 0.1 &  -0.4654*0.2187*0.1\\\\\n-0.1673*0.2239 * 0.2 &  -0.4654*0.2187*0.2\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n-0.0037 &  -0.0102\\\\\n-0.0075 &  -0.0204\n\\end{bmatrix}</script><h4 id=\"Update-weights-according-to-learning-rate-1\"><a href=\"#Update-weights-according-to-learning-rate-1\" class=\"headerlink\" title=\"Update weights according to learning rate\"></a>Update weights according to learning rate</h4><p>Update the weights with learning rate 0.02we got the final weight matrix</p>\n<script type=\"math/tex; mode=display\">\nW_{ih}^*=\n\\begin{bmatrix}\nW_{i1h1} - \\eta (W_{i1h1}') &  W_{i1h2} - \\eta (W_{i1h2}')\\\\\nW_{i2h1} - \\eta (W_{i2h1}') &  W_{i2h2} - \\eta (W_{i2h2}')\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(0.7 -0.2*(-0.0037))& (0.6 - 0.2 *(-0.0102)) \\\\\n(0.5 - 0.2*(-0.0075)) & (0.4 - 0.2 * (-0.0204))\n\\end{bmatrix}\n\\\\\n=\n\\begin{bmatrix}\n0.7007 & 0.6020\\\\\n0.5015 & 0.4041\n\\end{bmatrix}</script><h3 id=\"5-3-Wrap-up\"><a href=\"#5-3-Wrap-up\" class=\"headerlink\" title=\"5.3 Wrap up\"></a>5.3 Wrap up</h3><p>Finally we get all the weights updated</p>\n<script type=\"math/tex; mode=display\">\nW_{ho}^* = \n\\begin{bmatrix}\n0.4055 & 0.6075\\\\\n0.9057 & 0.8077\n\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\nW_{ih}^*=\n\\begin{bmatrix}\n0.7007 & 0.6020\\\\\n0.5015 & 0.4041\n\\end{bmatrix}</script><h2 id=\"6-Conclusion\"><a href=\"#6-Conclusion\" class=\"headerlink\" title=\"6. Conclusion\"></a>6. Conclusion</h2><ul>\n<li>Backpropagation is beautiful designed architecture. Every gate in the diagram gets some input and makes some output, the gradients of input concerning the output indicates how strongly the gate wants the output to increase or decrease. The communication between these smart gates make it possible for complicated prediction or classification tasks.</li>\n<li>The activation function matters. Take Sigmoid as an example, and we saw the gradients of its gates vanish significantly to 0.00XXX, this will make the rest of backward pass almost to zero due to the multiplication in chain rule. So we should always be nervous in Sigmoid, Relu is possibly a better choice.</li>\n<li>If we look back to the computing process,  a lot can be done when we implement the neural network with codes, such as the caching of gradients when we do forward propagation and the extracting of common gradient computation functions.</li>\n</ul>\n<h2 id=\"7-Reference\"><a href=\"#7-Reference\" class=\"headerlink\" title=\"7. Reference\"></a>7. Reference</h2><ol>\n<li><a href=\"&#109;&#97;&#x69;&#108;&#116;&#111;&#x3a;&#104;&#116;&#116;&#112;&#x73;&#58;&#x2f;&#47;&#x6d;&#x65;&#100;&#x69;&#117;&#x6d;&#x2e;&#x63;&#111;&#109;&#47;&#64;&#x31;&#52;&#112;&#114;&#x61;&#x6b;&#97;&#115;&#104;&#x2f;&#x62;&#97;&#x63;&#107;&#45;&#x70;&#114;&#111;&#112;&#97;&#103;&#97;&#x74;&#x69;&#x6f;&#110;&#45;&#105;&#x73;&#45;&#x76;&#x65;&#x72;&#x79;&#45;&#115;&#x69;&#x6d;&#x70;&#x6c;&#101;&#45;&#x77;&#104;&#x6f;&#x2d;&#x6d;&#97;&#x64;&#101;&#45;&#105;&#x74;&#x2d;&#x63;&#111;&#x6d;&#112;&#x6c;&#x69;&#99;&#x61;&#x74;&#101;&#x64;&#x2d;&#x39;&#x37;&#x62;&#x37;&#x39;&#52;&#x63;&#57;&#55;&#101;&#x35;&#x63;\">&#104;&#116;&#116;&#112;&#x73;&#58;&#x2f;&#47;&#x6d;&#x65;&#100;&#x69;&#117;&#x6d;&#x2e;&#x63;&#111;&#109;&#47;&#64;&#x31;&#52;&#112;&#114;&#x61;&#x6b;&#97;&#115;&#104;&#x2f;&#x62;&#97;&#x63;&#107;&#45;&#x70;&#114;&#111;&#112;&#97;&#103;&#97;&#x74;&#x69;&#x6f;&#110;&#45;&#105;&#x73;&#45;&#x76;&#x65;&#x72;&#x79;&#45;&#115;&#x69;&#x6d;&#x70;&#x6c;&#101;&#45;&#x77;&#104;&#x6f;&#x2d;&#x6d;&#97;&#x64;&#101;&#45;&#105;&#x74;&#x2d;&#x63;&#111;&#x6d;&#112;&#x6c;&#x69;&#99;&#x61;&#x74;&#101;&#x64;&#x2d;&#x39;&#x37;&#x62;&#x37;&#x39;&#52;&#x63;&#57;&#55;&#101;&#x35;&#x63;</a>.</li>\n<li><a href=\"https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\" target=\"_blank\" rel=\"noopener\">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>.  </li>\n<li><a href=\"&#109;&#97;&#x69;&#108;&#x74;&#x6f;&#58;&#x68;&#x74;&#116;&#x70;&#115;&#58;&#x2f;&#47;&#x6d;&#101;&#x64;&#105;&#x75;&#109;&#x2e;&#99;&#x6f;&#x6d;&#47;&#x40;&#107;&#x61;&#x72;&#112;&#x61;&#x74;&#x68;&#x79;&#x2f;&#x79;&#101;&#x73;&#45;&#121;&#111;&#x75;&#45;&#x73;&#104;&#x6f;&#x75;&#x6c;&#x64;&#45;&#117;&#x6e;&#x64;&#x65;&#x72;&#x73;&#x74;&#97;&#x6e;&#100;&#x2d;&#x62;&#x61;&#x63;&#107;&#x70;&#x72;&#111;&#x70;&#45;&#x65;&#x32;&#x66;&#x30;&#54;&#101;&#97;&#x62;&#52;&#57;&#x36;&#x62;\">&#x68;&#x74;&#116;&#x70;&#115;&#58;&#x2f;&#47;&#x6d;&#101;&#x64;&#105;&#x75;&#109;&#x2e;&#99;&#x6f;&#x6d;&#47;&#x40;&#107;&#x61;&#x72;&#112;&#x61;&#x74;&#x68;&#x79;&#x2f;&#x79;&#101;&#x73;&#45;&#121;&#111;&#x75;&#45;&#x73;&#104;&#x6f;&#x75;&#x6c;&#x64;&#45;&#117;&#x6e;&#x64;&#x65;&#x72;&#x73;&#x74;&#97;&#x6e;&#100;&#x2d;&#x62;&#x61;&#x63;&#107;&#x70;&#x72;&#111;&#x70;&#45;&#x65;&#x32;&#x66;&#x30;&#54;&#101;&#97;&#x62;&#52;&#57;&#x36;&#x62;</a></li>\n<li><a href=\"http://cs231n.github.io/optimization-2/\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/optimization-2/</a> I</li>\n<li><a href=\"https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/\" target=\"_blank\" rel=\"noopener\">https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/</a> </li>\n<li><a href=\"https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb\" target=\"_blank\" rel=\"noopener\">https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb</a></li>\n</ol>\n"},{"title":"Basics of words embedding","date":"2018-10-01T11:14:28.000Z","_content":"\n### Why embedding\n\nNatural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing:\n\n1. These encodings provide no useful information regarding the **relationships** that may exist between the individual symbols.\n2. Discrete ids furthermore lead to **data sparsity**. We may need more data to train statistical models successfully.\n\nTo address these two problems, word embeddings provide a solution to  represent words and their relative meanings densely.\n\n### Overview\n\nEmbedding derives from [Vector Space Models(VSMs)](https://en.wikipedia.org/wiki/Vector_space_model),  one of its well-known schemes is [Tf-Idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) weights. VSMs can transfer text documents into vectors of identifies, the fundamental theory they rely on is Distribution hypothesis which states that **words that appear in the same contexts share semantic meaning**. \n\nVSMs have two main approaches: Count-based methods and predictive methods.  Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors regarding learned small, dense *embedding vectors* (considered parameters of the model).\n\nAmong all the embedding methods, Glove(Count-based) and Word2vec(Predictive) are the most popular.\n\n### Count-based Embedding\n\nGloVe is an **unsupervised learning algorithm** for obtaining vector representations for words.  The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. It is designed to enable the vector differences between words  to capture as much as possible the meaning specified by the juxtaposition of two words.\n\nThe [project page of glove](https://nlp.stanford.edu/projects/glove/)  gives detailed information of how glove vectors are computed and provides several pretrained glove word vectors. As the article highlighted , glove was developed with the following consideration:\n\n>The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.\n>\n>Similarity metrics used for nearest neighbor may be problematic when two given words almost always exhibit more intricate relationships than can be captured by a single number. \n>\n>It is necessary for a model to associate more than a single number to the word pair.\n\n\n\nTraining GloVe model on a large corpus can be extremely time consuming, but it is a one-time cost.  [project page of glove](https://nlp.stanford.edu/projects/glove/) also provides some pre-trained word vectors, e.g. glove.6B.zip is word vectors trained from words on Wikipedia, take the first line of this file for example\n\n```\nthe -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 ...\n\n```\n\n''the'' is followed by 100 floats which are the vector values of this word\n\nWe can build a dict whose key is words and value is their glove vector\n\n```python\nembeddings_index = dict()\nf = open('./glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype = 'float32')\n    embeddings_index[word] = coefs\nf.close()\n```\n\nWhen we need to build a embedding layer , we just look up the vectors  for input words in the dict.\n\n### Word2vec\n\nWord2vec, as illustrated in the first part,  is a predictive model for word embedding. There are two main branches of Word2vec, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. The two models predict words in a different direction, CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. Therefore,  CBOW treats an entire context as one observation and is compatible with smaller datasets, while skip-gram treats each context-target pair as a new observation and play better with larger datasets. \n\nWe will focus on skip-gram as we need to deal with large datasets in most time. Here is the structure of this model.\n\n![Skip-gram Neural Network Architecture](https://raw.githubusercontent.com/niuguy/niuguy.github.io/master/pic/skip_gram.npg.png)\n\nThe Skip-gram model is trained like this,  Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the nearby word that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. \n\nThe coding example of how to build and train the Skip-gram model can be found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)\n\n\n\nNext post I will try to use embedding to solve an interesting real world problem.\n\n\n\n","source":"_posts/Basics-of-words-embedding.md","raw":"---\ntitle: Basics of words embedding\ndate: 2018-10-01 11:14:28\ncategories: 'Machine Learning'\n---\n\n### Why embedding\n\nNatural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing:\n\n1. These encodings provide no useful information regarding the **relationships** that may exist between the individual symbols.\n2. Discrete ids furthermore lead to **data sparsity**. We may need more data to train statistical models successfully.\n\nTo address these two problems, word embeddings provide a solution to  represent words and their relative meanings densely.\n\n### Overview\n\nEmbedding derives from [Vector Space Models(VSMs)](https://en.wikipedia.org/wiki/Vector_space_model),  one of its well-known schemes is [Tf-Idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) weights. VSMs can transfer text documents into vectors of identifies, the fundamental theory they rely on is Distribution hypothesis which states that **words that appear in the same contexts share semantic meaning**. \n\nVSMs have two main approaches: Count-based methods and predictive methods.  Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors regarding learned small, dense *embedding vectors* (considered parameters of the model).\n\nAmong all the embedding methods, Glove(Count-based) and Word2vec(Predictive) are the most popular.\n\n### Count-based Embedding\n\nGloVe is an **unsupervised learning algorithm** for obtaining vector representations for words.  The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. It is designed to enable the vector differences between words  to capture as much as possible the meaning specified by the juxtaposition of two words.\n\nThe [project page of glove](https://nlp.stanford.edu/projects/glove/)  gives detailed information of how glove vectors are computed and provides several pretrained glove word vectors. As the article highlighted , glove was developed with the following consideration:\n\n>The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.\n>\n>Similarity metrics used for nearest neighbor may be problematic when two given words almost always exhibit more intricate relationships than can be captured by a single number. \n>\n>It is necessary for a model to associate more than a single number to the word pair.\n\n\n\nTraining GloVe model on a large corpus can be extremely time consuming, but it is a one-time cost.  [project page of glove](https://nlp.stanford.edu/projects/glove/) also provides some pre-trained word vectors, e.g. glove.6B.zip is word vectors trained from words on Wikipedia, take the first line of this file for example\n\n```\nthe -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 ...\n\n```\n\n''the'' is followed by 100 floats which are the vector values of this word\n\nWe can build a dict whose key is words and value is their glove vector\n\n```python\nembeddings_index = dict()\nf = open('./glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype = 'float32')\n    embeddings_index[word] = coefs\nf.close()\n```\n\nWhen we need to build a embedding layer , we just look up the vectors  for input words in the dict.\n\n### Word2vec\n\nWord2vec, as illustrated in the first part,  is a predictive model for word embedding. There are two main branches of Word2vec, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. The two models predict words in a different direction, CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. Therefore,  CBOW treats an entire context as one observation and is compatible with smaller datasets, while skip-gram treats each context-target pair as a new observation and play better with larger datasets. \n\nWe will focus on skip-gram as we need to deal with large datasets in most time. Here is the structure of this model.\n\n![Skip-gram Neural Network Architecture](https://raw.githubusercontent.com/niuguy/niuguy.github.io/master/pic/skip_gram.npg.png)\n\nThe Skip-gram model is trained like this,  Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the nearby word that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. \n\nThe coding example of how to build and train the Skip-gram model can be found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)\n\n\n\nNext post I will try to use embedding to solve an interesting real world problem.\n\n\n\n","slug":"Basics-of-words-embedding","published":1,"updated":"2018-10-04T21:31:24.832Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3br0003m2tqaf3t1uch","content":"<h3 id=\"Why-embedding\"><a href=\"#Why-embedding\" class=\"headerlink\" title=\"Why embedding\"></a>Why embedding</h3><p>Natural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing:</p>\n<ol>\n<li>These encodings provide no useful information regarding the <strong>relationships</strong> that may exist between the individual symbols.</li>\n<li>Discrete ids furthermore lead to <strong>data sparsity</strong>. We may need more data to train statistical models successfully.</li>\n</ol>\n<p>To address these two problems, word embeddings provide a solution to  represent words and their relative meanings densely.</p>\n<h3 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h3><p>Embedding derives from <a href=\"https://en.wikipedia.org/wiki/Vector_space_model\" target=\"_blank\" rel=\"noopener\">Vector Space Models(VSMs)</a>,  one of its well-known schemes is <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\" rel=\"noopener\">Tf-Idf</a> weights. VSMs can transfer text documents into vectors of identifies, the fundamental theory they rely on is Distribution hypothesis which states that <strong>words that appear in the same contexts share semantic meaning</strong>. </p>\n<p>VSMs have two main approaches: Count-based methods and predictive methods.  Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors regarding learned small, dense <em>embedding vectors</em> (considered parameters of the model).</p>\n<p>Among all the embedding methods, Glove(Count-based) and Word2vec(Predictive) are the most popular.</p>\n<h3 id=\"Count-based-Embedding\"><a href=\"#Count-based-Embedding\" class=\"headerlink\" title=\"Count-based Embedding\"></a>Count-based Embedding</h3><p>GloVe is an <strong>unsupervised learning algorithm</strong> for obtaining vector representations for words.  The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. It is designed to enable the vector differences between words  to capture as much as possible the meaning specified by the juxtaposition of two words.</p>\n<p>The <a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\" rel=\"noopener\">project page of glove</a>  gives detailed information of how glove vectors are computed and provides several pretrained glove word vectors. As the article highlighted , glove was developed with the following consideration:</p>\n<blockquote>\n<p>The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.</p>\n<p>Similarity metrics used for nearest neighbor may be problematic when two given words almost always exhibit more intricate relationships than can be captured by a single number. </p>\n<p>It is necessary for a model to associate more than a single number to the word pair.</p>\n</blockquote>\n<p>Training GloVe model on a large corpus can be extremely time consuming, but it is a one-time cost.  <a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\" rel=\"noopener\">project page of glove</a> also provides some pre-trained word vectors, e.g. glove.6B.zip is word vectors trained from words on Wikipedia, take the first line of this file for example</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 ...</span><br></pre></td></tr></table></figure>\n<p>the is followed by 100 floats which are the vector values of this word</p>\n<p>We can build a dict whose key is words and value is their glove vector</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">embeddings_index = dict()</span><br><span class=\"line\">f = open(<span class=\"string\">'./glove.6B.100d.txt'</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> f:</span><br><span class=\"line\">    values = line.split()</span><br><span class=\"line\">    word = values[<span class=\"number\">0</span>]</span><br><span class=\"line\">    coefs = asarray(values[<span class=\"number\">1</span>:], dtype = <span class=\"string\">'float32'</span>)</span><br><span class=\"line\">    embeddings_index[word] = coefs</span><br><span class=\"line\">f.close()</span><br></pre></td></tr></table></figure>\n<p>When we need to build a embedding layer , we just look up the vectors  for input words in the dict.</p>\n<h3 id=\"Word2vec\"><a href=\"#Word2vec\" class=\"headerlink\" title=\"Word2vec\"></a>Word2vec</h3><p>Word2vec, as illustrated in the first part,  is a predictive model for word embedding. There are two main branches of Word2vec, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. The two models predict words in a different direction, CBOW predicts target words (e.g. mat) from source context words (the cat sits on the), while the skip-gram does the inverse and predicts source context-words from the target words. Therefore,  CBOW treats an entire context as one observation and is compatible with smaller datasets, while skip-gram treats each context-target pair as a new observation and play better with larger datasets. </p>\n<p>We will focus on skip-gram as we need to deal with large datasets in most time. Here is the structure of this model.</p>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/niuguy.github.io/master/pic/skip_gram.npg.png\" alt=\"Skip-gram Neural Network Architecture\"></p>\n<p>The Skip-gram model is trained like this,  Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the nearby word that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. </p>\n<p>The coding example of how to build and train the Skip-gram model can be found <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\" target=\"_blank\" rel=\"noopener\">here</a></p>\n<p>Next post I will try to use embedding to solve an interesting real world problem.</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Why-embedding\"><a href=\"#Why-embedding\" class=\"headerlink\" title=\"Why embedding\"></a>Why embedding</h3><p>Natural language processing systems traditionally treat words as discrete atomic symbols, and this may lead to some obstacles in word preprocessing:</p>\n<ol>\n<li>These encodings provide no useful information regarding the <strong>relationships</strong> that may exist between the individual symbols.</li>\n<li>Discrete ids furthermore lead to <strong>data sparsity</strong>. We may need more data to train statistical models successfully.</li>\n</ol>\n<p>To address these two problems, word embeddings provide a solution to  represent words and their relative meanings densely.</p>\n<h3 id=\"Overview\"><a href=\"#Overview\" class=\"headerlink\" title=\"Overview\"></a>Overview</h3><p>Embedding derives from <a href=\"https://en.wikipedia.org/wiki/Vector_space_model\" target=\"_blank\" rel=\"noopener\">Vector Space Models(VSMs)</a>,  one of its well-known schemes is <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\" rel=\"noopener\">Tf-Idf</a> weights. VSMs can transfer text documents into vectors of identifies, the fundamental theory they rely on is Distribution hypothesis which states that <strong>words that appear in the same contexts share semantic meaning</strong>. </p>\n<p>VSMs have two main approaches: Count-based methods and predictive methods.  Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors regarding learned small, dense <em>embedding vectors</em> (considered parameters of the model).</p>\n<p>Among all the embedding methods, Glove(Count-based) and Word2vec(Predictive) are the most popular.</p>\n<h3 id=\"Count-based-Embedding\"><a href=\"#Count-based-Embedding\" class=\"headerlink\" title=\"Count-based Embedding\"></a>Count-based Embedding</h3><p>GloVe is an <strong>unsupervised learning algorithm</strong> for obtaining vector representations for words.  The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. It is designed to enable the vector differences between words  to capture as much as possible the meaning specified by the juxtaposition of two words.</p>\n<p>The <a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\" rel=\"noopener\">project page of glove</a>  gives detailed information of how glove vectors are computed and provides several pretrained glove word vectors. As the article highlighted , glove was developed with the following consideration:</p>\n<blockquote>\n<p>The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.</p>\n<p>Similarity metrics used for nearest neighbor may be problematic when two given words almost always exhibit more intricate relationships than can be captured by a single number. </p>\n<p>It is necessary for a model to associate more than a single number to the word pair.</p>\n</blockquote>\n<p>Training GloVe model on a large corpus can be extremely time consuming, but it is a one-time cost.  <a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\" rel=\"noopener\">project page of glove</a> also provides some pre-trained word vectors, e.g. glove.6B.zip is word vectors trained from words on Wikipedia, take the first line of this file for example</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 ...</span><br></pre></td></tr></table></figure>\n<p>the is followed by 100 floats which are the vector values of this word</p>\n<p>We can build a dict whose key is words and value is their glove vector</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">embeddings_index = dict()</span><br><span class=\"line\">f = open(<span class=\"string\">'./glove.6B.100d.txt'</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> f:</span><br><span class=\"line\">    values = line.split()</span><br><span class=\"line\">    word = values[<span class=\"number\">0</span>]</span><br><span class=\"line\">    coefs = asarray(values[<span class=\"number\">1</span>:], dtype = <span class=\"string\">'float32'</span>)</span><br><span class=\"line\">    embeddings_index[word] = coefs</span><br><span class=\"line\">f.close()</span><br></pre></td></tr></table></figure>\n<p>When we need to build a embedding layer , we just look up the vectors  for input words in the dict.</p>\n<h3 id=\"Word2vec\"><a href=\"#Word2vec\" class=\"headerlink\" title=\"Word2vec\"></a>Word2vec</h3><p>Word2vec, as illustrated in the first part,  is a predictive model for word embedding. There are two main branches of Word2vec, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. The two models predict words in a different direction, CBOW predicts target words (e.g. mat) from source context words (the cat sits on the), while the skip-gram does the inverse and predicts source context-words from the target words. Therefore,  CBOW treats an entire context as one observation and is compatible with smaller datasets, while skip-gram treats each context-target pair as a new observation and play better with larger datasets. </p>\n<p>We will focus on skip-gram as we need to deal with large datasets in most time. Here is the structure of this model.</p>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/niuguy.github.io/master/pic/skip_gram.npg.png\" alt=\"Skip-gram Neural Network Architecture\"></p>\n<p>The Skip-gram model is trained like this,  Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the nearby word that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. </p>\n<p>The coding example of how to build and train the Skip-gram model can be found <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py\" target=\"_blank\" rel=\"noopener\">here</a></p>\n<p>Next post I will try to use embedding to solve an interesting real world problem.</p>\n"},{"title":"[Pandas]Handle missing data","date":"2018-09-23T20:40:37.000Z","_content":"\nMissing data is a common problem in real data preprocessing, luckily pandas has done a lot to help us handle it. This article will show the codes on how to do it.\n\n###Produce some data with missing values.\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f', 'h'],\n                 columns=['one', 'two', 'three'])\ndf['four']='bar'\ndf['five']= df['one'] > 0\ndf['date']= pd.Timestamp('2018-01-01')\ndf2 = df.reindex(['a','b', 'c','d', 'e', 'f','g', 'h'])\ndf2\n```\n\n|      | one       | two       | three     | four | five  | date       |\n| ---- | --------- | --------- | --------- | ---- | ----- | ---------- |\n| a    | -0.614301 | 0.628725  | -0.903163 | bar  | False | 2018-01-01 |\n| b    | NaN       | NaN       | NaN       | NaN  | NaN   | NaT        |\n| c    | -0.297019 | 1.357266  | -0.665749 | bar  | False | 2018-01-01 |\n| d    | NaN       | NaN       | NaN       | NaN  | NaN   | NaT        |\n| e    | 0.311524  | -0.328388 | 0.777467  | bar  | True  | 2018-01-01 |\n| f    | 0.572373  | -0.309563 | -0.296276 | bar  | True  | 2018-01-01 |\n| g    | NaN       | NaN       | NaN       | NaN  | NaN   | NaT        |\n| h    | -1.303842 | 1.239911  | -0.909255 | bar  | False | 2018-01-01 |\n\nNotice that NaN is the default marker of missing number, text or boolean, NaT is for missing datetime\n\n### Detect missing value\n\npandas provides two methods, isna() and notna()\n\n```python\ndf2.isna()\n```\n\n|      | one   | two   | three | four  | five  | date  |\n| ---- | ----- | ----- | ----- | ----- | ----- | ----- |\n| a    | False | False | False | False | False | False |\n| b    | True  | True  | True  | True  | True  | True  |\n| c    | False | False | False | False | False | False |\n| d    | True  | True  | True  | True  | True  | True  |\n| e    | False | False | False | False | False | False |\n| f    | False | False | False | False | False | False |\n| g    | True  | True  | True  | True  | True  | True  |\n| h    | False | False | False | False | False | False |\n\n```python\ndf2.notna()\n```\n\n|      | one   | two   | three | four  | five  | date  |\n| ---- | ----- | ----- | ----- | ----- | ----- | ----- |\n| a    | True  | True  | True  | True  | True  | True  |\n| b    | False | False | False | False | False | False |\n| c    | True  | True  | True  | True  | True  | True  |\n| d    | False | False | False | False | False | False |\n| e    | True  | True  | True  | True  | True  | True  |\n| f    | True  | True  | True  | True  | True  | True  |\n| g    | False | False | False | False | False | False |\n| h    | True  | True  | True  | True  | True  | True  |\n\n### Cleaning\n\ndrop na value, dropna() has a parameter axis which indicates either colums(axis = 1) or rows(axis = 0) will be dropped, the default value of axis is 0\n\n```python\ndf2.dropna()\n```\n\n|      | one       | two       | three     | four | five  | date       |\n| ---- | --------- | --------- | --------- | ---- | ----- | ---------- |\n| a    | -0.614301 | 0.628725  | -0.903163 | bar  | False | 2018-01-01 |\n| c    | -0.297019 | 1.357266  | -0.665749 | bar  | False | 2018-01-01 |\n| e    | 0.311524  | -0.328388 | 0.777467  | bar  | True  | 2018-01-01 |\n| f    | 0.572373  | -0.309563 | -0.296276 | bar  | True  | 2018-01-01 |\n| h    | -1.303842 | 1.239911  | -0.909255 | bar  | False | 2018-01-01 |\n\nDrop columns with missing data\n\n```python\ndf2.dropna(axis = 1)\n```\n\n|      |\n| ---- |\n| a    |\n| b    |\n| c    |\n| d    |\n| e    |\n| f    |\n| g    |\n| h    |\n\nAll columns are removed..\n\n### Fill na\n\nNa value can be easily filled by fillna()\n\n```python\ndf2.fillna(0)\n```\n\n|      | one       | two       | three     | four | five  | date                |\n| ---- | --------- | --------- | --------- | ---- | ----- | ------------------- |\n| a    | -0.614301 | 0.628725  | -0.903163 | bar  | False | 2018-01-01 00:00:00 |\n| b    | 0.000000  | 0.000000  | 0.000000  | 0    | 0     | 0                   |\n| c    | -0.297019 | 1.357266  | -0.665749 | bar  | False | 2018-01-01 00:00:00 |\n| d    | 0.000000  | 0.000000  | 0.000000  | 0    | 0     | 0                   |\n| e    | 0.311524  | -0.328388 | 0.777467  | bar  | True  | 2018-01-01 00:00:00 |\n| f    | 0.572373  | -0.309563 | -0.296276 | bar  | True  | 2018-01-01 00:00:00 |\n| g    | 0.000000  | 0.000000  | 0.000000  | 0    | 0     | 0                   |\n| h    | -1.303842 | 1.239911  | -0.909255 | bar  | False | 2018-01-01 00:00:00 |\n\nWe can also fill data with respect to their column property\n\n```python\ndf2.fillna(value = {'one': 0, 'four': 'missing', 'five': False, 'date': pd.Timestamp('2000-01-01')})\n```\n\n|      | one       | two       | three     | four    | five  | date       |\n| ---- | --------- | --------- | --------- | ------- | ----- | ---------- |\n| a    | -0.614301 | 0.628725  | -0.903163 | bar     | False | 2018-01-01 |\n| b    | 0.000000  | NaN       | NaN       | missing | False | 2000-01-01 |\n| c    | -0.297019 | 1.357266  | -0.665749 | bar     | False | 2018-01-01 |\n| d    | 0.000000  | NaN       | NaN       | missing | False | 2000-01-01 |\n| e    | 0.311524  | -0.328388 | 0.777467  | bar     | True  | 2018-01-01 |\n| f    | 0.572373  | -0.309563 | -0.296276 | bar     | True  | 2018-01-01 |\n| g    | 0.000000  | NaN       | NaN       | missing | False | 2000-01-01 |\n| h    | -1.303842 | 1.239911  | -0.909255 | bar     | False | 2018-01-01 |\n\nRemind that fillna() will not replace na in orignial dataframe, in other words, the df2 remains the same after all the above operation, If you want it to be permanantely changed, add parameter inplace = True like this.\n\n```python\ndf2.fillna(value = 0, inplace = True)\n```\n\n","source":"_posts/Pandas-Handle-missing-data.md","raw":"---\ntitle: '[Pandas]Handle missing data'\ndate: 2018-09-23 20:40:37\ncategories: 'pandas'\n---\n\nMissing data is a common problem in real data preprocessing, luckily pandas has done a lot to help us handle it. This article will show the codes on how to do it.\n\n###Produce some data with missing values.\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(5, 3), index=['a', 'c', 'e', 'f', 'h'],\n                 columns=['one', 'two', 'three'])\ndf['four']='bar'\ndf['five']= df['one'] > 0\ndf['date']= pd.Timestamp('2018-01-01')\ndf2 = df.reindex(['a','b', 'c','d', 'e', 'f','g', 'h'])\ndf2\n```\n\n|      | one       | two       | three     | four | five  | date       |\n| ---- | --------- | --------- | --------- | ---- | ----- | ---------- |\n| a    | -0.614301 | 0.628725  | -0.903163 | bar  | False | 2018-01-01 |\n| b    | NaN       | NaN       | NaN       | NaN  | NaN   | NaT        |\n| c    | -0.297019 | 1.357266  | -0.665749 | bar  | False | 2018-01-01 |\n| d    | NaN       | NaN       | NaN       | NaN  | NaN   | NaT        |\n| e    | 0.311524  | -0.328388 | 0.777467  | bar  | True  | 2018-01-01 |\n| f    | 0.572373  | -0.309563 | -0.296276 | bar  | True  | 2018-01-01 |\n| g    | NaN       | NaN       | NaN       | NaN  | NaN   | NaT        |\n| h    | -1.303842 | 1.239911  | -0.909255 | bar  | False | 2018-01-01 |\n\nNotice that NaN is the default marker of missing number, text or boolean, NaT is for missing datetime\n\n### Detect missing value\n\npandas provides two methods, isna() and notna()\n\n```python\ndf2.isna()\n```\n\n|      | one   | two   | three | four  | five  | date  |\n| ---- | ----- | ----- | ----- | ----- | ----- | ----- |\n| a    | False | False | False | False | False | False |\n| b    | True  | True  | True  | True  | True  | True  |\n| c    | False | False | False | False | False | False |\n| d    | True  | True  | True  | True  | True  | True  |\n| e    | False | False | False | False | False | False |\n| f    | False | False | False | False | False | False |\n| g    | True  | True  | True  | True  | True  | True  |\n| h    | False | False | False | False | False | False |\n\n```python\ndf2.notna()\n```\n\n|      | one   | two   | three | four  | five  | date  |\n| ---- | ----- | ----- | ----- | ----- | ----- | ----- |\n| a    | True  | True  | True  | True  | True  | True  |\n| b    | False | False | False | False | False | False |\n| c    | True  | True  | True  | True  | True  | True  |\n| d    | False | False | False | False | False | False |\n| e    | True  | True  | True  | True  | True  | True  |\n| f    | True  | True  | True  | True  | True  | True  |\n| g    | False | False | False | False | False | False |\n| h    | True  | True  | True  | True  | True  | True  |\n\n### Cleaning\n\ndrop na value, dropna() has a parameter axis which indicates either colums(axis = 1) or rows(axis = 0) will be dropped, the default value of axis is 0\n\n```python\ndf2.dropna()\n```\n\n|      | one       | two       | three     | four | five  | date       |\n| ---- | --------- | --------- | --------- | ---- | ----- | ---------- |\n| a    | -0.614301 | 0.628725  | -0.903163 | bar  | False | 2018-01-01 |\n| c    | -0.297019 | 1.357266  | -0.665749 | bar  | False | 2018-01-01 |\n| e    | 0.311524  | -0.328388 | 0.777467  | bar  | True  | 2018-01-01 |\n| f    | 0.572373  | -0.309563 | -0.296276 | bar  | True  | 2018-01-01 |\n| h    | -1.303842 | 1.239911  | -0.909255 | bar  | False | 2018-01-01 |\n\nDrop columns with missing data\n\n```python\ndf2.dropna(axis = 1)\n```\n\n|      |\n| ---- |\n| a    |\n| b    |\n| c    |\n| d    |\n| e    |\n| f    |\n| g    |\n| h    |\n\nAll columns are removed..\n\n### Fill na\n\nNa value can be easily filled by fillna()\n\n```python\ndf2.fillna(0)\n```\n\n|      | one       | two       | three     | four | five  | date                |\n| ---- | --------- | --------- | --------- | ---- | ----- | ------------------- |\n| a    | -0.614301 | 0.628725  | -0.903163 | bar  | False | 2018-01-01 00:00:00 |\n| b    | 0.000000  | 0.000000  | 0.000000  | 0    | 0     | 0                   |\n| c    | -0.297019 | 1.357266  | -0.665749 | bar  | False | 2018-01-01 00:00:00 |\n| d    | 0.000000  | 0.000000  | 0.000000  | 0    | 0     | 0                   |\n| e    | 0.311524  | -0.328388 | 0.777467  | bar  | True  | 2018-01-01 00:00:00 |\n| f    | 0.572373  | -0.309563 | -0.296276 | bar  | True  | 2018-01-01 00:00:00 |\n| g    | 0.000000  | 0.000000  | 0.000000  | 0    | 0     | 0                   |\n| h    | -1.303842 | 1.239911  | -0.909255 | bar  | False | 2018-01-01 00:00:00 |\n\nWe can also fill data with respect to their column property\n\n```python\ndf2.fillna(value = {'one': 0, 'four': 'missing', 'five': False, 'date': pd.Timestamp('2000-01-01')})\n```\n\n|      | one       | two       | three     | four    | five  | date       |\n| ---- | --------- | --------- | --------- | ------- | ----- | ---------- |\n| a    | -0.614301 | 0.628725  | -0.903163 | bar     | False | 2018-01-01 |\n| b    | 0.000000  | NaN       | NaN       | missing | False | 2000-01-01 |\n| c    | -0.297019 | 1.357266  | -0.665749 | bar     | False | 2018-01-01 |\n| d    | 0.000000  | NaN       | NaN       | missing | False | 2000-01-01 |\n| e    | 0.311524  | -0.328388 | 0.777467  | bar     | True  | 2018-01-01 |\n| f    | 0.572373  | -0.309563 | -0.296276 | bar     | True  | 2018-01-01 |\n| g    | 0.000000  | NaN       | NaN       | missing | False | 2000-01-01 |\n| h    | -1.303842 | 1.239911  | -0.909255 | bar     | False | 2018-01-01 |\n\nRemind that fillna() will not replace na in orignial dataframe, in other words, the df2 remains the same after all the above operation, If you want it to be permanantely changed, add parameter inplace = True like this.\n\n```python\ndf2.fillna(value = 0, inplace = True)\n```\n\n","slug":"Pandas-Handle-missing-data","published":1,"updated":"2018-09-23T22:15:39.816Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3bu0005m2tqgv7e5j1e","content":"<p>Missing data is a common problem in real data preprocessing, luckily pandas has done a lot to help us handle it. This article will show the codes on how to do it.</p>\n<h3 id=\"Produce-some-data-with-missing-values\"><a href=\"#Produce-some-data-with-missing-values\" class=\"headerlink\" title=\"Produce some data with missing values.\"></a>Produce some data with missing values.</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">5</span>, <span class=\"number\">3</span>), index=[<span class=\"string\">'a'</span>, <span class=\"string\">'c'</span>, <span class=\"string\">'e'</span>, <span class=\"string\">'f'</span>, <span class=\"string\">'h'</span>],</span><br><span class=\"line\">                 columns=[<span class=\"string\">'one'</span>, <span class=\"string\">'two'</span>, <span class=\"string\">'three'</span>])</span><br><span class=\"line\">df[<span class=\"string\">'four'</span>]=<span class=\"string\">'bar'</span></span><br><span class=\"line\">df[<span class=\"string\">'five'</span>]= df[<span class=\"string\">'one'</span>] &gt; <span class=\"number\">0</span></span><br><span class=\"line\">df[<span class=\"string\">'date'</span>]= pd.Timestamp(<span class=\"string\">'2018-01-01'</span>)</span><br><span class=\"line\">df2 = df.reindex([<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>, <span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>, <span class=\"string\">'e'</span>, <span class=\"string\">'f'</span>,<span class=\"string\">'g'</span>, <span class=\"string\">'h'</span>])</span><br><span class=\"line\">df2</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>-0.614301</td>\n<td>0.628725</td>\n<td>-0.903163</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>b</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaT</td>\n</tr>\n<tr>\n<td>c</td>\n<td>-0.297019</td>\n<td>1.357266</td>\n<td>-0.665749</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>d</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaT</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.311524</td>\n<td>-0.328388</td>\n<td>0.777467</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>f</td>\n<td>0.572373</td>\n<td>-0.309563</td>\n<td>-0.296276</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>g</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaT</td>\n</tr>\n<tr>\n<td>h</td>\n<td>-1.303842</td>\n<td>1.239911</td>\n<td>-0.909255</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Notice that NaN is the default marker of missing number, text or boolean, NaT is for missing datetime</p>\n<h3 id=\"Detect-missing-value\"><a href=\"#Detect-missing-value\" class=\"headerlink\" title=\"Detect missing value\"></a>Detect missing value</h3><p>pandas provides two methods, isna() and notna()</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.isna()</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>b</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>c</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>d</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>e</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>f</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>g</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>h</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n</tbody>\n</table>\n</div>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.notna()</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>b</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>c</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>d</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>e</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>f</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>g</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>h</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Cleaning\"><a href=\"#Cleaning\" class=\"headerlink\" title=\"Cleaning\"></a>Cleaning</h3><p>drop na value, dropna() has a parameter axis which indicates either colums(axis = 1) or rows(axis = 0) will be dropped, the default value of axis is 0</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.dropna()</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>-0.614301</td>\n<td>0.628725</td>\n<td>-0.903163</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>c</td>\n<td>-0.297019</td>\n<td>1.357266</td>\n<td>-0.665749</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.311524</td>\n<td>-0.328388</td>\n<td>0.777467</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>f</td>\n<td>0.572373</td>\n<td>-0.309563</td>\n<td>-0.296276</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>h</td>\n<td>-1.303842</td>\n<td>1.239911</td>\n<td>-0.909255</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Drop columns with missing data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.dropna(axis = <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n</tr>\n<tr>\n<td>b</td>\n</tr>\n<tr>\n<td>c</td>\n</tr>\n<tr>\n<td>d</td>\n</tr>\n<tr>\n<td>e</td>\n</tr>\n<tr>\n<td>f</td>\n</tr>\n<tr>\n<td>g</td>\n</tr>\n<tr>\n<td>h</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>All columns are removed..</p>\n<h3 id=\"Fill-na\"><a href=\"#Fill-na\" class=\"headerlink\" title=\"Fill na\"></a>Fill na</h3><p>Na value can be easily filled by fillna()</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.fillna(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>-0.614301</td>\n<td>0.628725</td>\n<td>-0.903163</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>c</td>\n<td>-0.297019</td>\n<td>1.357266</td>\n<td>-0.665749</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n<tr>\n<td>d</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.311524</td>\n<td>-0.328388</td>\n<td>0.777467</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n<tr>\n<td>f</td>\n<td>0.572373</td>\n<td>-0.309563</td>\n<td>-0.296276</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n<tr>\n<td>g</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>h</td>\n<td>-1.303842</td>\n<td>1.239911</td>\n<td>-0.909255</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>We can also fill data with respect to their column property</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.fillna(value = &#123;<span class=\"string\">'one'</span>: <span class=\"number\">0</span>, <span class=\"string\">'four'</span>: <span class=\"string\">'missing'</span>, <span class=\"string\">'five'</span>: <span class=\"keyword\">False</span>, <span class=\"string\">'date'</span>: pd.Timestamp(<span class=\"string\">'2000-01-01'</span>)&#125;)</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>-0.614301</td>\n<td>0.628725</td>\n<td>-0.903163</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.000000</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>missing</td>\n<td>False</td>\n<td>2000-01-01</td>\n</tr>\n<tr>\n<td>c</td>\n<td>-0.297019</td>\n<td>1.357266</td>\n<td>-0.665749</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>d</td>\n<td>0.000000</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>missing</td>\n<td>False</td>\n<td>2000-01-01</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.311524</td>\n<td>-0.328388</td>\n<td>0.777467</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>f</td>\n<td>0.572373</td>\n<td>-0.309563</td>\n<td>-0.296276</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>g</td>\n<td>0.000000</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>missing</td>\n<td>False</td>\n<td>2000-01-01</td>\n</tr>\n<tr>\n<td>h</td>\n<td>-1.303842</td>\n<td>1.239911</td>\n<td>-0.909255</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Remind that fillna() will not replace na in orignial dataframe, in other words, the df2 remains the same after all the above operation, If you want it to be permanantely changed, add parameter inplace = True like this.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.fillna(value = <span class=\"number\">0</span>, inplace = <span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>Missing data is a common problem in real data preprocessing, luckily pandas has done a lot to help us handle it. This article will show the codes on how to do it.</p>\n<h3 id=\"Produce-some-data-with-missing-values\"><a href=\"#Produce-some-data-with-missing-values\" class=\"headerlink\" title=\"Produce some data with missing values.\"></a>Produce some data with missing values.</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(np.random.randn(<span class=\"number\">5</span>, <span class=\"number\">3</span>), index=[<span class=\"string\">'a'</span>, <span class=\"string\">'c'</span>, <span class=\"string\">'e'</span>, <span class=\"string\">'f'</span>, <span class=\"string\">'h'</span>],</span><br><span class=\"line\">                 columns=[<span class=\"string\">'one'</span>, <span class=\"string\">'two'</span>, <span class=\"string\">'three'</span>])</span><br><span class=\"line\">df[<span class=\"string\">'four'</span>]=<span class=\"string\">'bar'</span></span><br><span class=\"line\">df[<span class=\"string\">'five'</span>]= df[<span class=\"string\">'one'</span>] &gt; <span class=\"number\">0</span></span><br><span class=\"line\">df[<span class=\"string\">'date'</span>]= pd.Timestamp(<span class=\"string\">'2018-01-01'</span>)</span><br><span class=\"line\">df2 = df.reindex([<span class=\"string\">'a'</span>,<span class=\"string\">'b'</span>, <span class=\"string\">'c'</span>,<span class=\"string\">'d'</span>, <span class=\"string\">'e'</span>, <span class=\"string\">'f'</span>,<span class=\"string\">'g'</span>, <span class=\"string\">'h'</span>])</span><br><span class=\"line\">df2</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>-0.614301</td>\n<td>0.628725</td>\n<td>-0.903163</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>b</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaT</td>\n</tr>\n<tr>\n<td>c</td>\n<td>-0.297019</td>\n<td>1.357266</td>\n<td>-0.665749</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>d</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaT</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.311524</td>\n<td>-0.328388</td>\n<td>0.777467</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>f</td>\n<td>0.572373</td>\n<td>-0.309563</td>\n<td>-0.296276</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>g</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>NaT</td>\n</tr>\n<tr>\n<td>h</td>\n<td>-1.303842</td>\n<td>1.239911</td>\n<td>-0.909255</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Notice that NaN is the default marker of missing number, text or boolean, NaT is for missing datetime</p>\n<h3 id=\"Detect-missing-value\"><a href=\"#Detect-missing-value\" class=\"headerlink\" title=\"Detect missing value\"></a>Detect missing value</h3><p>pandas provides two methods, isna() and notna()</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.isna()</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>b</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>c</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>d</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>e</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>f</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>g</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>h</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n</tbody>\n</table>\n</div>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.notna()</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>b</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>c</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>d</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>e</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>f</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n<tr>\n<td>g</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n<td>False</td>\n</tr>\n<tr>\n<td>h</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n<td>True</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Cleaning\"><a href=\"#Cleaning\" class=\"headerlink\" title=\"Cleaning\"></a>Cleaning</h3><p>drop na value, dropna() has a parameter axis which indicates either colums(axis = 1) or rows(axis = 0) will be dropped, the default value of axis is 0</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.dropna()</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>-0.614301</td>\n<td>0.628725</td>\n<td>-0.903163</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>c</td>\n<td>-0.297019</td>\n<td>1.357266</td>\n<td>-0.665749</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.311524</td>\n<td>-0.328388</td>\n<td>0.777467</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>f</td>\n<td>0.572373</td>\n<td>-0.309563</td>\n<td>-0.296276</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>h</td>\n<td>-1.303842</td>\n<td>1.239911</td>\n<td>-0.909255</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Drop columns with missing data</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.dropna(axis = <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n</tr>\n<tr>\n<td>b</td>\n</tr>\n<tr>\n<td>c</td>\n</tr>\n<tr>\n<td>d</td>\n</tr>\n<tr>\n<td>e</td>\n</tr>\n<tr>\n<td>f</td>\n</tr>\n<tr>\n<td>g</td>\n</tr>\n<tr>\n<td>h</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>All columns are removed..</p>\n<h3 id=\"Fill-na\"><a href=\"#Fill-na\" class=\"headerlink\" title=\"Fill na\"></a>Fill na</h3><p>Na value can be easily filled by fillna()</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.fillna(<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>-0.614301</td>\n<td>0.628725</td>\n<td>-0.903163</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>c</td>\n<td>-0.297019</td>\n<td>1.357266</td>\n<td>-0.665749</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n<tr>\n<td>d</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.311524</td>\n<td>-0.328388</td>\n<td>0.777467</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n<tr>\n<td>f</td>\n<td>0.572373</td>\n<td>-0.309563</td>\n<td>-0.296276</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n<tr>\n<td>g</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0.000000</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>h</td>\n<td>-1.303842</td>\n<td>1.239911</td>\n<td>-0.909255</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01 00:00:00</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>We can also fill data with respect to their column property</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.fillna(value = &#123;<span class=\"string\">'one'</span>: <span class=\"number\">0</span>, <span class=\"string\">'four'</span>: <span class=\"string\">'missing'</span>, <span class=\"string\">'five'</span>: <span class=\"keyword\">False</span>, <span class=\"string\">'date'</span>: pd.Timestamp(<span class=\"string\">'2000-01-01'</span>)&#125;)</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>one</th>\n<th>two</th>\n<th>three</th>\n<th>four</th>\n<th>five</th>\n<th>date</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>-0.614301</td>\n<td>0.628725</td>\n<td>-0.903163</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.000000</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>missing</td>\n<td>False</td>\n<td>2000-01-01</td>\n</tr>\n<tr>\n<td>c</td>\n<td>-0.297019</td>\n<td>1.357266</td>\n<td>-0.665749</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>d</td>\n<td>0.000000</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>missing</td>\n<td>False</td>\n<td>2000-01-01</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.311524</td>\n<td>-0.328388</td>\n<td>0.777467</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>f</td>\n<td>0.572373</td>\n<td>-0.309563</td>\n<td>-0.296276</td>\n<td>bar</td>\n<td>True</td>\n<td>2018-01-01</td>\n</tr>\n<tr>\n<td>g</td>\n<td>0.000000</td>\n<td>NaN</td>\n<td>NaN</td>\n<td>missing</td>\n<td>False</td>\n<td>2000-01-01</td>\n</tr>\n<tr>\n<td>h</td>\n<td>-1.303842</td>\n<td>1.239911</td>\n<td>-0.909255</td>\n<td>bar</td>\n<td>False</td>\n<td>2018-01-01</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Remind that fillna() will not replace na in orignial dataframe, in other words, the df2 remains the same after all the above operation, If you want it to be permanantely changed, add parameter inplace = True like this.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df2.fillna(value = <span class=\"number\">0</span>, inplace = <span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure>\n"},{"title":"[Pandas]How to calculate datetime difference in years","date":"2018-10-22T16:43:34.000Z","_content":"\nSuppose you got a person's regist time in one column and his birth date in another column, now you need to calculate his age when he did the registration. There are two ways to reach this result.\n\n1. With the help of relativedelta\n\n```Python\nfrom dateutil import relativedelta\nfor i in df.index:\n    df.loc[i, 'age'] = relativedelta.relativedelta(df.loc[i, 'regDate'], df.loc[i, 'DateOfBirth']).years\n   \n```\n\n\n\n2. With the help of np.timedelta64\n\n```Python\nimport numpy as np\n\ndf['age'] = (df['regDate'] - df['DateOfBirth'])/np.timedelta64(1, 'Y')\n# You may need to convert to integer\ndf['age'].apply(np.int64)\n```\n\n","source":"_posts/Pandas-How-to-calculate-datetime-difference-in-years.md","raw":"---\ntitle: '[Pandas]How to calculate datetime difference in years'\ndate: 2018-10-22 16:43:34\ncategories: 'Pandas'\n---\n\nSuppose you got a person's regist time in one column and his birth date in another column, now you need to calculate his age when he did the registration. There are two ways to reach this result.\n\n1. With the help of relativedelta\n\n```Python\nfrom dateutil import relativedelta\nfor i in df.index:\n    df.loc[i, 'age'] = relativedelta.relativedelta(df.loc[i, 'regDate'], df.loc[i, 'DateOfBirth']).years\n   \n```\n\n\n\n2. With the help of np.timedelta64\n\n```Python\nimport numpy as np\n\ndf['age'] = (df['regDate'] - df['DateOfBirth'])/np.timedelta64(1, 'Y')\n# You may need to convert to integer\ndf['age'].apply(np.int64)\n```\n\n","slug":"Pandas-How-to-calculate-datetime-difference-in-years","published":1,"updated":"2018-10-22T15:57:22.627Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3bv0006m2tq668fydig","content":"<p>Suppose you got a persons regist time in one column and his birth date in another column, now you need to calculate his age when he did the registration. There are two ways to reach this result.</p>\n<ol>\n<li>With the help of relativedelta</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> dateutil <span class=\"keyword\">import</span> relativedelta</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> df.index:</span><br><span class=\"line\">    df.loc[i, <span class=\"string\">'age'</span>] = relativedelta.relativedelta(df.loc[i, <span class=\"string\">'regDate'</span>], df.loc[i, <span class=\"string\">'DateOfBirth'</span>]).years</span><br></pre></td></tr></table></figure>\n<ol>\n<li>With the help of np.timedelta64</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df[<span class=\"string\">'age'</span>] = (df[<span class=\"string\">'regDate'</span>] - df[<span class=\"string\">'DateOfBirth'</span>])/np.timedelta64(<span class=\"number\">1</span>, <span class=\"string\">'Y'</span>)</span><br><span class=\"line\"><span class=\"comment\"># You may need to convert to integer</span></span><br><span class=\"line\">df[<span class=\"string\">'age'</span>].apply(np.int64)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>Suppose you got a persons regist time in one column and his birth date in another column, now you need to calculate his age when he did the registration. There are two ways to reach this result.</p>\n<ol>\n<li>With the help of relativedelta</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> dateutil <span class=\"keyword\">import</span> relativedelta</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> df.index:</span><br><span class=\"line\">    df.loc[i, <span class=\"string\">'age'</span>] = relativedelta.relativedelta(df.loc[i, <span class=\"string\">'regDate'</span>], df.loc[i, <span class=\"string\">'DateOfBirth'</span>]).years</span><br></pre></td></tr></table></figure>\n<ol>\n<li>With the help of np.timedelta64</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df[<span class=\"string\">'age'</span>] = (df[<span class=\"string\">'regDate'</span>] - df[<span class=\"string\">'DateOfBirth'</span>])/np.timedelta64(<span class=\"number\">1</span>, <span class=\"string\">'Y'</span>)</span><br><span class=\"line\"><span class=\"comment\"># You may need to convert to integer</span></span><br><span class=\"line\">df[<span class=\"string\">'age'</span>].apply(np.int64)</span><br></pre></td></tr></table></figure>\n"},{"title":"[Pandas]How to import CSV","date":"2018-09-16T16:30:37.000Z","_content":"\n\n### Locate the file\nThere are two ways to locate the csv file, \"absolute path\" or \"relative path\"\ne.g.\nAbsolute path:\n\"`/work/project/data/great.csv`\"(Mac) or \"`C:\\work\\project\\data\\great.csv`\"(Windows)\nRelative path:\n\"`data/greate.csv`\"(Mac) or \"`data\\great.csv`\"(Windows)\n\n### Read with pandas.read_csv()\nThe result is a dataframe Object\n\n\n```python\nimport pandas as pd\nimport numpy as np \n\ndf = pd.read_csv('data/great.csv')\ndf\n```\n\n|company|locate|employees|avenue|\n|--- |--- |--- |--- |\n|orange|New York|10000|4000.0|\n|banana|London|2000|1000.0|\n|pinch|Paris|4000|5000.0|\n|pear|Berlin|3000|NaN|\n\n\n\n\n### No infered header\n\n\n```python\ndf = pd.read_csv('data/great.csv', header= None)\ndf\n```\n\n|0|1|2|3|\n|--- |--- |--- |--- |\n|company|locate|employees|avenue|\n|orange|New York|10000|4000|\n|banana|London|2000|1000|\n|pinch|Paris|4000|5000|\n|pear|Berlin|3000|NaN|\n\n\n\n###  Filter rows \n\n\n```python\ndf = pd.read_csv('data/great.csv', skiprows=[3,4])\ndf\n```\n\n|company|locate|employees|avenue|\n|--- |--- |--- |--- |\n|orange|New York|10000|4000|\n|banana|London|2000|1000|\n\n\nlambda is also welcomed\n\n\n```python\ndf = pd.read_csv('data/great.csv', skiprows=lambda x:x/2==1)\ndf\n```\n\n|company|locate|employees|avenue|\n|--- |--- |--- |--- |\n|orange|New York|10000|4000.0|\n|pear|Berlin|3000|NaN|\n\n\n### Filter columns(or keep wanted)\n\n\n```python\ndf = pd.read_csv('data/great.csv', usecols=['company'])\ndf\n```\n\n|company|\n|--- |\n|orange|\n|banana|\n|pinch|\n|pear|\n\n\nOther options refer https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n","source":"_posts/Pandas-How-to-import-CSV.md","raw":"---\ntitle: '[Pandas]How to import CSV'\ndate: 2018-09-16 16:30:37\ntags:\ncategories: 'pandas'\n---\n\n\n### Locate the file\nThere are two ways to locate the csv file, \"absolute path\" or \"relative path\"\ne.g.\nAbsolute path:\n\"`/work/project/data/great.csv`\"(Mac) or \"`C:\\work\\project\\data\\great.csv`\"(Windows)\nRelative path:\n\"`data/greate.csv`\"(Mac) or \"`data\\great.csv`\"(Windows)\n\n### Read with pandas.read_csv()\nThe result is a dataframe Object\n\n\n```python\nimport pandas as pd\nimport numpy as np \n\ndf = pd.read_csv('data/great.csv')\ndf\n```\n\n|company|locate|employees|avenue|\n|--- |--- |--- |--- |\n|orange|New York|10000|4000.0|\n|banana|London|2000|1000.0|\n|pinch|Paris|4000|5000.0|\n|pear|Berlin|3000|NaN|\n\n\n\n\n### No infered header\n\n\n```python\ndf = pd.read_csv('data/great.csv', header= None)\ndf\n```\n\n|0|1|2|3|\n|--- |--- |--- |--- |\n|company|locate|employees|avenue|\n|orange|New York|10000|4000|\n|banana|London|2000|1000|\n|pinch|Paris|4000|5000|\n|pear|Berlin|3000|NaN|\n\n\n\n###  Filter rows \n\n\n```python\ndf = pd.read_csv('data/great.csv', skiprows=[3,4])\ndf\n```\n\n|company|locate|employees|avenue|\n|--- |--- |--- |--- |\n|orange|New York|10000|4000|\n|banana|London|2000|1000|\n\n\nlambda is also welcomed\n\n\n```python\ndf = pd.read_csv('data/great.csv', skiprows=lambda x:x/2==1)\ndf\n```\n\n|company|locate|employees|avenue|\n|--- |--- |--- |--- |\n|orange|New York|10000|4000.0|\n|pear|Berlin|3000|NaN|\n\n\n### Filter columns(or keep wanted)\n\n\n```python\ndf = pd.read_csv('data/great.csv', usecols=['company'])\ndf\n```\n\n|company|\n|--- |\n|orange|\n|banana|\n|pinch|\n|pear|\n\n\nOther options refer https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n","slug":"Pandas-How-to-import-CSV","published":1,"updated":"2018-09-16T16:37:10.211Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3bw0007m2tqzuhxgjpe","content":"<h3 id=\"Locate-the-file\"><a href=\"#Locate-the-file\" class=\"headerlink\" title=\"Locate the file\"></a>Locate the file</h3><p>There are two ways to locate the csv file, absolute path or relative path<br>e.g.<br>Absolute path:<br><code>/work/project/data/great.csv</code>(Mac) or <code>C:\\work\\project\\data\\great.csv</code>(Windows)<br>Relative path:<br><code>data/greate.csv</code>(Mac) or <code>data\\great.csv</code>(Windows)</p>\n<h3 id=\"Read-with-pandas-read-csv\"><a href=\"#Read-with-pandas-read-csv\" class=\"headerlink\" title=\"Read with pandas.read_csv()\"></a>Read with pandas.read_csv()</h3><p>The result is a dataframe Object</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np </span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>company</th>\n<th>locate</th>\n<th>employees</th>\n<th>avenue</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>orange</td>\n<td>New York</td>\n<td>10000</td>\n<td>4000.0</td>\n</tr>\n<tr>\n<td>banana</td>\n<td>London</td>\n<td>2000</td>\n<td>1000.0</td>\n</tr>\n<tr>\n<td>pinch</td>\n<td>Paris</td>\n<td>4000</td>\n<td>5000.0</td>\n</tr>\n<tr>\n<td>pear</td>\n<td>Berlin</td>\n<td>3000</td>\n<td>NaN</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"No-infered-header\"><a href=\"#No-infered-header\" class=\"headerlink\" title=\"No infered header\"></a>No infered header</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>, header= <span class=\"keyword\">None</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>0</th>\n<th>1</th>\n<th>2</th>\n<th>3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>company</td>\n<td>locate</td>\n<td>employees</td>\n<td>avenue</td>\n</tr>\n<tr>\n<td>orange</td>\n<td>New York</td>\n<td>10000</td>\n<td>4000</td>\n</tr>\n<tr>\n<td>banana</td>\n<td>London</td>\n<td>2000</td>\n<td>1000</td>\n</tr>\n<tr>\n<td>pinch</td>\n<td>Paris</td>\n<td>4000</td>\n<td>5000</td>\n</tr>\n<tr>\n<td>pear</td>\n<td>Berlin</td>\n<td>3000</td>\n<td>NaN</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Filter-rows\"><a href=\"#Filter-rows\" class=\"headerlink\" title=\"Filter rows\"></a>Filter rows</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>, skiprows=[<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>company</th>\n<th>locate</th>\n<th>employees</th>\n<th>avenue</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>orange</td>\n<td>New York</td>\n<td>10000</td>\n<td>4000</td>\n</tr>\n<tr>\n<td>banana</td>\n<td>London</td>\n<td>2000</td>\n<td>1000</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>lambda is also welcomed</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>, skiprows=<span class=\"keyword\">lambda</span> x:x/<span class=\"number\">2</span>==<span class=\"number\">1</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>company</th>\n<th>locate</th>\n<th>employees</th>\n<th>avenue</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>orange</td>\n<td>New York</td>\n<td>10000</td>\n<td>4000.0</td>\n</tr>\n<tr>\n<td>pear</td>\n<td>Berlin</td>\n<td>3000</td>\n<td>NaN</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Filter-columns-or-keep-wanted\"><a href=\"#Filter-columns-or-keep-wanted\" class=\"headerlink\" title=\"Filter columns(or keep wanted)\"></a>Filter columns(or keep wanted)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>, usecols=[<span class=\"string\">'company'</span>])</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>company</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>orange</td>\n</tr>\n<tr>\n<td>banana</td>\n</tr>\n<tr>\n<td>pinch</td>\n</tr>\n<tr>\n<td>pear</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Other options refer <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" target=\"_blank\" rel=\"noopener\">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Locate-the-file\"><a href=\"#Locate-the-file\" class=\"headerlink\" title=\"Locate the file\"></a>Locate the file</h3><p>There are two ways to locate the csv file, absolute path or relative path<br>e.g.<br>Absolute path:<br><code>/work/project/data/great.csv</code>(Mac) or <code>C:\\work\\project\\data\\great.csv</code>(Windows)<br>Relative path:<br><code>data/greate.csv</code>(Mac) or <code>data\\great.csv</code>(Windows)</p>\n<h3 id=\"Read-with-pandas-read-csv\"><a href=\"#Read-with-pandas-read-csv\" class=\"headerlink\" title=\"Read with pandas.read_csv()\"></a>Read with pandas.read_csv()</h3><p>The result is a dataframe Object</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np </span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>company</th>\n<th>locate</th>\n<th>employees</th>\n<th>avenue</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>orange</td>\n<td>New York</td>\n<td>10000</td>\n<td>4000.0</td>\n</tr>\n<tr>\n<td>banana</td>\n<td>London</td>\n<td>2000</td>\n<td>1000.0</td>\n</tr>\n<tr>\n<td>pinch</td>\n<td>Paris</td>\n<td>4000</td>\n<td>5000.0</td>\n</tr>\n<tr>\n<td>pear</td>\n<td>Berlin</td>\n<td>3000</td>\n<td>NaN</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"No-infered-header\"><a href=\"#No-infered-header\" class=\"headerlink\" title=\"No infered header\"></a>No infered header</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>, header= <span class=\"keyword\">None</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>0</th>\n<th>1</th>\n<th>2</th>\n<th>3</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>company</td>\n<td>locate</td>\n<td>employees</td>\n<td>avenue</td>\n</tr>\n<tr>\n<td>orange</td>\n<td>New York</td>\n<td>10000</td>\n<td>4000</td>\n</tr>\n<tr>\n<td>banana</td>\n<td>London</td>\n<td>2000</td>\n<td>1000</td>\n</tr>\n<tr>\n<td>pinch</td>\n<td>Paris</td>\n<td>4000</td>\n<td>5000</td>\n</tr>\n<tr>\n<td>pear</td>\n<td>Berlin</td>\n<td>3000</td>\n<td>NaN</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Filter-rows\"><a href=\"#Filter-rows\" class=\"headerlink\" title=\"Filter rows\"></a>Filter rows</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>, skiprows=[<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>company</th>\n<th>locate</th>\n<th>employees</th>\n<th>avenue</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>orange</td>\n<td>New York</td>\n<td>10000</td>\n<td>4000</td>\n</tr>\n<tr>\n<td>banana</td>\n<td>London</td>\n<td>2000</td>\n<td>1000</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>lambda is also welcomed</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>, skiprows=<span class=\"keyword\">lambda</span> x:x/<span class=\"number\">2</span>==<span class=\"number\">1</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>company</th>\n<th>locate</th>\n<th>employees</th>\n<th>avenue</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>orange</td>\n<td>New York</td>\n<td>10000</td>\n<td>4000.0</td>\n</tr>\n<tr>\n<td>pear</td>\n<td>Berlin</td>\n<td>3000</td>\n<td>NaN</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Filter-columns-or-keep-wanted\"><a href=\"#Filter-columns-or-keep-wanted\" class=\"headerlink\" title=\"Filter columns(or keep wanted)\"></a>Filter columns(or keep wanted)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df = pd.read_csv(<span class=\"string\">'data/great.csv'</span>, usecols=[<span class=\"string\">'company'</span>])</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>company</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>orange</td>\n</tr>\n<tr>\n<td>banana</td>\n</tr>\n<tr>\n<td>pinch</td>\n</tr>\n<tr>\n<td>pear</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Other options refer <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\" target=\"_blank\" rel=\"noopener\">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html</a></p>\n"},{"title":"[Pandas] How to import from Sql Server","date":"2018-10-18T20:08:25.000Z","_content":"\nWe need to rely on pyodbc, the sample code is as belows.\n\n```python\nimport pyodbc\ncnxn = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER=SQLSERVER2017;DATABASE=Adventureworks;Trusted_Connection=yes')\n```\n\nWrite SQL and execute with pandas.read_sql\n\n```Python\nimport pandas\nquery = \"SELECT * from a\"\ndf = pd.read_sql(query, sql_conn)\n```\n\n","source":"_posts/Pandas-How-to-import-from-Sql-Server.md","raw":"---\ntitle: '[Pandas] How to import from Sql Server'\ndate: 2018-10-18 20:08:25\ncategories: 'pandas'\n---\n\nWe need to rely on pyodbc, the sample code is as belows.\n\n```python\nimport pyodbc\ncnxn = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER=SQLSERVER2017;DATABASE=Adventureworks;Trusted_Connection=yes')\n```\n\nWrite SQL and execute with pandas.read_sql\n\n```Python\nimport pandas\nquery = \"SELECT * from a\"\ndf = pd.read_sql(query, sql_conn)\n```\n\n","slug":"Pandas-How-to-import-from-Sql-Server","published":1,"updated":"2018-10-18T19:13:30.526Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3by0009m2tqa9xr1h2o","content":"<p>We need to rely on pyodbc, the sample code is as belows.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pyodbc</span><br><span class=\"line\">cnxn = pyodbc.connect(<span class=\"string\">'DRIVER=&#123;ODBC Driver 13 for SQL Server&#125;;SERVER=SQLSERVER2017;DATABASE=Adventureworks;Trusted_Connection=yes'</span>)</span><br></pre></td></tr></table></figure>\n<p>Write SQL and execute with pandas.read_sql</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas</span><br><span class=\"line\">query = <span class=\"string\">\"SELECT * from a\"</span></span><br><span class=\"line\">df = pd.read_sql(query, sql_conn)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>We need to rely on pyodbc, the sample code is as belows.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pyodbc</span><br><span class=\"line\">cnxn = pyodbc.connect(<span class=\"string\">'DRIVER=&#123;ODBC Driver 13 for SQL Server&#125;;SERVER=SQLSERVER2017;DATABASE=Adventureworks;Trusted_Connection=yes'</span>)</span><br></pre></td></tr></table></figure>\n<p>Write SQL and execute with pandas.read_sql</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas</span><br><span class=\"line\">query = <span class=\"string\">\"SELECT * from a\"</span></span><br><span class=\"line\">df = pd.read_sql(query, sql_conn)</span><br></pre></td></tr></table></figure>\n"},{"title":"[Pandas]How to list all columns","date":"2018-10-19T22:31:23.000Z","_content":"\n```python\nlist(dataframe.columns.values)\n```\n\n","source":"_posts/Pandas-How-to-list-all-columns.md","raw":"---\ntitle: '[Pandas]How to list all columns'\ndate: 2018-10-19 22:31:23\ncategories: 'pandas'\n---\n\n```python\nlist(dataframe.columns.values)\n```\n\n","slug":"Pandas-How-to-list-all-columns","published":1,"updated":"2018-10-19T21:35:19.314Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3bz000am2tqz1rz2ye5","content":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list(dataframe.columns.values)</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list(dataframe.columns.values)</span><br></pre></td></tr></table></figure>\n"},{"title":"[Pandas]How to rename columns","date":"2018-09-17T22:11:55.000Z","_content":"\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'A#':[1,2], 'B#':[3,4], 'C#':[5,6]})\ndf\n```\n\n| A#   | B#   | C#   |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\n**Rename with DataFrame.rename()**\n\n```python\ndf.rename(index= str, columns={\"A#\": \"a\", \"B#\": \"b\"})\n```\n\n| a    | b    | C#   |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\nPrint df again\n\n| A#   | B#   | C#   |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\nColumn names did'nt change. \n\n**To change names permanently, use  \"inplace=True\"**\n\n```python\ndf.rename(index= str, columns={\"A#\": \"a\", \"B#\": \"b\"}, inplace = True)\ndf\n```\n\n| a    | b    | C#   |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\n**Rename in batch**\n\n```python\ndf.rename(index= str, columns=lambda x:x.replace('#',''))\n```\n\n| A    | B    | C    |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\nLambda is amazing","source":"_posts/Pandas-How-to-rename-columns.md","raw":"---\ntitle: '[Pandas]How to rename columns'\ndate: 2018-09-17 22:11:55\ncategories: 'pandas'\n---\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'A#':[1,2], 'B#':[3,4], 'C#':[5,6]})\ndf\n```\n\n| A#   | B#   | C#   |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\n**Rename with DataFrame.rename()**\n\n```python\ndf.rename(index= str, columns={\"A#\": \"a\", \"B#\": \"b\"})\n```\n\n| a    | b    | C#   |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\nPrint df again\n\n| A#   | B#   | C#   |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\nColumn names did'nt change. \n\n**To change names permanently, use  \"inplace=True\"**\n\n```python\ndf.rename(index= str, columns={\"A#\": \"a\", \"B#\": \"b\"}, inplace = True)\ndf\n```\n\n| a    | b    | C#   |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\n**Rename in batch**\n\n```python\ndf.rename(index= str, columns=lambda x:x.replace('#',''))\n```\n\n| A    | B    | C    |\n| ---- | ---- | ---- |\n| 1    | 3    | 5    |\n| 2    | 4    | 6    |\n\nLambda is amazing","slug":"Pandas-How-to-rename-columns","published":1,"updated":"2018-09-17T22:03:37.104Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3c0000dm2tqrstm7wzp","content":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">'A#'</span>:[<span class=\"number\">1</span>,<span class=\"number\">2</span>], <span class=\"string\">'B#'</span>:[<span class=\"number\">3</span>,<span class=\"number\">4</span>], <span class=\"string\">'C#'</span>:[<span class=\"number\">5</span>,<span class=\"number\">6</span>]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n<th>B#</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>Rename with DataFrame.rename()</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.rename(index= str, columns=&#123;<span class=\"string\">\"A#\"</span>: <span class=\"string\">\"a\"</span>, <span class=\"string\">\"B#\"</span>: <span class=\"string\">\"b\"</span>&#125;)</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>a</th>\n<th>b</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Print df again</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n<th>B#</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Column names didnt change. </p>\n<p><strong>To change names permanently, use  inplace=True</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.rename(index= str, columns=&#123;<span class=\"string\">\"A#\"</span>: <span class=\"string\">\"a\"</span>, <span class=\"string\">\"B#\"</span>: <span class=\"string\">\"b\"</span>&#125;, inplace = <span class=\"keyword\">True</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>a</th>\n<th>b</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>Rename in batch</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.rename(index= str, columns=<span class=\"keyword\">lambda</span> x:x.replace(<span class=\"string\">'#'</span>,<span class=\"string\">''</span>))</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Lambda is amazing</p>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">'A#'</span>:[<span class=\"number\">1</span>,<span class=\"number\">2</span>], <span class=\"string\">'B#'</span>:[<span class=\"number\">3</span>,<span class=\"number\">4</span>], <span class=\"string\">'C#'</span>:[<span class=\"number\">5</span>,<span class=\"number\">6</span>]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n<th>B#</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>Rename with DataFrame.rename()</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.rename(index= str, columns=&#123;<span class=\"string\">\"A#\"</span>: <span class=\"string\">\"a\"</span>, <span class=\"string\">\"B#\"</span>: <span class=\"string\">\"b\"</span>&#125;)</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>a</th>\n<th>b</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Print df again</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n<th>B#</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Column names didnt change. </p>\n<p><strong>To change names permanently, use  inplace=True</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.rename(index= str, columns=&#123;<span class=\"string\">\"A#\"</span>: <span class=\"string\">\"a\"</span>, <span class=\"string\">\"B#\"</span>: <span class=\"string\">\"b\"</span>&#125;, inplace = <span class=\"keyword\">True</span>)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>a</th>\n<th>b</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>Rename in batch</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.rename(index= str, columns=<span class=\"keyword\">lambda</span> x:x.replace(<span class=\"string\">'#'</span>,<span class=\"string\">''</span>))</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>3</td>\n<td>5</td>\n</tr>\n<tr>\n<td>2</td>\n<td>4</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>Lambda is amazing</p>\n"},{"title":"[Pandas]How to drop columns/rows","date":"2018-09-18T21:39:25.000Z","_content":"\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'A#':[1,2,3], 'B#':[4,5,6], 'C#':[7,8,9]})\ndf\n```\n\n| A#   | B#   | C#   |\n| ---- | ---- | ---- |\n| 1    | 4    | 7    |\n| 2    | 5    | 8    |\n| 3    | 6    | 9    |\n\n### Drop columns\n\n```python\ndf.drop(['B#', 'C#'], axis = 1)\n#or\ndf.drop(columns=['B#', 'C#'])\n```\n\n| A#   |\n| ---- |\n| 1    |\n| 2    |\n| 3    |\n\n### Drop rows\n\n```python\ndf.drop([0,1])\n```\n\n| A#   | B#   | C#   |\n| ---- | ---- | ---- |\n| 3    | 6    | 9    |","source":"_posts/Pandas-How-to-drop-columns-rows.md","raw":"---\ntitle: '[Pandas]How to drop columns/rows'\ndate: 2018-09-18 21:39:25\ncategories: 'pandas'\n---\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'A#':[1,2,3], 'B#':[4,5,6], 'C#':[7,8,9]})\ndf\n```\n\n| A#   | B#   | C#   |\n| ---- | ---- | ---- |\n| 1    | 4    | 7    |\n| 2    | 5    | 8    |\n| 3    | 6    | 9    |\n\n### Drop columns\n\n```python\ndf.drop(['B#', 'C#'], axis = 1)\n#or\ndf.drop(columns=['B#', 'C#'])\n```\n\n| A#   |\n| ---- |\n| 1    |\n| 2    |\n| 3    |\n\n### Drop rows\n\n```python\ndf.drop([0,1])\n```\n\n| A#   | B#   | C#   |\n| ---- | ---- | ---- |\n| 3    | 6    | 9    |","slug":"Pandas-How-to-drop-columns-rows","published":1,"updated":"2018-09-18T20:49:18.318Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3c1000em2tqm17qgp4y","content":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">'A#'</span>:[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>], <span class=\"string\">'B#'</span>:[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>], <span class=\"string\">'C#'</span>:[<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n<th>B#</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>4</td>\n<td>7</td>\n</tr>\n<tr>\n<td>2</td>\n<td>5</td>\n<td>8</td>\n</tr>\n<tr>\n<td>3</td>\n<td>6</td>\n<td>9</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Drop-columns\"><a href=\"#Drop-columns\" class=\"headerlink\" title=\"Drop columns\"></a>Drop columns</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.drop([<span class=\"string\">'B#'</span>, <span class=\"string\">'C#'</span>], axis = <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#or</span></span><br><span class=\"line\">df.drop(columns=[<span class=\"string\">'B#'</span>, <span class=\"string\">'C#'</span>])</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n</tr>\n<tr>\n<td>2</td>\n</tr>\n<tr>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Drop-rows\"><a href=\"#Drop-rows\" class=\"headerlink\" title=\"Drop rows\"></a>Drop rows</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.drop([<span class=\"number\">0</span>,<span class=\"number\">1</span>])</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n<th>B#</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>6</td>\n<td>9</td>\n</tr>\n</tbody>\n</table>\n</div>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">'A#'</span>:[<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>], <span class=\"string\">'B#'</span>:[<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>], <span class=\"string\">'C#'</span>:[<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>]&#125;)</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n<th>B#</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>4</td>\n<td>7</td>\n</tr>\n<tr>\n<td>2</td>\n<td>5</td>\n<td>8</td>\n</tr>\n<tr>\n<td>3</td>\n<td>6</td>\n<td>9</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Drop-columns\"><a href=\"#Drop-columns\" class=\"headerlink\" title=\"Drop columns\"></a>Drop columns</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.drop([<span class=\"string\">'B#'</span>, <span class=\"string\">'C#'</span>], axis = <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\">#or</span></span><br><span class=\"line\">df.drop(columns=[<span class=\"string\">'B#'</span>, <span class=\"string\">'C#'</span>])</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n</tr>\n<tr>\n<td>2</td>\n</tr>\n<tr>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Drop-rows\"><a href=\"#Drop-rows\" class=\"headerlink\" title=\"Drop rows\"></a>Drop rows</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.drop([<span class=\"number\">0</span>,<span class=\"number\">1</span>])</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th>A#</th>\n<th>B#</th>\n<th>C#</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>3</td>\n<td>6</td>\n<td>9</td>\n</tr>\n</tbody>\n</table>\n</div>\n"},{"title":"[Pandas]How to plot counts of each value","date":"2018-10-22T20:40:47.000Z","_content":"\n1. With pandas build in function(actually matplotlib)\n\n```Python\nimport pandas as pd\n% matplotlib inline\ncol_values = ('x', 'x', 'y', 'y' , 'y', 'z')\ndf = pd.DataFrame({'col':col_values})\ndf['col'].value_counts().plot.bar()\n```\n\n![xyz](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz.png)\n\n2. With seaborn\n\n```Python\nimport seaborn as sns\nsns.countplot(df['col'])\n```\n\n![xyz](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz2.png)","source":"_posts/Pandas-How-to-plot-counts-of-each-value.md","raw":"---\ntitle: '[Pandas]How to plot counts of each value'\ndate: 2018-10-22 20:40:47\ncategories: 'pandas'\n\n---\n\n1. With pandas build in function(actually matplotlib)\n\n```Python\nimport pandas as pd\n% matplotlib inline\ncol_values = ('x', 'x', 'y', 'y' , 'y', 'z')\ndf = pd.DataFrame({'col':col_values})\ndf['col'].value_counts().plot.bar()\n```\n\n![xyz](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz.png)\n\n2. With seaborn\n\n```Python\nimport seaborn as sns\nsns.countplot(df['col'])\n```\n\n![xyz](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz2.png)","slug":"Pandas-How-to-plot-counts-of-each-value","published":1,"updated":"2018-10-22T20:18:06.968Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3c2000hm2tqqeu9nl8m","content":"<ol>\n<li>With pandas build in function(actually matplotlib)</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">% matplotlib inline</span><br><span class=\"line\">col_values = (<span class=\"string\">'x'</span>, <span class=\"string\">'x'</span>, <span class=\"string\">'y'</span>, <span class=\"string\">'y'</span> , <span class=\"string\">'y'</span>, <span class=\"string\">'z'</span>)</span><br><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">'col'</span>:col_values&#125;)</span><br><span class=\"line\">df[<span class=\"string\">'col'</span>].value_counts().plot.bar()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz.png\" alt=\"xyz\"></p>\n<ol>\n<li>With seaborn</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\">sns.countplot(df[<span class=\"string\">'col'</span>])</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz2.png\" alt=\"xyz\"></p>\n","site":{"data":{}},"excerpt":"","more":"<ol>\n<li>With pandas build in function(actually matplotlib)</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">% matplotlib inline</span><br><span class=\"line\">col_values = (<span class=\"string\">'x'</span>, <span class=\"string\">'x'</span>, <span class=\"string\">'y'</span>, <span class=\"string\">'y'</span> , <span class=\"string\">'y'</span>, <span class=\"string\">'z'</span>)</span><br><span class=\"line\">df = pd.DataFrame(&#123;<span class=\"string\">'col'</span>:col_values&#125;)</span><br><span class=\"line\">df[<span class=\"string\">'col'</span>].value_counts().plot.bar()</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz.png\" alt=\"xyz\"></p>\n<ol>\n<li>With seaborn</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\">sns.countplot(df[<span class=\"string\">'col'</span>])</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/xyz2.png\" alt=\"xyz\"></p>\n"},{"title":"[Pandas]How to select data","date":"2018-09-20T21:49:51.000Z","_content":"\nThis article shows the most common methods regarding data selection\n\nFirst, let's create a dataframe.\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(5,4), index = ['a', 'b', 'c', 'd', 'e'], columns= ['A', 'B', 'C', 'D'])\ndf\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| a    | 0.122248 | 0.581777 | 0.972888 | 0.869366 |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n| d    | 0.077122 | 0.407514 | 0.019102 | 0.553546 |\n| e    | 0.921520 | 0.157199 | 0.371028 | 0.825792 |\n\n### Use [] for columns selection\n\n```python\ndf[['A', 'B']]\n```\n\n|      | A        | B        |\n| ---- | -------- | -------- |\n| a    | 0.122248 | 0.581777 |\n| b    | 0.476451 | 0.453979 |\n| c    | 0.954790 | 0.747131 |\n| d    | 0.077122 | 0.407514 |\n| e    | 0.921520 | 0.157199 |\n\n### Select a range\n\n```python\ndf[1:3] # or df['b':'c']\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n\n### Use .loc \n\n```python\ndf.loc['a':'c', ['A','B']]\n```\n\n|      | A        | B        |\n| ---- | -------- | -------- |\n| a    | 0.122248 | 0.581777 |\n| b    | 0.476451 | 0.453979 |\n| c    | 0.954790 | 0.747131 |\n\n###  Select all columns\n\n```python\ndf.loc['a':'c', :]\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| a    | 0.122248 | 0.581777 | 0.972888 | 0.869366 |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n\n### Select with boolean\n\n```python\ndf[df['A']>0.2]\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n| e    | 0.921520 | 0.157199 | 0.371028 | 0.825792 |\n\n### Select with callable(lambda)\n\n```python\ndf[lambda df: df['A']>0.2]\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n| e    | 0.921520 | 0.157199 | 0.371028 | 0.825792 |\n\n","source":"_posts/Pandas-How-to-select-data.md","raw":"---\ntitle: '[Pandas]How to select data'\ndate: 2018-09-20 21:49:51\ncategories: 'pandas'\n---\n\nThis article shows the most common methods regarding data selection\n\nFirst, let's create a dataframe.\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(5,4), index = ['a', 'b', 'c', 'd', 'e'], columns= ['A', 'B', 'C', 'D'])\ndf\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| a    | 0.122248 | 0.581777 | 0.972888 | 0.869366 |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n| d    | 0.077122 | 0.407514 | 0.019102 | 0.553546 |\n| e    | 0.921520 | 0.157199 | 0.371028 | 0.825792 |\n\n### Use [] for columns selection\n\n```python\ndf[['A', 'B']]\n```\n\n|      | A        | B        |\n| ---- | -------- | -------- |\n| a    | 0.122248 | 0.581777 |\n| b    | 0.476451 | 0.453979 |\n| c    | 0.954790 | 0.747131 |\n| d    | 0.077122 | 0.407514 |\n| e    | 0.921520 | 0.157199 |\n\n### Select a range\n\n```python\ndf[1:3] # or df['b':'c']\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n\n### Use .loc \n\n```python\ndf.loc['a':'c', ['A','B']]\n```\n\n|      | A        | B        |\n| ---- | -------- | -------- |\n| a    | 0.122248 | 0.581777 |\n| b    | 0.476451 | 0.453979 |\n| c    | 0.954790 | 0.747131 |\n\n###  Select all columns\n\n```python\ndf.loc['a':'c', :]\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| a    | 0.122248 | 0.581777 | 0.972888 | 0.869366 |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n\n### Select with boolean\n\n```python\ndf[df['A']>0.2]\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n| e    | 0.921520 | 0.157199 | 0.371028 | 0.825792 |\n\n### Select with callable(lambda)\n\n```python\ndf[lambda df: df['A']>0.2]\n```\n\n|      | A        | B        | C        | D        |\n| ---- | -------- | -------- | -------- | -------- |\n| b    | 0.476451 | 0.453979 | 0.004705 | 0.644530 |\n| c    | 0.954790 | 0.747131 | 0.652936 | 0.758767 |\n| e    | 0.921520 | 0.157199 | 0.371028 | 0.825792 |\n\n","slug":"Pandas-How-to-select-data","published":1,"updated":"2018-09-21T08:51:35.544Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3c4000im2tq8itj5mgt","content":"<p>This article shows the most common methods regarding data selection</p>\n<p>First, lets create a dataframe.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(np.random.rand(<span class=\"number\">5</span>,<span class=\"number\">4</span>), index = [<span class=\"string\">'a'</span>, <span class=\"string\">'b'</span>, <span class=\"string\">'c'</span>, <span class=\"string\">'d'</span>, <span class=\"string\">'e'</span>], columns= [<span class=\"string\">'A'</span>, <span class=\"string\">'B'</span>, <span class=\"string\">'C'</span>, <span class=\"string\">'D'</span>])</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>0.122248</td>\n<td>0.581777</td>\n<td>0.972888</td>\n<td>0.869366</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n<tr>\n<td>d</td>\n<td>0.077122</td>\n<td>0.407514</td>\n<td>0.019102</td>\n<td>0.553546</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.921520</td>\n<td>0.157199</td>\n<td>0.371028</td>\n<td>0.825792</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Use-for-columns-selection\"><a href=\"#Use-for-columns-selection\" class=\"headerlink\" title=\"Use [] for columns selection\"></a>Use [] for columns selection</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[[<span class=\"string\">'A'</span>, <span class=\"string\">'B'</span>]]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>0.122248</td>\n<td>0.581777</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n</tr>\n<tr>\n<td>d</td>\n<td>0.077122</td>\n<td>0.407514</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.921520</td>\n<td>0.157199</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Select-a-range\"><a href=\"#Select-a-range\" class=\"headerlink\" title=\"Select a range\"></a>Select a range</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[<span class=\"number\">1</span>:<span class=\"number\">3</span>] <span class=\"comment\"># or df['b':'c']</span></span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Use-loc\"><a href=\"#Use-loc\" class=\"headerlink\" title=\"Use .loc\"></a>Use .loc</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.loc[<span class=\"string\">'a'</span>:<span class=\"string\">'c'</span>, [<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>]]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>0.122248</td>\n<td>0.581777</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Select-all-columns\"><a href=\"#Select-all-columns\" class=\"headerlink\" title=\"Select all columns\"></a>Select all columns</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.loc[<span class=\"string\">'a'</span>:<span class=\"string\">'c'</span>, :]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>0.122248</td>\n<td>0.581777</td>\n<td>0.972888</td>\n<td>0.869366</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Select-with-boolean\"><a href=\"#Select-with-boolean\" class=\"headerlink\" title=\"Select with boolean\"></a>Select with boolean</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df[<span class=\"string\">'A'</span>]&gt;<span class=\"number\">0.2</span>]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.921520</td>\n<td>0.157199</td>\n<td>0.371028</td>\n<td>0.825792</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Select-with-callable-lambda\"><a href=\"#Select-with-callable-lambda\" class=\"headerlink\" title=\"Select with callable(lambda)\"></a>Select with callable(lambda)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[<span class=\"keyword\">lambda</span> df: df[<span class=\"string\">'A'</span>]&gt;<span class=\"number\">0.2</span>]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.921520</td>\n<td>0.157199</td>\n<td>0.371028</td>\n<td>0.825792</td>\n</tr>\n</tbody>\n</table>\n</div>\n","site":{"data":{}},"excerpt":"","more":"<p>This article shows the most common methods regarding data selection</p>\n<p>First, lets create a dataframe.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">df = pd.DataFrame(np.random.rand(<span class=\"number\">5</span>,<span class=\"number\">4</span>), index = [<span class=\"string\">'a'</span>, <span class=\"string\">'b'</span>, <span class=\"string\">'c'</span>, <span class=\"string\">'d'</span>, <span class=\"string\">'e'</span>], columns= [<span class=\"string\">'A'</span>, <span class=\"string\">'B'</span>, <span class=\"string\">'C'</span>, <span class=\"string\">'D'</span>])</span><br><span class=\"line\">df</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>0.122248</td>\n<td>0.581777</td>\n<td>0.972888</td>\n<td>0.869366</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n<tr>\n<td>d</td>\n<td>0.077122</td>\n<td>0.407514</td>\n<td>0.019102</td>\n<td>0.553546</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.921520</td>\n<td>0.157199</td>\n<td>0.371028</td>\n<td>0.825792</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Use-for-columns-selection\"><a href=\"#Use-for-columns-selection\" class=\"headerlink\" title=\"Use [] for columns selection\"></a>Use [] for columns selection</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[[<span class=\"string\">'A'</span>, <span class=\"string\">'B'</span>]]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>0.122248</td>\n<td>0.581777</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n</tr>\n<tr>\n<td>d</td>\n<td>0.077122</td>\n<td>0.407514</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.921520</td>\n<td>0.157199</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Select-a-range\"><a href=\"#Select-a-range\" class=\"headerlink\" title=\"Select a range\"></a>Select a range</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[<span class=\"number\">1</span>:<span class=\"number\">3</span>] <span class=\"comment\"># or df['b':'c']</span></span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Use-loc\"><a href=\"#Use-loc\" class=\"headerlink\" title=\"Use .loc\"></a>Use .loc</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.loc[<span class=\"string\">'a'</span>:<span class=\"string\">'c'</span>, [<span class=\"string\">'A'</span>,<span class=\"string\">'B'</span>]]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>0.122248</td>\n<td>0.581777</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Select-all-columns\"><a href=\"#Select-all-columns\" class=\"headerlink\" title=\"Select all columns\"></a>Select all columns</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df.loc[<span class=\"string\">'a'</span>:<span class=\"string\">'c'</span>, :]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>a</td>\n<td>0.122248</td>\n<td>0.581777</td>\n<td>0.972888</td>\n<td>0.869366</td>\n</tr>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Select-with-boolean\"><a href=\"#Select-with-boolean\" class=\"headerlink\" title=\"Select with boolean\"></a>Select with boolean</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[df[<span class=\"string\">'A'</span>]&gt;<span class=\"number\">0.2</span>]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.921520</td>\n<td>0.157199</td>\n<td>0.371028</td>\n<td>0.825792</td>\n</tr>\n</tbody>\n</table>\n</div>\n<h3 id=\"Select-with-callable-lambda\"><a href=\"#Select-with-callable-lambda\" class=\"headerlink\" title=\"Select with callable(lambda)\"></a>Select with callable(lambda)</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df[<span class=\"keyword\">lambda</span> df: df[<span class=\"string\">'A'</span>]&gt;<span class=\"number\">0.2</span>]</span><br></pre></td></tr></table></figure>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th>A</th>\n<th>B</th>\n<th>C</th>\n<th>D</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>b</td>\n<td>0.476451</td>\n<td>0.453979</td>\n<td>0.004705</td>\n<td>0.644530</td>\n</tr>\n<tr>\n<td>c</td>\n<td>0.954790</td>\n<td>0.747131</td>\n<td>0.652936</td>\n<td>0.758767</td>\n</tr>\n<tr>\n<td>e</td>\n<td>0.921520</td>\n<td>0.157199</td>\n<td>0.371028</td>\n<td>0.825792</td>\n</tr>\n</tbody>\n</table>\n</div>\n"},{"title":"What is tf.data and how to use","date":"2018-09-26T20:30:31.000Z","_content":"\nTf.data is a high level API provided by tensorflow, it performs as a pipeline for complex input and output. The core data structure of tf.data is Dataset which represents a potentially large set of elements. \n\nHere is the defination of Dataset given by tensorflow.org\n\n> A `Dataset` can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a \"logical plan\" of transformations that act on those elements.\n\nTo summarize, the dataset is a data pipeline, and we can do some preprocessing on it. The core problem of a pipeline is how the data be imported and consumed,  the following part will explain that as well as some useful APIs in preprocessing data.\n\n### 1. Data input\n\nDataset can be built from several sources including csv file, numpy array and tensors.\n\n#### From CSV\n\nTf.data provides a convenient API  make_csv_dataset to read records from one or more csv files.\n\nSuppose the csv file is \n\n```csv\na,b,c,d\nhow,are,you,mate\nI,am,fine,thanks\n```\n\nWe can build a dataset from the above csv in the following way\n\n```python\ndataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=2)\n```\n\nHere batch_size represents how many records would be aquired in a batch\n\nWe can use Iterator to see what contains in this dataset\n\n```python\nbatch = dataset.make_one_shot_iterator().get_next()\nprint(batch['a'])\n```\n\nThe result is \n\n```\ntf.Tensor(['how' 'I'], shape=(2,), dtype=string)\n```\n\nmake_csv_dataset defaultly takes the first row as header, if there are no header in the csv file like this\n\n```csv\nhow,are,you,mate\nI,am,fine,thanks\n```\n\nWe can set `header=False`and `column_names=['a','b','c''d']`\n\n```\ndataset2 = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=2, header=False,column_names=['a','b','c''d'])\n```\n\nDataset2 should have the same value with dataset1\n\n####  From Tensor slices\n\nWe can also create a dataset from tensors, the related API is  tf.data.Dataset.from_tensor_slices()\n\n```python\ndataset2 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([10, 5]))\n```\n\nActually the input of this API is not necessarily tensors,  numpy arrays are also adaptable .\n\n```\ndataset3 = tf.data.Dataset.from_tensor_slices(np.random.sample((10, 5)))\n```\n\n### 2. Data consuming\n\nThe only way to retrieve the data is Iterator(),  Iterator enables us to loop over all the dataset and get back the data we want. There are basically two kinds of Iterator which are [`make_one_shot_iterator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_one_shot_iterator) and [`make_initializable_iterator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_initializable_iterator).\n\n#### make_one_shot_iterator\n\nThe examples can be find in the first part when we show how to import csv files\n\n#### make_initializable_iterator\n\nCompared to one shot iterator, initializable iterator allows data to be changed after dataset has already been built.Note that this cannot work in eager_execution model. Here is the example\n\n```python\n# using a placeholder\nx = tf.placeholder(tf.float32, shape=[None,2])\ndataset = tf.data.Dataset.from_tensor_slices(x)\ndata = np.random.sample((100,2))\niter = dataset.make_initializable_iterator() # create the iterator\nel = iter.get_next()\nwith tf.Session() as sess:\n    # feed the placeholder with data\n    sess.run(iter.initializer, feed_dict={ x: data }) \n    print(sess.run(el)) # output [ 0.11342909, 0.81430183]\n```\n\n\n\n### 3. Data proprocessing\n\nTf.data provides several tools for data preprocessing such as batch and shuffle\n\n#### Batch\n\ndataset.batch(BATCH_SIZE)  given the BATCH_SIZE, this API will make the output in a batch way, and output BATCH_SIZE size of data at one time.\n\n```python\n# BATCHING\ntf.enable_eager_execution()\nBATCH_SIZE = 2\nx = np.array([1,2,3,4])\n# make a dataset from a numpy array\ndataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\niter = dataset.make_one_shot_iterator()\niter.get_next()\n```\n\nOutput\n\n```python\n<tf.Tensor: id=102, shape=(2,), dtype=int64, numpy=array([1, 2])>\n```\n\n\n\n #### Shuffle\n\nWhen preparing the training data, one important step is shuffling the data to mitigate overfitting, tf.data offers convenient API to do that.\n\n```python\nBATCH_SIZE = 2\nx = np.array([1,2,3,4])\n# make a dataset from a numpy array\ndataset = tf.data.Dataset.from_tensor_slices(x).shuffle(buffer_size = 10).batch(BATCH_SIZE)\niter = dataset.make_one_shot_iterator()\niter.get_next()\n```\n\nOutput\n\n```python\n<tf.Tensor: id=115, shape=(2,), dtype=int64, numpy=array([2, 3])>\n```\n\n\n\n","source":"_posts/What-is-tf-data-and-how-to-use.md","raw":"---\ntitle: What is tf.data and how to use\ndate: 2018-09-26 20:30:31\ncategories: 'tensorflow'\n---\n\nTf.data is a high level API provided by tensorflow, it performs as a pipeline for complex input and output. The core data structure of tf.data is Dataset which represents a potentially large set of elements. \n\nHere is the defination of Dataset given by tensorflow.org\n\n> A `Dataset` can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a \"logical plan\" of transformations that act on those elements.\n\nTo summarize, the dataset is a data pipeline, and we can do some preprocessing on it. The core problem of a pipeline is how the data be imported and consumed,  the following part will explain that as well as some useful APIs in preprocessing data.\n\n### 1. Data input\n\nDataset can be built from several sources including csv file, numpy array and tensors.\n\n#### From CSV\n\nTf.data provides a convenient API  make_csv_dataset to read records from one or more csv files.\n\nSuppose the csv file is \n\n```csv\na,b,c,d\nhow,are,you,mate\nI,am,fine,thanks\n```\n\nWe can build a dataset from the above csv in the following way\n\n```python\ndataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=2)\n```\n\nHere batch_size represents how many records would be aquired in a batch\n\nWe can use Iterator to see what contains in this dataset\n\n```python\nbatch = dataset.make_one_shot_iterator().get_next()\nprint(batch['a'])\n```\n\nThe result is \n\n```\ntf.Tensor(['how' 'I'], shape=(2,), dtype=string)\n```\n\nmake_csv_dataset defaultly takes the first row as header, if there are no header in the csv file like this\n\n```csv\nhow,are,you,mate\nI,am,fine,thanks\n```\n\nWe can set `header=False`and `column_names=['a','b','c''d']`\n\n```\ndataset2 = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=2, header=False,column_names=['a','b','c''d'])\n```\n\nDataset2 should have the same value with dataset1\n\n####  From Tensor slices\n\nWe can also create a dataset from tensors, the related API is  tf.data.Dataset.from_tensor_slices()\n\n```python\ndataset2 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([10, 5]))\n```\n\nActually the input of this API is not necessarily tensors,  numpy arrays are also adaptable .\n\n```\ndataset3 = tf.data.Dataset.from_tensor_slices(np.random.sample((10, 5)))\n```\n\n### 2. Data consuming\n\nThe only way to retrieve the data is Iterator(),  Iterator enables us to loop over all the dataset and get back the data we want. There are basically two kinds of Iterator which are [`make_one_shot_iterator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_one_shot_iterator) and [`make_initializable_iterator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_initializable_iterator).\n\n#### make_one_shot_iterator\n\nThe examples can be find in the first part when we show how to import csv files\n\n#### make_initializable_iterator\n\nCompared to one shot iterator, initializable iterator allows data to be changed after dataset has already been built.Note that this cannot work in eager_execution model. Here is the example\n\n```python\n# using a placeholder\nx = tf.placeholder(tf.float32, shape=[None,2])\ndataset = tf.data.Dataset.from_tensor_slices(x)\ndata = np.random.sample((100,2))\niter = dataset.make_initializable_iterator() # create the iterator\nel = iter.get_next()\nwith tf.Session() as sess:\n    # feed the placeholder with data\n    sess.run(iter.initializer, feed_dict={ x: data }) \n    print(sess.run(el)) # output [ 0.11342909, 0.81430183]\n```\n\n\n\n### 3. Data proprocessing\n\nTf.data provides several tools for data preprocessing such as batch and shuffle\n\n#### Batch\n\ndataset.batch(BATCH_SIZE)  given the BATCH_SIZE, this API will make the output in a batch way, and output BATCH_SIZE size of data at one time.\n\n```python\n# BATCHING\ntf.enable_eager_execution()\nBATCH_SIZE = 2\nx = np.array([1,2,3,4])\n# make a dataset from a numpy array\ndataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\niter = dataset.make_one_shot_iterator()\niter.get_next()\n```\n\nOutput\n\n```python\n<tf.Tensor: id=102, shape=(2,), dtype=int64, numpy=array([1, 2])>\n```\n\n\n\n #### Shuffle\n\nWhen preparing the training data, one important step is shuffling the data to mitigate overfitting, tf.data offers convenient API to do that.\n\n```python\nBATCH_SIZE = 2\nx = np.array([1,2,3,4])\n# make a dataset from a numpy array\ndataset = tf.data.Dataset.from_tensor_slices(x).shuffle(buffer_size = 10).batch(BATCH_SIZE)\niter = dataset.make_one_shot_iterator()\niter.get_next()\n```\n\nOutput\n\n```python\n<tf.Tensor: id=115, shape=(2,), dtype=int64, numpy=array([2, 3])>\n```\n\n\n\n","slug":"What-is-tf-data-and-how-to-use","published":1,"updated":"2018-09-27T19:59:35.482Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3c5000lm2tqphd8faty","content":"<p>Tf.data is a high level API provided by tensorflow, it performs as a pipeline for complex input and output. The core data structure of tf.data is Dataset which represents a potentially large set of elements. </p>\n<p>Here is the defination of Dataset given by tensorflow.org</p>\n<blockquote>\n<p>A <code>Dataset</code> can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a logical plan of transformations that act on those elements.</p>\n</blockquote>\n<p>To summarize, the dataset is a data pipeline, and we can do some preprocessing on it. The core problem of a pipeline is how the data be imported and consumed,  the following part will explain that as well as some useful APIs in preprocessing data.</p>\n<h3 id=\"1-Data-input\"><a href=\"#1-Data-input\" class=\"headerlink\" title=\"1. Data input\"></a>1. Data input</h3><p>Dataset can be built from several sources including csv file, numpy array and tensors.</p>\n<h4 id=\"From-CSV\"><a href=\"#From-CSV\" class=\"headerlink\" title=\"From CSV\"></a>From CSV</h4><p>Tf.data provides a convenient API  make_csv_dataset to read records from one or more csv files.</p>\n<p>Suppose the csv file is </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a,b,c,d</span><br><span class=\"line\">how,are,you,mate</span><br><span class=\"line\">I,am,fine,thanks</span><br></pre></td></tr></table></figure>\n<p>We can build a dataset from the above csv in the following way</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n<p>Here batch_size represents how many records would be aquired in a batch</p>\n<p>We can use Iterator to see what contains in this dataset</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch = dataset.make_one_shot_iterator().get_next()</span><br><span class=\"line\">print(batch[<span class=\"string\">'a'</span>])</span><br></pre></td></tr></table></figure>\n<p>The result is </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tf.Tensor([&apos;how&apos; &apos;I&apos;], shape=(2,), dtype=string)</span><br></pre></td></tr></table></figure>\n<p>make_csv_dataset defaultly takes the first row as header, if there are no header in the csv file like this</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">how,are,you,mate</span><br><span class=\"line\">I,am,fine,thanks</span><br></pre></td></tr></table></figure>\n<p>We can set <code>header=False</code>and <code>column_names=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;&#39;d&#39;]</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataset2 = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=2, header=False,column_names=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;&apos;d&apos;])</span><br></pre></td></tr></table></figure>\n<p>Dataset2 should have the same value with dataset1</p>\n<h4 id=\"From-Tensor-slices\"><a href=\"#From-Tensor-slices\" class=\"headerlink\" title=\"From Tensor slices\"></a>From Tensor slices</h4><p>We can also create a dataset from tensors, the related API is  tf.data.Dataset.from_tensor_slices()</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataset2 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([<span class=\"number\">10</span>, <span class=\"number\">5</span>]))</span><br></pre></td></tr></table></figure>\n<p>Actually the input of this API is not necessarily tensors,  numpy arrays are also adaptable .</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataset3 = tf.data.Dataset.from_tensor_slices(np.random.sample((10, 5)))</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-Data-consuming\"><a href=\"#2-Data-consuming\" class=\"headerlink\" title=\"2. Data consuming\"></a>2. Data consuming</h3><p>The only way to retrieve the data is Iterator(),  Iterator enables us to loop over all the dataset and get back the data we want. There are basically two kinds of Iterator which are <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_one_shot_iterator\" target=\"_blank\" rel=\"noopener\"><code>make_one_shot_iterator</code></a> and <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_initializable_iterator\" target=\"_blank\" rel=\"noopener\"><code>make_initializable_iterator</code></a>.</p>\n<h4 id=\"make-one-shot-iterator\"><a href=\"#make-one-shot-iterator\" class=\"headerlink\" title=\"make_one_shot_iterator\"></a>make_one_shot_iterator</h4><p>The examples can be find in the first part when we show how to import csv files</p>\n<h4 id=\"make-initializable-iterator\"><a href=\"#make-initializable-iterator\" class=\"headerlink\" title=\"make_initializable_iterator\"></a>make_initializable_iterator</h4><p>Compared to one shot iterator, initializable iterator allows data to be changed after dataset has already been built.Note that this cannot work in eager_execution model. Here is the example</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># using a placeholder</span></span><br><span class=\"line\">x = tf.placeholder(tf.float32, shape=[<span class=\"keyword\">None</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\">dataset = tf.data.Dataset.from_tensor_slices(x)</span><br><span class=\"line\">data = np.random.sample((<span class=\"number\">100</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">iter = dataset.make_initializable_iterator() <span class=\"comment\"># create the iterator</span></span><br><span class=\"line\">el = iter.get_next()</span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    <span class=\"comment\"># feed the placeholder with data</span></span><br><span class=\"line\">    sess.run(iter.initializer, feed_dict=&#123; x: data &#125;) </span><br><span class=\"line\">    print(sess.run(el)) <span class=\"comment\"># output [ 0.11342909, 0.81430183]</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Data-proprocessing\"><a href=\"#3-Data-proprocessing\" class=\"headerlink\" title=\"3. Data proprocessing\"></a>3. Data proprocessing</h3><p>Tf.data provides several tools for data preprocessing such as batch and shuffle</p>\n<h4 id=\"Batch\"><a href=\"#Batch\" class=\"headerlink\" title=\"Batch\"></a>Batch</h4><p>dataset.batch(BATCH_SIZE)  given the BATCH_SIZE, this API will make the output in a batch way, and output BATCH_SIZE size of data at one time.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># BATCHING</span></span><br><span class=\"line\">tf.enable_eager_execution()</span><br><span class=\"line\">BATCH_SIZE = <span class=\"number\">2</span></span><br><span class=\"line\">x = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"comment\"># make a dataset from a numpy array</span></span><br><span class=\"line\">dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)</span><br><span class=\"line\">iter = dataset.make_one_shot_iterator()</span><br><span class=\"line\">iter.get_next()</span><br></pre></td></tr></table></figure>\n<p>Output</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;tf.Tensor: id=<span class=\"number\">102</span>, shape=(<span class=\"number\">2</span>,), dtype=int64, numpy=array([<span class=\"number\">1</span>, <span class=\"number\">2</span>])&gt;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Shuffle\"><a href=\"#Shuffle\" class=\"headerlink\" title=\"Shuffle\"></a>Shuffle</h4><p>When preparing the training data, one important step is shuffling the data to mitigate overfitting, tf.data offers convenient API to do that.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BATCH_SIZE = <span class=\"number\">2</span></span><br><span class=\"line\">x = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"comment\"># make a dataset from a numpy array</span></span><br><span class=\"line\">dataset = tf.data.Dataset.from_tensor_slices(x).shuffle(buffer_size = <span class=\"number\">10</span>).batch(BATCH_SIZE)</span><br><span class=\"line\">iter = dataset.make_one_shot_iterator()</span><br><span class=\"line\">iter.get_next()</span><br></pre></td></tr></table></figure>\n<p>Output</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;tf.Tensor: id=<span class=\"number\">115</span>, shape=(<span class=\"number\">2</span>,), dtype=int64, numpy=array([<span class=\"number\">2</span>, <span class=\"number\">3</span>])&gt;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>Tf.data is a high level API provided by tensorflow, it performs as a pipeline for complex input and output. The core data structure of tf.data is Dataset which represents a potentially large set of elements. </p>\n<p>Here is the defination of Dataset given by tensorflow.org</p>\n<blockquote>\n<p>A <code>Dataset</code> can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a logical plan of transformations that act on those elements.</p>\n</blockquote>\n<p>To summarize, the dataset is a data pipeline, and we can do some preprocessing on it. The core problem of a pipeline is how the data be imported and consumed,  the following part will explain that as well as some useful APIs in preprocessing data.</p>\n<h3 id=\"1-Data-input\"><a href=\"#1-Data-input\" class=\"headerlink\" title=\"1. Data input\"></a>1. Data input</h3><p>Dataset can be built from several sources including csv file, numpy array and tensors.</p>\n<h4 id=\"From-CSV\"><a href=\"#From-CSV\" class=\"headerlink\" title=\"From CSV\"></a>From CSV</h4><p>Tf.data provides a convenient API  make_csv_dataset to read records from one or more csv files.</p>\n<p>Suppose the csv file is </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a,b,c,d</span><br><span class=\"line\">how,are,you,mate</span><br><span class=\"line\">I,am,fine,thanks</span><br></pre></td></tr></table></figure>\n<p>We can build a dataset from the above csv in the following way</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n<p>Here batch_size represents how many records would be aquired in a batch</p>\n<p>We can use Iterator to see what contains in this dataset</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch = dataset.make_one_shot_iterator().get_next()</span><br><span class=\"line\">print(batch[<span class=\"string\">'a'</span>])</span><br></pre></td></tr></table></figure>\n<p>The result is </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tf.Tensor([&apos;how&apos; &apos;I&apos;], shape=(2,), dtype=string)</span><br></pre></td></tr></table></figure>\n<p>make_csv_dataset defaultly takes the first row as header, if there are no header in the csv file like this</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">how,are,you,mate</span><br><span class=\"line\">I,am,fine,thanks</span><br></pre></td></tr></table></figure>\n<p>We can set <code>header=False</code>and <code>column_names=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;&#39;d&#39;]</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataset2 = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=2, header=False,column_names=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;&apos;d&apos;])</span><br></pre></td></tr></table></figure>\n<p>Dataset2 should have the same value with dataset1</p>\n<h4 id=\"From-Tensor-slices\"><a href=\"#From-Tensor-slices\" class=\"headerlink\" title=\"From Tensor slices\"></a>From Tensor slices</h4><p>We can also create a dataset from tensors, the related API is  tf.data.Dataset.from_tensor_slices()</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataset2 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([<span class=\"number\">10</span>, <span class=\"number\">5</span>]))</span><br></pre></td></tr></table></figure>\n<p>Actually the input of this API is not necessarily tensors,  numpy arrays are also adaptable .</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dataset3 = tf.data.Dataset.from_tensor_slices(np.random.sample((10, 5)))</span><br></pre></td></tr></table></figure>\n<h3 id=\"2-Data-consuming\"><a href=\"#2-Data-consuming\" class=\"headerlink\" title=\"2. Data consuming\"></a>2. Data consuming</h3><p>The only way to retrieve the data is Iterator(),  Iterator enables us to loop over all the dataset and get back the data we want. There are basically two kinds of Iterator which are <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_one_shot_iterator\" target=\"_blank\" rel=\"noopener\"><code>make_one_shot_iterator</code></a> and <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset#make_initializable_iterator\" target=\"_blank\" rel=\"noopener\"><code>make_initializable_iterator</code></a>.</p>\n<h4 id=\"make-one-shot-iterator\"><a href=\"#make-one-shot-iterator\" class=\"headerlink\" title=\"make_one_shot_iterator\"></a>make_one_shot_iterator</h4><p>The examples can be find in the first part when we show how to import csv files</p>\n<h4 id=\"make-initializable-iterator\"><a href=\"#make-initializable-iterator\" class=\"headerlink\" title=\"make_initializable_iterator\"></a>make_initializable_iterator</h4><p>Compared to one shot iterator, initializable iterator allows data to be changed after dataset has already been built.Note that this cannot work in eager_execution model. Here is the example</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># using a placeholder</span></span><br><span class=\"line\">x = tf.placeholder(tf.float32, shape=[<span class=\"keyword\">None</span>,<span class=\"number\">2</span>])</span><br><span class=\"line\">dataset = tf.data.Dataset.from_tensor_slices(x)</span><br><span class=\"line\">data = np.random.sample((<span class=\"number\">100</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">iter = dataset.make_initializable_iterator() <span class=\"comment\"># create the iterator</span></span><br><span class=\"line\">el = iter.get_next()</span><br><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> sess:</span><br><span class=\"line\">    <span class=\"comment\"># feed the placeholder with data</span></span><br><span class=\"line\">    sess.run(iter.initializer, feed_dict=&#123; x: data &#125;) </span><br><span class=\"line\">    print(sess.run(el)) <span class=\"comment\"># output [ 0.11342909, 0.81430183]</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"3-Data-proprocessing\"><a href=\"#3-Data-proprocessing\" class=\"headerlink\" title=\"3. Data proprocessing\"></a>3. Data proprocessing</h3><p>Tf.data provides several tools for data preprocessing such as batch and shuffle</p>\n<h4 id=\"Batch\"><a href=\"#Batch\" class=\"headerlink\" title=\"Batch\"></a>Batch</h4><p>dataset.batch(BATCH_SIZE)  given the BATCH_SIZE, this API will make the output in a batch way, and output BATCH_SIZE size of data at one time.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># BATCHING</span></span><br><span class=\"line\">tf.enable_eager_execution()</span><br><span class=\"line\">BATCH_SIZE = <span class=\"number\">2</span></span><br><span class=\"line\">x = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"comment\"># make a dataset from a numpy array</span></span><br><span class=\"line\">dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)</span><br><span class=\"line\">iter = dataset.make_one_shot_iterator()</span><br><span class=\"line\">iter.get_next()</span><br></pre></td></tr></table></figure>\n<p>Output</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;tf.Tensor: id=<span class=\"number\">102</span>, shape=(<span class=\"number\">2</span>,), dtype=int64, numpy=array([<span class=\"number\">1</span>, <span class=\"number\">2</span>])&gt;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Shuffle\"><a href=\"#Shuffle\" class=\"headerlink\" title=\"Shuffle\"></a>Shuffle</h4><p>When preparing the training data, one important step is shuffling the data to mitigate overfitting, tf.data offers convenient API to do that.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BATCH_SIZE = <span class=\"number\">2</span></span><br><span class=\"line\">x = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"comment\"># make a dataset from a numpy array</span></span><br><span class=\"line\">dataset = tf.data.Dataset.from_tensor_slices(x).shuffle(buffer_size = <span class=\"number\">10</span>).batch(BATCH_SIZE)</span><br><span class=\"line\">iter = dataset.make_one_shot_iterator()</span><br><span class=\"line\">iter.get_next()</span><br></pre></td></tr></table></figure>\n<p>Output</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;tf.Tensor: id=<span class=\"number\">115</span>, shape=(<span class=\"number\">2</span>,), dtype=int64, numpy=array([<span class=\"number\">2</span>, <span class=\"number\">3</span>])&gt;</span><br></pre></td></tr></table></figure>\n"},{"title":"What is Attention and how to use","date":"2018-09-20T10:06:45.000Z","_content":"\n### Introduction\n\nAttention or Bahdanau Attention is getting more and more interest in Neural Machine Translation(NMT)  and other sequence prediction research, in this article I will briefly introduce what is Attention mechanism, why important it is and how do we use it(in Tensorflow)\n\n### Why Attention\n\nAttention is a mechanism derived from the seq-seq model which started the era of NMT,  in this [paper](https://arxiv.org/pdf/1409.3215.pdf) Sutskever proposed a novel RNN network called encode-decode network to tackle seq-seq prediction problems such as translation.\n\n![Example of an Encoder-Decode Network, from \"Sequence to Sequence Learning with Neural Network\" 2014](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Example-of-an-Encoder-Decode-Network.png)\n\n \n\nThe model performed well in many translation tasks, but it turned out to be limited to very long sequences. The reason lies in this network needs to be able to capture all information about the source sentence, that is easy to long sentences, especially those that are longer than sentences in the training corpus.\n\nAttention provides a solution to this problem, and its core idea is to focus on a relevant part of the source sequence on each step of the decoder.\n\nMaybe unexpectedly, Attention also benefits seq2seq model in other ways, the first one is that it helps with vanishing gradient problem by providing a shortcut to faraway states; the second one is that it gives some interpretability which I will illustrate in the following sector.\n\n### What is Attention\n\nAttention is merely a context vector that provides a richer encoding of the source sequence. **The vector is computed at every decoder time step.**\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_mechanism.jpg)\n\nAs illustrated in the figure above, the attention computation can be summarized into the following three steps:\n\n1. Compute attention weights based on the current target hidden state and all source state(Figure 1)\n\n2. The weighted average of the source states based on the attention weights are then computed, and the result is a context vector(Figure 2)\n\n3. Context vector combined with the current target hidden state yields the attention vector(Figure 3)\n\n   The attention vector is then fed to the next decoding step. \n\n   ![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_0.jpg)\n\n   the score in Figure 1 is computed as follows:\n\n   ![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_1.jpg)\n\nRegarding the score,  the methods by which it is calculated lead to different performance. \n\n### Coding Attention with Tensorflow\n\nSuppose we have already got an encoder-decoder implementation, what we need to do is trivial because Tensorflow has realized in advance the most of the attention building process(Figure 1-3).\n\n```Python\n# Transfer encoder_outputs to attention_states \nattention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n\n# Apply existing attention mechanism \nattention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n    num_units, attention_states,\n    memory_sequence_length=source_sequence_length)\n\n# Feed to the decoder\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    decoder_cell, attention_mechanism,\n    attention_layer_size=num_units)\n```\n\nThe rest codes are mostly the same as standard encoder-decoder.","source":"_posts/What-is-Attention-and-how-to-use.md","raw":"---\ntitle: What is Attention and how to use\ndate: 2018-09-20 10:06:45\ncategories: 'Machine Learning'\n\n---\n\n### Introduction\n\nAttention or Bahdanau Attention is getting more and more interest in Neural Machine Translation(NMT)  and other sequence prediction research, in this article I will briefly introduce what is Attention mechanism, why important it is and how do we use it(in Tensorflow)\n\n### Why Attention\n\nAttention is a mechanism derived from the seq-seq model which started the era of NMT,  in this [paper](https://arxiv.org/pdf/1409.3215.pdf) Sutskever proposed a novel RNN network called encode-decode network to tackle seq-seq prediction problems such as translation.\n\n![Example of an Encoder-Decode Network, from \"Sequence to Sequence Learning with Neural Network\" 2014](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Example-of-an-Encoder-Decode-Network.png)\n\n \n\nThe model performed well in many translation tasks, but it turned out to be limited to very long sequences. The reason lies in this network needs to be able to capture all information about the source sentence, that is easy to long sentences, especially those that are longer than sentences in the training corpus.\n\nAttention provides a solution to this problem, and its core idea is to focus on a relevant part of the source sequence on each step of the decoder.\n\nMaybe unexpectedly, Attention also benefits seq2seq model in other ways, the first one is that it helps with vanishing gradient problem by providing a shortcut to faraway states; the second one is that it gives some interpretability which I will illustrate in the following sector.\n\n### What is Attention\n\nAttention is merely a context vector that provides a richer encoding of the source sequence. **The vector is computed at every decoder time step.**\n\n![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_mechanism.jpg)\n\nAs illustrated in the figure above, the attention computation can be summarized into the following three steps:\n\n1. Compute attention weights based on the current target hidden state and all source state(Figure 1)\n\n2. The weighted average of the source states based on the attention weights are then computed, and the result is a context vector(Figure 2)\n\n3. Context vector combined with the current target hidden state yields the attention vector(Figure 3)\n\n   The attention vector is then fed to the next decoding step. \n\n   ![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_0.jpg)\n\n   the score in Figure 1 is computed as follows:\n\n   ![img](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_1.jpg)\n\nRegarding the score,  the methods by which it is calculated lead to different performance. \n\n### Coding Attention with Tensorflow\n\nSuppose we have already got an encoder-decoder implementation, what we need to do is trivial because Tensorflow has realized in advance the most of the attention building process(Figure 1-3).\n\n```Python\n# Transfer encoder_outputs to attention_states \nattention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n\n# Apply existing attention mechanism \nattention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n    num_units, attention_states,\n    memory_sequence_length=source_sequence_length)\n\n# Feed to the decoder\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n    decoder_cell, attention_mechanism,\n    attention_layer_size=num_units)\n```\n\nThe rest codes are mostly the same as standard encoder-decoder.","slug":"What-is-Attention-and-how-to-use","published":1,"updated":"2018-09-21T13:29:22.423Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3c6000nm2tqk8mle4gc","content":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>Attention or Bahdanau Attention is getting more and more interest in Neural Machine Translation(NMT)  and other sequence prediction research, in this article I will briefly introduce what is Attention mechanism, why important it is and how do we use it(in Tensorflow)</p>\n<h3 id=\"Why-Attention\"><a href=\"#Why-Attention\" class=\"headerlink\" title=\"Why Attention\"></a>Why Attention</h3><p>Attention is a mechanism derived from the seq-seq model which started the era of NMT,  in this <a href=\"https://arxiv.org/pdf/1409.3215.pdf\" target=\"_blank\" rel=\"noopener\">paper</a> Sutskever proposed a novel RNN network called encode-decode network to tackle seq-seq prediction problems such as translation.</p>\n<p><img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Example-of-an-Encoder-Decode-Network.png\" alt=\"Example of an Encoder-Decode Network, from &quot;Sequence to Sequence Learning with Neural Network&quot; 2014\"></p>\n<p>The model performed well in many translation tasks, but it turned out to be limited to very long sequences. The reason lies in this network needs to be able to capture all information about the source sentence, that is easy to long sentences, especially those that are longer than sentences in the training corpus.</p>\n<p>Attention provides a solution to this problem, and its core idea is to focus on a relevant part of the source sequence on each step of the decoder.</p>\n<p>Maybe unexpectedly, Attention also benefits seq2seq model in other ways, the first one is that it helps with vanishing gradient problem by providing a shortcut to faraway states; the second one is that it gives some interpretability which I will illustrate in the following sector.</p>\n<h3 id=\"What-is-Attention\"><a href=\"#What-is-Attention\" class=\"headerlink\" title=\"What is Attention\"></a>What is Attention</h3><p>Attention is merely a context vector that provides a richer encoding of the source sequence. <strong>The vector is computed at every decoder time step.</strong></p>\n<p><img src=\"https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_mechanism.jpg\" alt=\"img\"></p>\n<p>As illustrated in the figure above, the attention computation can be summarized into the following three steps:</p>\n<ol>\n<li><p>Compute attention weights based on the current target hidden state and all source state(Figure 1)</p>\n</li>\n<li><p>The weighted average of the source states based on the attention weights are then computed, and the result is a context vector(Figure 2)</p>\n</li>\n<li><p>Context vector combined with the current target hidden state yields the attention vector(Figure 3)</p>\n<p>The attention vector is then fed to the next decoding step. </p>\n<p><img src=\"https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_0.jpg\" alt=\"img\"></p>\n<p>the score in Figure 1 is computed as follows:</p>\n<p><img src=\"https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_1.jpg\" alt=\"img\"></p>\n</li>\n</ol>\n<p>Regarding the score,  the methods by which it is calculated lead to different performance. </p>\n<h3 id=\"Coding-Attention-with-Tensorflow\"><a href=\"#Coding-Attention-with-Tensorflow\" class=\"headerlink\" title=\"Coding Attention with Tensorflow\"></a>Coding Attention with Tensorflow</h3><p>Suppose we have already got an encoder-decoder implementation, what we need to do is trivial because Tensorflow has realized in advance the most of the attention building process(Figure 1-3).</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Transfer encoder_outputs to attention_states </span></span><br><span class=\"line\">attention_states = tf.transpose(encoder_outputs, [<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Apply existing attention mechanism </span></span><br><span class=\"line\">attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(</span><br><span class=\"line\">    num_units, attention_states,</span><br><span class=\"line\">    memory_sequence_length=source_sequence_length)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Feed to the decoder</span></span><br><span class=\"line\">decoder_cell = tf.contrib.seq2seq.AttentionWrapper(</span><br><span class=\"line\">    decoder_cell, attention_mechanism,</span><br><span class=\"line\">    attention_layer_size=num_units)</span><br></pre></td></tr></table></figure>\n<p>The rest codes are mostly the same as standard encoder-decoder.</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h3><p>Attention or Bahdanau Attention is getting more and more interest in Neural Machine Translation(NMT)  and other sequence prediction research, in this article I will briefly introduce what is Attention mechanism, why important it is and how do we use it(in Tensorflow)</p>\n<h3 id=\"Why-Attention\"><a href=\"#Why-Attention\" class=\"headerlink\" title=\"Why Attention\"></a>Why Attention</h3><p>Attention is a mechanism derived from the seq-seq model which started the era of NMT,  in this <a href=\"https://arxiv.org/pdf/1409.3215.pdf\" target=\"_blank\" rel=\"noopener\">paper</a> Sutskever proposed a novel RNN network called encode-decode network to tackle seq-seq prediction problems such as translation.</p>\n<p><img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Example-of-an-Encoder-Decode-Network.png\" alt=\"Example of an Encoder-Decode Network, from &quot;Sequence to Sequence Learning with Neural Network&quot; 2014\"></p>\n<p>The model performed well in many translation tasks, but it turned out to be limited to very long sequences. The reason lies in this network needs to be able to capture all information about the source sentence, that is easy to long sentences, especially those that are longer than sentences in the training corpus.</p>\n<p>Attention provides a solution to this problem, and its core idea is to focus on a relevant part of the source sequence on each step of the decoder.</p>\n<p>Maybe unexpectedly, Attention also benefits seq2seq model in other ways, the first one is that it helps with vanishing gradient problem by providing a shortcut to faraway states; the second one is that it gives some interpretability which I will illustrate in the following sector.</p>\n<h3 id=\"What-is-Attention\"><a href=\"#What-is-Attention\" class=\"headerlink\" title=\"What is Attention\"></a>What is Attention</h3><p>Attention is merely a context vector that provides a richer encoding of the source sequence. <strong>The vector is computed at every decoder time step.</strong></p>\n<p><img src=\"https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_mechanism.jpg\" alt=\"img\"></p>\n<p>As illustrated in the figure above, the attention computation can be summarized into the following three steps:</p>\n<ol>\n<li><p>Compute attention weights based on the current target hidden state and all source state(Figure 1)</p>\n</li>\n<li><p>The weighted average of the source states based on the attention weights are then computed, and the result is a context vector(Figure 2)</p>\n</li>\n<li><p>Context vector combined with the current target hidden state yields the attention vector(Figure 3)</p>\n<p>The attention vector is then fed to the next decoding step. </p>\n<p><img src=\"https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_0.jpg\" alt=\"img\"></p>\n<p>the score in Figure 1 is computed as follows:</p>\n<p><img src=\"https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_equation_1.jpg\" alt=\"img\"></p>\n</li>\n</ol>\n<p>Regarding the score,  the methods by which it is calculated lead to different performance. </p>\n<h3 id=\"Coding-Attention-with-Tensorflow\"><a href=\"#Coding-Attention-with-Tensorflow\" class=\"headerlink\" title=\"Coding Attention with Tensorflow\"></a>Coding Attention with Tensorflow</h3><p>Suppose we have already got an encoder-decoder implementation, what we need to do is trivial because Tensorflow has realized in advance the most of the attention building process(Figure 1-3).</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Transfer encoder_outputs to attention_states </span></span><br><span class=\"line\">attention_states = tf.transpose(encoder_outputs, [<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Apply existing attention mechanism </span></span><br><span class=\"line\">attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(</span><br><span class=\"line\">    num_units, attention_states,</span><br><span class=\"line\">    memory_sequence_length=source_sequence_length)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Feed to the decoder</span></span><br><span class=\"line\">decoder_cell = tf.contrib.seq2seq.AttentionWrapper(</span><br><span class=\"line\">    decoder_cell, attention_mechanism,</span><br><span class=\"line\">    attention_layer_size=num_units)</span><br></pre></td></tr></table></figure>\n<p>The rest codes are mostly the same as standard encoder-decoder.</p>\n"},{"title":"how to debug python with vscode","date":"2018-10-09T20:39:12.000Z","_content":"\nVisual Studio Code(VScode) is a fast growing code editor in recent years, it supports python in several ways, debugging is an important one. This article will show the details of how to use this feature.\n\n\n\n### 1. \n\n\n\n","source":"_posts/how-to-debug-python-with-vscode.md","raw":"---\ntitle: how to debug python with vscode\ndate: 2018-10-09 20:39:12\ntags: \ncategories: 'tools'\n---\n\nVisual Studio Code(VScode) is a fast growing code editor in recent years, it supports python in several ways, debugging is an important one. This article will show the details of how to use this feature.\n\n\n\n### 1. \n\n\n\n","slug":"how-to-debug-python-with-vscode","published":1,"updated":"2018-10-09T20:50:35.236Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3c7000qm2tq6443uw9n","content":"<p>Visual Studio Code(VScode) is a fast growing code editor in recent years, it supports python in several ways, debugging is an important one. This article will show the details of how to use this feature.</p>\n<h3 id=\"1\"><a href=\"#1\" class=\"headerlink\" title=\"1.\"></a>1.</h3>","site":{"data":{}},"excerpt":"","more":"<p>Visual Studio Code(VScode) is a fast growing code editor in recent years, it supports python in several ways, debugging is an important one. This article will show the details of how to use this feature.</p>\n<h3 id=\"1\"><a href=\"#1\" class=\"headerlink\" title=\"1.\"></a>1.</h3>"},{"title":"Tensorflow101","date":"2018-07-10T22:25:42.000Z","_content":"\n\nTensorflow is a high performance numerical computation software library, it is mostly known for its strong support for machine learning and deep learning.\n\n### How to Install\n\nIf you have pip, everything is simple\n`pip install tensorflow`\n\n### Basic Concepts\n\n> Graph  \n\nGraph is a fundamental concept in Tensorflow. Take ReLU computation as an example, the function of ReLU is \n\n*h=ReLU(Wx+b)*\n\nIn the view of Tensorflow, the function looks like this\n\n![relu_graph](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_01.png)\n\n> Nodes  \n\nVariables such as *W* and *b* , placeholders such as x, are operations such as *MatMul*, *Add* are all nodes in the graph.\n\n\n> Edges  \n>   \nThe edges between nodes indicate the data which flow between nodes,  in tensorflow data is represented as tensor .  \n\ntensor + flow  = tensorflow\n\n\n### Codes \n\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nif __name__ == '__main__':\n\n\tb = tf.Variable(tf.zeros((10,)))\n\tW = tf.Variable(tf.random_uniform((20, 10), -1, 1))\n\n\tx = tf.placeholder(tf.float32, (10, 20))\n\n\th = tf.nn.relu(tf.matmul(x, W) + b)\n\n\twriter = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n\n\twith tf.Session() as sess:\n\t\tsess.run(tf.global_variables_initializer())\t\n\t\tsess.run(h, {x: np.random.random((10, 20))})\n\n\twriter.close()\n\n```\n\nHere are the steps described in the above codes.\n\n1. Create a graph using Variables and placeholders.\n2. Start a tensorflow session and deploy the graph into the session.\n3. Run the session, let the tensors flow.\n4. Write processing logs using tools such as tf.summary.\n\nSession is the so called execution environment. It needs two parameters which are Fetches and Feeds.\n\n`sess.run(fetches, feeds)`\n\nFetches: List of graph nodes\n\nFeeds: Dictionary mapping from graph nodes to concrete values.\n\nIn the ReLU example, Fetches = h = tf.nn.relu(tf.matmul(x, W) + b) , Feeds = {x: np.random.random((10, 20))}\n\nIt would be interesting to see what happened during the running time. We can try TensorBoard.\n\n\n\n## TensorBoard Result\n\n![tensorboard](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_02.png)\n\n\n![tensorboard_relu](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_03.png)\n\nThe Main Graph clearly shows how the tensor flows through the graph. In a complex machine learning program, the printed diagram is a convenient tool to increase the confidence of the result.\n\n","source":"_posts/Tensorflow101.md","raw":"---\ntitle: Tensorflow101\ndate: 2018-07-10 22:25:42\ntags:\ncategories: \"tensorflow\" \n---\n\n\nTensorflow is a high performance numerical computation software library, it is mostly known for its strong support for machine learning and deep learning.\n\n### How to Install\n\nIf you have pip, everything is simple\n`pip install tensorflow`\n\n### Basic Concepts\n\n> Graph  \n\nGraph is a fundamental concept in Tensorflow. Take ReLU computation as an example, the function of ReLU is \n\n*h=ReLU(Wx+b)*\n\nIn the view of Tensorflow, the function looks like this\n\n![relu_graph](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_01.png)\n\n> Nodes  \n\nVariables such as *W* and *b* , placeholders such as x, are operations such as *MatMul*, *Add* are all nodes in the graph.\n\n\n> Edges  \n>   \nThe edges between nodes indicate the data which flow between nodes,  in tensorflow data is represented as tensor .  \n\ntensor + flow  = tensorflow\n\n\n### Codes \n\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nif __name__ == '__main__':\n\n\tb = tf.Variable(tf.zeros((10,)))\n\tW = tf.Variable(tf.random_uniform((20, 10), -1, 1))\n\n\tx = tf.placeholder(tf.float32, (10, 20))\n\n\th = tf.nn.relu(tf.matmul(x, W) + b)\n\n\twriter = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n\n\twith tf.Session() as sess:\n\t\tsess.run(tf.global_variables_initializer())\t\n\t\tsess.run(h, {x: np.random.random((10, 20))})\n\n\twriter.close()\n\n```\n\nHere are the steps described in the above codes.\n\n1. Create a graph using Variables and placeholders.\n2. Start a tensorflow session and deploy the graph into the session.\n3. Run the session, let the tensors flow.\n4. Write processing logs using tools such as tf.summary.\n\nSession is the so called execution environment. It needs two parameters which are Fetches and Feeds.\n\n`sess.run(fetches, feeds)`\n\nFetches: List of graph nodes\n\nFeeds: Dictionary mapping from graph nodes to concrete values.\n\nIn the ReLU example, Fetches = h = tf.nn.relu(tf.matmul(x, W) + b) , Feeds = {x: np.random.random((10, 20))}\n\nIt would be interesting to see what happened during the running time. We can try TensorBoard.\n\n\n\n## TensorBoard Result\n\n![tensorboard](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_02.png)\n\n\n![tensorboard_relu](https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_03.png)\n\nThe Main Graph clearly shows how the tensor flows through the graph. In a complex machine learning program, the printed diagram is a convenient tool to increase the confidence of the result.\n\n","slug":"Tensorflow101","published":1,"updated":"2018-09-26T19:09:32.298Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjo8jh3c8000sm2tqi8zkqyyb","content":"<p>Tensorflow is a high performance numerical computation software library, it is mostly known for its strong support for machine learning and deep learning.</p>\n<h3 id=\"How-to-Install\"><a href=\"#How-to-Install\" class=\"headerlink\" title=\"How to Install\"></a>How to Install</h3><p>If you have pip, everything is simple<br><code>pip install tensorflow</code></p>\n<h3 id=\"Basic-Concepts\"><a href=\"#Basic-Concepts\" class=\"headerlink\" title=\"Basic Concepts\"></a>Basic Concepts</h3><blockquote>\n<p>Graph  </p>\n</blockquote>\n<p>Graph is a fundamental concept in Tensorflow. Take ReLU computation as an example, the function of ReLU is </p>\n<p><em>h=ReLU(Wx+b)</em></p>\n<p>In the view of Tensorflow, the function looks like this</p>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_01.png\" alt=\"relu_graph\"></p>\n<blockquote>\n<p>Nodes  </p>\n</blockquote>\n<p>Variables such as <em>W</em> and <em>b</em> , placeholders such as x, are operations such as <em>MatMul</em>, <em>Add</em> are all nodes in the graph.</p>\n<blockquote>\n<p>Edges  </p>\n<p>The edges between nodes indicate the data which flow between nodes,  in tensorflow data is represented as tensor .  </p>\n</blockquote>\n<p>tensor + flow  = tensorflow</p>\n<h3 id=\"Codes\"><a href=\"#Codes\" class=\"headerlink\" title=\"Codes\"></a>Codes</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\"></span><br><span class=\"line\">\tb = tf.Variable(tf.zeros((10,)))</span><br><span class=\"line\">\tW = tf.Variable(tf.random_uniform((20, 10), -1, 1))</span><br><span class=\"line\"></span><br><span class=\"line\">\tx = tf.placeholder(tf.float32, (10, 20))</span><br><span class=\"line\"></span><br><span class=\"line\">\th = tf.nn.relu(tf.matmul(x, W) + b)</span><br><span class=\"line\"></span><br><span class=\"line\">\twriter = tf.summary.FileWriter(&apos;./graphs&apos;, tf.get_default_graph())</span><br><span class=\"line\"></span><br><span class=\"line\">\twith tf.Session() as sess:</span><br><span class=\"line\">\t\tsess.run(tf.global_variables_initializer())\t</span><br><span class=\"line\">\t\tsess.run(h, &#123;x: np.random.random((10, 20))&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\twriter.close()</span><br></pre></td></tr></table></figure>\n<p>Here are the steps described in the above codes.</p>\n<ol>\n<li>Create a graph using Variables and placeholders.</li>\n<li>Start a tensorflow session and deploy the graph into the session.</li>\n<li>Run the session, let the tensors flow.</li>\n<li>Write processing logs using tools such as tf.summary.</li>\n</ol>\n<p>Session is the so called execution environment. It needs two parameters which are Fetches and Feeds.</p>\n<p><code>sess.run(fetches, feeds)</code></p>\n<p>Fetches: List of graph nodes</p>\n<p>Feeds: Dictionary mapping from graph nodes to concrete values.</p>\n<p>In the ReLU example, Fetches = h = tf.nn.relu(tf.matmul(x, W) + b) , Feeds = {x: np.random.random((10, 20))}</p>\n<p>It would be interesting to see what happened during the running time. We can try TensorBoard.</p>\n<h2 id=\"TensorBoard-Result\"><a href=\"#TensorBoard-Result\" class=\"headerlink\" title=\"TensorBoard Result\"></a>TensorBoard Result</h2><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_02.png\" alt=\"tensorboard\"></p>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_03.png\" alt=\"tensorboard_relu\"></p>\n<p>The Main Graph clearly shows how the tensor flows through the graph. In a complex machine learning program, the printed diagram is a convenient tool to increase the confidence of the result.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Tensorflow is a high performance numerical computation software library, it is mostly known for its strong support for machine learning and deep learning.</p>\n<h3 id=\"How-to-Install\"><a href=\"#How-to-Install\" class=\"headerlink\" title=\"How to Install\"></a>How to Install</h3><p>If you have pip, everything is simple<br><code>pip install tensorflow</code></p>\n<h3 id=\"Basic-Concepts\"><a href=\"#Basic-Concepts\" class=\"headerlink\" title=\"Basic Concepts\"></a>Basic Concepts</h3><blockquote>\n<p>Graph  </p>\n</blockquote>\n<p>Graph is a fundamental concept in Tensorflow. Take ReLU computation as an example, the function of ReLU is </p>\n<p><em>h=ReLU(Wx+b)</em></p>\n<p>In the view of Tensorflow, the function looks like this</p>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_01.png\" alt=\"relu_graph\"></p>\n<blockquote>\n<p>Nodes  </p>\n</blockquote>\n<p>Variables such as <em>W</em> and <em>b</em> , placeholders such as x, are operations such as <em>MatMul</em>, <em>Add</em> are all nodes in the graph.</p>\n<blockquote>\n<p>Edges  </p>\n<p>The edges between nodes indicate the data which flow between nodes,  in tensorflow data is represented as tensor .  </p>\n</blockquote>\n<p>tensor + flow  = tensorflow</p>\n<h3 id=\"Codes\"><a href=\"#Codes\" class=\"headerlink\" title=\"Codes\"></a>Codes</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import tensorflow as tf</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\"></span><br><span class=\"line\">\tb = tf.Variable(tf.zeros((10,)))</span><br><span class=\"line\">\tW = tf.Variable(tf.random_uniform((20, 10), -1, 1))</span><br><span class=\"line\"></span><br><span class=\"line\">\tx = tf.placeholder(tf.float32, (10, 20))</span><br><span class=\"line\"></span><br><span class=\"line\">\th = tf.nn.relu(tf.matmul(x, W) + b)</span><br><span class=\"line\"></span><br><span class=\"line\">\twriter = tf.summary.FileWriter(&apos;./graphs&apos;, tf.get_default_graph())</span><br><span class=\"line\"></span><br><span class=\"line\">\twith tf.Session() as sess:</span><br><span class=\"line\">\t\tsess.run(tf.global_variables_initializer())\t</span><br><span class=\"line\">\t\tsess.run(h, &#123;x: np.random.random((10, 20))&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\twriter.close()</span><br></pre></td></tr></table></figure>\n<p>Here are the steps described in the above codes.</p>\n<ol>\n<li>Create a graph using Variables and placeholders.</li>\n<li>Start a tensorflow session and deploy the graph into the session.</li>\n<li>Run the session, let the tensors flow.</li>\n<li>Write processing logs using tools such as tf.summary.</li>\n</ol>\n<p>Session is the so called execution environment. It needs two parameters which are Fetches and Feeds.</p>\n<p><code>sess.run(fetches, feeds)</code></p>\n<p>Fetches: List of graph nodes</p>\n<p>Feeds: Dictionary mapping from graph nodes to concrete values.</p>\n<p>In the ReLU example, Fetches = h = tf.nn.relu(tf.matmul(x, W) + b) , Feeds = {x: np.random.random((10, 20))}</p>\n<p>It would be interesting to see what happened during the running time. We can try TensorBoard.</p>\n<h2 id=\"TensorBoard-Result\"><a href=\"#TensorBoard-Result\" class=\"headerlink\" title=\"TensorBoard Result\"></a>TensorBoard Result</h2><p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_02.png\" alt=\"tensorboard\"></p>\n<p><img src=\"https://raw.githubusercontent.com/niuguy/blog/master/public/pic/101_03.png\" alt=\"tensorboard_relu\"></p>\n<p>The Main Graph clearly shows how the tensor flows through the graph. In a complex machine learning program, the printed diagram is a convenient tool to increase the confidence of the result.</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjo8jh3bo0001m2tq75eap7o3","category_id":"cjo8jh3bt0004m2tqgyht698y","_id":"cjo8jh3c0000cm2tqnp4qg533"},{"post_id":"cjo8jh3br0003m2tqaf3t1uch","category_id":"cjo8jh3bt0004m2tqgyht698y","_id":"cjo8jh3c2000fm2tq57vsz2h0"},{"post_id":"cjo8jh3c0000dm2tqrstm7wzp","category_id":"cjo8jh3c0000bm2tqaidnyd2i","_id":"cjo8jh3c4000jm2tqjhy0flbk"},{"post_id":"cjo8jh3bu0005m2tqgv7e5j1e","category_id":"cjo8jh3c0000bm2tqaidnyd2i","_id":"cjo8jh3c5000mm2tqe2nnfgys"},{"post_id":"cjo8jh3c1000em2tqm17qgp4y","category_id":"cjo8jh3c0000bm2tqaidnyd2i","_id":"cjo8jh3c6000om2tq83n11l8v"},{"post_id":"cjo8jh3c2000hm2tqqeu9nl8m","category_id":"cjo8jh3c0000bm2tqaidnyd2i","_id":"cjo8jh3c8000rm2tqgamdn43e"},{"post_id":"cjo8jh3bv0006m2tq668fydig","category_id":"cjo8jh3c2000gm2tq0caa0gmg","_id":"cjo8jh3c9000tm2tqctghqbwy"},{"post_id":"cjo8jh3c4000im2tq8itj5mgt","category_id":"cjo8jh3c0000bm2tqaidnyd2i","_id":"cjo8jh3ca000vm2tqnx2ohd4w"},{"post_id":"cjo8jh3bw0007m2tqzuhxgjpe","category_id":"cjo8jh3c0000bm2tqaidnyd2i","_id":"cjo8jh3ca000wm2tqvewpjq4s"},{"post_id":"cjo8jh3c6000nm2tqk8mle4gc","category_id":"cjo8jh3bt0004m2tqgyht698y","_id":"cjo8jh3cb000ym2tqwkf0n7sv"},{"post_id":"cjo8jh3by0009m2tqa9xr1h2o","category_id":"cjo8jh3c0000bm2tqaidnyd2i","_id":"cjo8jh3cb000zm2tq3fp5ghiq"},{"post_id":"cjo8jh3bz000am2tqz1rz2ye5","category_id":"cjo8jh3c0000bm2tqaidnyd2i","_id":"cjo8jh3cb0011m2tqz92slm9d"},{"post_id":"cjo8jh3c5000lm2tqphd8faty","category_id":"cjo8jh3ca000xm2tq5335ckop","_id":"cjo8jh3cc0012m2tqmjb97k1e"},{"post_id":"cjo8jh3c7000qm2tq6443uw9n","category_id":"cjo8jh3cb0010m2tqzejcidy7","_id":"cjo8jh3cd0014m2tq2do6rl4c"},{"post_id":"cjo8jh3c8000sm2tqi8zkqyyb","category_id":"cjo8jh3ca000xm2tq5335ckop","_id":"cjo8jh3cd0015m2tq52m78ryf"}],"PostTag":[],"Tag":[]}}